{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 6: Data Preprocessing\n",
    "\n",
    "## Goals\n",
    "\n",
    "- **Neuroscience / Neuroimaging concepts**\n",
    "    * Scanner artifacts\n",
    "    * Motion correction\n",
    "    * Slice time correction\n",
    "- **Datascience / Coding concepts**\n",
    "    * Functions\n",
    "    * Conditional Statements\n",
    "    * Standard Deviation\n",
    "    * Interpolation (linear and cubic)\n",
    "    * Temporal Filtering (highpass filtering)\n",
    "    * Spatial Filtering (Gaussian filtering)\n",
    "    * Temporal Normalization (Z-Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Imports\n",
    "\n",
    "This cell will import the Python modules needed for today's lecture. Simply run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nibabel\n",
    "import cortex\n",
    "import os\n",
    "import copy\n",
    "import urllib.request\n",
    "from nipype.interfaces import fsl\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, Image, widgets\n",
    "\n",
    "# Set plotting defaults\n",
    "%matplotlib inline\n",
    "\n",
    "# make sure nipype uses zipped NIFTI files\n",
    "fsl.FSLCommand.set_default_output_type('NIFTI_GZ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "This cell will load data necessary for today's lecture. Simply run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an output folder\n",
    "if not os.path.exists('output'):\n",
    "    os.mkdir('output')\n",
    "\n",
    "# download the data we'll be using today\n",
    "filename_nopreproc = 'output/nopreproc_rotated.nii'\n",
    "_ = urllib.request.urlretrieve('https://berkeley.box.com/shared/static/x4dd2k5jh86njm19220y54rkh5s42vw0.nii', filename_nopreproc)\n",
    "filename_nopreproc_original = 'output/nopreproc_original.nii.gz'\n",
    "_ = urllib.request.urlretrieve('https://berkeley.box.com/shared/static/xaxel1xj4npmvlsa1ijdxyvqgxf00op5.gz', filename_nopreproc_original)\n",
    "filename_nopreproc_corrected = 'output/nopreproc_corrected.nii'\n",
    "_ = urllib.request.urlretrieve('https://berkeley.box.com/shared/static/7r83ol7i3zikdekx4gds2n95vyyecxie.nii', filename_nopreproc_corrected)\n",
    "\n",
    "# load the data that hasn't been preprocessed\n",
    "img_nopreproc_raw = nibabel.load(filename_nopreproc)\n",
    "data_nopreproc = img_nopreproc_raw.get_data().T\n",
    "img_nopreproc_original = nibabel.load(filename_nopreproc_original)\n",
    "data_nopreproc_original = img_nopreproc_original.get_data().T\n",
    "img_nopreproc_corrected = nibabel.load(filename_nopreproc_corrected)\n",
    "data_nopreproc_corrected = img_nopreproc_corrected.get_data().T\n",
    "\n",
    "# load the data that has been prepocessed and can be drawn on a flatmap\n",
    "img_preproc = nibabel.load(\"/data/cogneuro/fMRI/categories/s01_categories_01.nii.gz\")\n",
    "data_preproc = img_preproc.get_data().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking\n",
    "\n",
    "Masking is a subsetting technique that uses an array of boolean (True/False) values to indicate which indices to keep in the subset. It can be applied to all of the axes of an array, or only a subset of them. It always reduces the number of dimension that are masked to 1. It is very similar to indexing, and can be considered a subset of indexing. We learned to create two kinds of masks that can be used on 3-D Volume data: brain masks and cortical masks. Let's review both here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brain Masks\n",
    "\n",
    "Brain masks include all of the voxels in the brain, including CSF, white matter and gray matter. We create a brain mask by choosing a threshold value to select brain vs. outside of brain voxels. In order to determine that threshold we will plot a histogram of all the voxel values and look for the start of the second peak of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,5))\n",
    "_ = plt.hist(data_preproc.flatten(), bins=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the threshold starts around 400, so we'll use that as a threshold, and create a mask. We'll create the mask by looking at the mean of all the volumes across time, because that is less susceptible to noise fluctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preproc_mean = data_preproc.mean(axis=0)\n",
    "brain_mask = data_preproc_mean > 400\n",
    "brain_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply a mask we use bracket notation as with indexing and slicing, and add the name of the mask into the axis or axes we want to mask. We'll mask a 4-D scan array, so we want to keep the time dimension and mask the 3 spatial dimensions. We should end up with a 2-D collection of voxel time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preproc_brainmasked = data_preproc[:, brain_mask]\n",
    "data_preproc_brainmasked.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cortical Masks\n",
    "\n",
    "Cortical masks are a subset of brain masks. They only contain the grey matter that is in the cortex. These masks are more precise than brains masks because they're not created using a threshold, rather based on a **segmentation** that is done by FreeSurfer. **Segmentation** labels each voxel as belonging to CSF, white matter or gray matter, and then the corical mask includes only those gray matter voxels in the cortex. We use the pycortex module to retrieve a cortical mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 's01'\n",
    "transform = 'catloc'\n",
    "cortical_mask = cortex.db.get_mask(subject, transform, type='cortical')\n",
    "cortical_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the cortical mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preproc_corticalmasked = data_preproc[:, cortical_mask]\n",
    "data_preproc_corticalmasked.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout Session\n",
    "\n",
    "1\\. Does masking make a copy of the original data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Create a histogram of the brain voxels and a second histogram of the cortical voxels. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of 3-D Volume Arrays\n",
    "\n",
    "We learned two ways to visualize 3-D volume arrays. The first involved plotting an image of each slice, and the second involved creating cortical flatmaps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple 2-D Slice Images\n",
    "\n",
    "If we need to see all of the voxels collected in a volume, then we can plot each slice (either axial, coronal or sagittal) onto a separate image, and put them all together into a single figure. To do that more efficiently we learned to use for loops to do a single operation multiple times over a set of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,15))\n",
    "num_axial_slices = data_preproc.shape[1]\n",
    "for s in range(num_axial_slices):\n",
    "    plt.subplot(5, 6, s + 1)\n",
    "    plt.imshow(data_preproc_mean[s,:,:], cmap='gray')\n",
    "    plt.axis('off')\n",
    "    \n",
    "plt.suptitle('Axial Slices of Mean Volume')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Application of Flatmaps: Signal to Noise Ratio (SNR)\n",
    "\n",
    "Let's use an example to illustrate how you might use flatmaps to plot some real fMRI data. A big constraint to finding interesting results in fMRI studies is the quality of the data, and we know that fMRI data is **NOISY**!!! One way to quantify data quality is by calculating the **Signal-to-Noise** ratio, or **SNR**, of your data. This ratio is higher when the actual signal you care about is higher relative to the noise in the data. While we can't know this for sure, there are several ways to estimate it.\n",
    "\n",
    "The most common way is to divide the average of each voxel over time (the signal) by the average standard deviation of all the voxels that lie outside the brain (noise). We'll get into detail about standard deviation later on in this lecture. For now, just know that standard deviation is a measure of how much the data tends to differ from the mean, so it's a measure of a signal's fluctuation. \n",
    "\n",
    "For more information on SNR in fMRI see this website (we did a slightly altered version than mentioned there): https://canlabweb.colorado.edu/wiki/doku.php/help/fmri_quality_control_overview\n",
    "\n",
    "Let's calculate the signal to noise ratio for some data, and then plot it on a flatmap. We'll start by calculating the **signal** of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = np.mean(data_preproc, axis=0)\n",
    "signal.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll define a function that will calculate the noise. You don't need to worry about the details yet, we'll cover that more later. The intuition is that this function will measure how much each voxel outside of the brain fluctuates over time, and take the average (or mean) of all of those fluctuations. We use outside the brain voxels because we assume the only thing affecting the BOLD signal in those voxels is noise the scanner records, since those voxels are just measuring air. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_noise(data):\n",
    "    \n",
    "    # determine the voxels that are outside the brain, \n",
    "    # assuming their value is around 300 in the first volume\n",
    "    outside_brain_mask = data[0] <= 300\n",
    "    \n",
    "    # get all the voxels that are not in the brain, for each time point\n",
    "    outside_brain_vox = data[:,outside_brain_mask]\n",
    "    \n",
    "    # find the standard deviation of the voxels outside the brain, for\n",
    "    # each volume across time\n",
    "    data_std = np.std(outside_brain_vox, axis=0)\n",
    "    \n",
    "    # take the average of all of those standard deviations\n",
    "    noise = np.mean(data_std)\n",
    "    return noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use this helper function on the data to calculate a single number that represents the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now detrend the data using the above function\n",
    "noise = calc_noise(data_preproc)\n",
    "noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to calculate the signal to noise, we'll divide the signal (for which we have a number for each voxel) by the single noise value. The result will be a 3-D volume array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally create a single volume that has the SNR for each volume\n",
    "# by dividing the signal by the noise\n",
    "snr = signal / noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We learned about `cortex.Volume` last week, which stores data, and information about the subject and transformation for that data. This is what pycortex uses when creating flatmaps or interactive maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 's01'\n",
    "transform = 'catloc'\n",
    "volume_SNR = cortex.Volume(snr, subject, transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a volume containing the SNR values for all of our voxels, let's plot it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = cortex.quickflat.make_figure(volume_SNR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting reusable code in functions\n",
    "\n",
    "As you've learned in the Data8 lecture:\n",
    ">\"We have used functions extensively already in this text, but never defined a function of our own. The purpose of defining a function is to give a name to a computational process that may be applied multiple times. There are many situations in computing that require repeated computation\"\n",
    "\n",
    "During the course of the last few lectures we've expanded our use of Python, and now string multiple commands together to accomplish more complex tasks, such as plotting a 2-D slice array. Here we will create a series of so-called **\"helper functions\"**, to automate several tasks which we will do over and over. \n",
    "\n",
    "First, let's review the syntax for functions:\n",
    "\n",
    " `def `**`function_name`**`(`**`argument1`**`, `**`argument2`**`):`\n",
    "<br/>\n",
    "    <p style=\"padding-left:5em\">**`function_body`**</p>\n",
    "    <p style=\"padding-left:5em\">`return `**`return_val`**</p>\n",
    "\n",
    "There are 5 parts to a function:\n",
    "1. `def`: Short for definition, this command tells Python that what comes is a function definition.\n",
    "2. **function_name**: A user-specified name that will be used to call the function. It can be almost anything, but must start with letters.\n",
    "3. **arguments**: The input to the function. They consist of zero or more names (i.e. data) that are passed into the function. Not all functions have arguments, and some have optional arguments which we'll cover later on.\n",
    "4. **function_body**: The code that does the computations of the function. If **arguments** are specified, then the **function_body** does some computations on those **arguments**.\n",
    "5. **return_val**: The ouput of the function, preceeded by the `return` command. A single object (though it can be a container to return multiple objects) that is the result of the computation, or a code indicating the status of the computations done. Not all functions have an explicit return value, and when they don't the `None` object is returned.\n",
    "\n",
    "Now let's create a function that extracts any axial slice from a 3-D volume array. It will take 2 arguments, the volume to extract the slice from, and the axial slice number. It will return a 2-D slice array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_axial_slice(volume, slice_number):\n",
    "    selected_slice = volume[slice_number, :, :]\n",
    "    return selected_slice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's use it on the data we loaded earlier to get the 15th slice of the first volume:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume0 = data_preproc[0,:,:,:]\n",
    "slice_axial15 = get_axial_slice(volume0, 15)\n",
    "slice_axial15.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if we wanted to get a coronal or sagittal slice? We could extend this function to take a 3rd argument called `slice_type` that specifies if it should be `axial`, `coronal`, or `sagittal`. To do that we'll need to use **conditional statements**, so let's do a quick review of that now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Statements: `if`, `elif`, & `else`\n",
    "As discussed in the Data8 lecture:\n",
    ">\"A conditional statement is a multi-line statement that allows Python to choose among different alternatives based on the truth value of an expression. While conditional statements can appear anywhere, they appear most often within the body of a function in order to express alternative behavior depending on argument values.\"\n",
    "\n",
    "The syntax of a conditional statement in Python is:\n",
    "\n",
    " `if `**`conditional_expression1`**`:`\n",
    "    <p style=\"padding-left:5em\">**`conditional1_body`**</p><br/>\n",
    " `elif `**`conditional_expression2`**`:`\n",
    "    <p style=\"padding-left:5em\">**`conditional2_body`**</p><br/>\n",
    " `else:`\n",
    "    <p style=\"padding-left:5em\">**`else_body`**</p>\n",
    "\n",
    "There are 5 different parts to this conditional statement:\n",
    "\n",
    "1. `if` command: Indicates the start of a conditional statement. Always needed for conditional statements.\n",
    "2. **conditional_expression**: When this expression evaluates to `True`, then the **conditional_body** for that `if` or `elif` statement is executed, otherwise if it evaluates to `False`, then the associated **conditional_body** is skipped.\n",
    "3. **conditional_body**: One or more lines of code to be executed when it's associated **conditional_statement** is `True`\n",
    "4. `elif` command: If all the above **conditional_expressions** of all the preceeding `if` and `elif` commands evaluate to false, then the **conditional_expression** of the `elif` command in question will be evaluated. Every conditional statement can have zero or more `elif` commands, they are optional.\n",
    "5. `else` command: If all the preceeding **conditional_expressions** evaluate to `False`, then the **conditional_body** of the `else` command is executed. Every conditional statement can have zero or one `else` commands, they are optional.\n",
    "\n",
    "Now let's use a conditional statement in a function to retrieve a slice on any plane:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slice(volume, slice_number, slice_type):\n",
    "    if slice_type == 'axial':\n",
    "        selected_slice = volume[slice_number, :, :]\n",
    "    elif slice_type == 'coronal':\n",
    "        selected_slice = volume[:, slice_number, :]\n",
    "    elif slice_type == 'sagittal':\n",
    "        selected_slice = volume[:, :, slice_number]\n",
    "    return selected_slice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use it to get the 50th coronal slice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_coronal50 = get_slice(volume0, 50, 'coronal')\n",
    "slice_coronal50.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've learned how to plot a 2-D slice array as an image, so let's make a function for that too! We'll start by giving it the same arguments as we did for `get_slice`, and call it `get_slice_simple` since this is our first, simple attempt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_slice_simple(volume, slice_number, slice_type):\n",
    "    selected_slice = get_slice(volume, slice_number, slice_type)\n",
    "    plt.imshow(selected_slice)\n",
    "    _ = plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we used `get_slice` inside this `plot_slice` function. That is one of the many benefits of using functions, they can be reused anywhere, even inside other functions!\n",
    "\n",
    "Now let's plot the axial slice at index 15:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(8,8))\n",
    "plot_slice_simple(volume0, 15, 'axial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use `plot_slice_simple` again to plot all 3 slices of a given voxel like we did last week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "middle_voxel_indices = (15,50,50)\n",
    "\n",
    "fig1 = plt.figure(figsize=(15,15))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plot_slice_simple(volume0, middle_voxel_indices[1], 'coronal')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plot_slice_simple(volume0, middle_voxel_indices[2], 'sagittal')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plot_slice_simple(volume0, middle_voxel_indices[0], 'axial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional arguments with defaults\n",
    "\n",
    "That looks ok, but the aspect ratio isn't right. It would be nice to add another argument for the aspect ratio, and while we're at it there are a few other nice options to have:\n",
    "* Aspect Ratio\n",
    "* Title\n",
    "* Whether to flip the image\n",
    "* A colormap\n",
    "\n",
    "While all of these options are nice to have, there may be times when we don't want specify them. We can make these arguments optional, and assign them a default value, so that the user can decide if the defaul value is good, or if they want to change it.\n",
    "\n",
    "To make an argument optional, simply provide a default value with an equals sign after the argument name, and the default value after that. Let's have a look: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_slice(volume, slice_number, slice_type, aspect=1.0, title='', flip_top_bottom=False, colormap=None):\n",
    "    selected_slice = get_slice(volume, slice_number, slice_type)\n",
    "    if flip_top_bottom:\n",
    "        selected_slice = selected_slice[::-1,]\n",
    "    plt.imshow(selected_slice, aspect=aspect, cmap=colormap)\n",
    "    plt.title(title)\n",
    "    _ = plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this function, you specify the required arguments first. Then you specify the optional arguments as name, value pairs like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_slice(volume0, 20, 'axial', title=\"Axial Slice 20\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the only optional argument specified was `title`, and everything worked just fine. The default values were used for all of the other arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use this new function to create another function that will plot all 3 slices for a single point. This is sometimes called an **ortho view**, so we'll call the function `plot_ortho`. And in this function, we'll pass it the voxel dimensions so it can calculate the correct aspect ratio itself, called `voxel_dims`. It will also need the single voxel indices to plot, we'll call that `voxel_indices`, and replace `slice_number` with it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ortho(volume, voxel_indices, voxel_dims=(1,1,1)):\n",
    "    aspect_coronal = voxel_dims[0] / voxel_dims[2]\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plot_slice(volume0, voxel_indices[1], 'coronal', aspect=aspect_coronal, \n",
    "               title='Coronal Slice', flip_top_bottom=True, colormap='gray')\n",
    "\n",
    "    aspect_sagittal = voxel_dims[0] / voxel_dims[1]\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plot_slice(volume0, voxel_indices[2], 'sagittal', aspect=aspect_sagittal, \n",
    "               title='Sagittal Slice', flip_top_bottom=True, colormap='gray')\n",
    "\n",
    "    aspect_axial = voxel_dims[1] / voxel_dims[2]\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plot_slice(volume0, voxel_indices[0], 'axial', aspect=aspect_axial, \n",
    "               title='Axial Slice', flip_top_bottom=False, colormap='gray')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get the voxel dimensions, remove the last unnused number, and flip them around so the order is `(Z,Y,X)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxel_dims = img_preproc.header.get_zooms()\n",
    "voxel_dims = voxel_dims[0:3]\n",
    "voxel_dims = voxel_dims[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally use the function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 15))\n",
    "plot_ortho(volume0, middle_voxel_indices, voxel_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's extend these **helper functions** even further, and create a function that plots all the slices of a volume in a given axis, as we learned to do last week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_volume_as_slices(volume, slice_type, nrows, ncols, colormap=None, aspect=1.0, flip_top_bottom=False, title='' ):\n",
    "\n",
    "    # determine how many slices will be created based on the type of slice to plot\n",
    "    if slice_type == 'axial':\n",
    "        num_slices = volume.shape[0]\n",
    "    elif slice_type == 'coronal':\n",
    "        num_slices = volume.shape[1]\n",
    "    elif slice_type == 'sagittal':\n",
    "        num_slices = volume.shape[2]\n",
    "\n",
    "    # create a figure and plot all the slices using a for loop to iterate over all of them\n",
    "    for s in range(num_slices):\n",
    "        plt.subplot(nrows, ncols, s + 1)\n",
    "        plot_slice(volume, s, slice_type, aspect=aspect, flip_top_bottom=flip_top_bottom, colormap=colormap)\n",
    "    \n",
    "    # add a title, and use tight_layout to remove the extra whitespace\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (15,15))\n",
    "_ =  plot_volume_as_slices(volume0, 'axial', 5, 6, colormap=\"gray\", title='Axial Slices of First Volume')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Overview\n",
    "\n",
    "Up to this point in the course we've learned how to work with fMRI data in N-D arrays through mathematical and logical operations, saving and loading files, subsetting (indexing, slicing and masking) and visualization. Before we move onto the statistical analyses that comprise the second half of this course we must first discuss a crucial set of steps that are done before those analyses, namely **Preprocessing**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "As we discussed in the review above, fMRI data is **noisy**, meaning there is a lot of fluctuation in the signals we measure that are not what we are interested in studying. There are various reasons for this noise. Issues with the MRI scanner and subject movement are among several sources which we can address, although it is important to keep in mind that many sources of noise we cannot do anything about, and just have to live with. **Preprocessing** comprises a set of computations that the researcher applies to their data to address some of those sources of noise. This results in improvements in the **signal to noise ratio** (**SNR**) that we discussed in our review today. This improved **SNR** gives the researcher a better chance of finding the signal they are interested in, and thus learning something about the brain!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Software Packages\n",
    "\n",
    "There are several freely available packages created for pre-processing and analysis of fMRI brain data. They have been created at universities (for the most part) and paid for by grants. There are pros and cons to each, but those are beyond the scope of this course. Here we will briefly mention the most popular free packages and provide links for those that want more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [FSL](https://fsl.fmrib.ox.ac.uk/fsl/fslwiki)\n",
    "\n",
    "<img src=\"figures/fsl_feat.gif\" align=\"left\" style=\"height: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [SPM](http://www.fil.ion.ucl.ac.uk/spm/)\n",
    "<img src=\"figures/SPM_Gui.png\" align=\"left\" style=\"height: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [AFNI](https://afni.nimh.nih.gov/)\n",
    "<img src=\"figures/AFNI_Gui.jpg\" align=\"left\" style=\"height: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [FreeSurfer](https://surfer.nmr.mgh.harvard.edu/fswiki/FreeSurferWiki)\n",
    "<img src=\"figures/Freesurfer_GUI.jpg\" align=\"left\" style=\"height: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nipype\n",
    "\n",
    "`Nipype` is a Python module that acts as a \"wrapper\" around all of the above neuroimaging software (and more). A wrapper is a collection of code that calls other programs or code and provides a uniform interface to those other programs. In this case, `Nipype` provides a collection of Python objects that allows the user to seamlessly use parts of these different neuroimaging tools together, or more simply just use Python to do fMRI analysis. We will use `Nipype` combined with FSL to run most of the preprocessing steps we'll learn about today. For more information on `Nipype` follow [this link](http://nipype.readthedocs.io/en/latest/#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Artifacts\n",
    "\n",
    "There are many ways that the scanner can output BOLD data that has errors, or **artifacts**. These artifacts wreak havoc on any data analysis we want to do, and so removing them should be the first step of any **preprocessing pipeline** (or sequence of preprocessing steps). The two most common sources of these artifacts are due to motion and electrical spiking. \n",
    "\n",
    "Dealing with data that contains artifacts is a simple process. We simply remove the bad volume and replace it with data from the previous time point. We also take note of the volumes that we removed so we can control for this in our later analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motion Artifacts\n",
    "\n",
    "Volumes are usually collected one axial (or horizontal) slice at a time. To collect the data, the scanner first has to \"excite\" the tissue it wants to image using low-powered radio-frequency waves (completely harmless to humans). The tissue eventually \"relaxes\" after a short period, but if tissue is excited twice, because it's on the edge between two slices, it will cause problems. For this reason, some scan sequences collect odd numbered slices first, and then even number slices, allowing the tissue to relax. This is called an **interleaved sequence**, and is in comparison to a **descending sequence** in which the slices are collected from the top to the bottom of the brain. The problem with **interleaved sequences** is that subject motion errors are amplified. This problem is illustrated in the below image. \n",
    "\n",
    "The top row shows an interleaved sequence and the bottom row a descending sequence. The left column shows images collected without very much subject movement, and the right column with severe subject movement. You can see striping throughout the image with the interleaved sequence, but only a single stretched part with the descending sequence.\n",
    "\n",
    "<img src=\"figures/Motion-Artifact.gif\" align=\"left\"  style=\"height: 300px;\"\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scanner Spiking Artifacts\n",
    "\n",
    "Radio-frequency waves are simply waves in the electro-magnetic field at a specific range of frequencies. Thus interference from any source of electrical activity has the potential to interfere with how the scanner **excites** the tissue, which will cause **spiking** artifacts. Electrical interference can be caused by the presence of magnetic materials in the scanner (such as hair-clips, metalic sports-bras and metal dental work), as well as  short-circuits caused by frayed wires, leaking water, and other maintenance issues. These spikes can range from the very subtle, to the extremely obvious. It is common practice to watch the fMRI data as it is being collected at the scanner and take notes about any spiking that is observed, for later removal. Follow [this link](https://practicalfmri.blogspot.com/2012/05/rare-intermittent-epi-artifacts-spiking.html) for more on spiking artifacts.\n",
    "\n",
    "Below is an example of some extreme spiking.\n",
    "\n",
    "<img src=\"https://j.gifs.com/W7O21v.gif\" align=\"left\" style=\"height: 300px;\"\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout Seession\n",
    "1\\. Now we're going to play a \"game\" that involves determining when fMRI data has artifacts or looks good. The researchers behind this game are actually using the general public's input to clean their fMRI data!  Navigate your browser to [braindr](https://braindr.us) to play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motion Correction\n",
    "\n",
    "Humans are not robots, and so they move while in the scanner, even the best trained subjects, and even when using a 'head-case'. We've already seen one way in which motion can cause artifacts when we have an **interleaved sequence**, but there are a couple of other problems that motion causes.\n",
    "\n",
    "Sources of noise being addressed:\n",
    "* After subject moves, the neurons that are being measured in a single voxel are different.\n",
    "* Movement induces warping in the image caused by magnetic field inhomogenities (called **susceptibility artifacts**). \n",
    "\n",
    "The motion correction stage of preprocessing attempts to fix these sources of noise in two ways. First, it finds a transformation (rotation and translation) for each volume to **align** it to the a reference image, usually the first image in the first scan of an experiment. Second, to address warping due to **susceptibility artifacts**, researchers will collect an extra scan called a **field map**. **Field maps** are pictures of the magnetic field that an algorithm can use to try to unwarp these artifacts. \n",
    "\n",
    "Below is an image showing how movement causes the same voxel to contain information about very different bits of tissue.\n",
    "\n",
    "<img src=\"figures/Motion-Correction-1.png\" align=\"left\"  style=\"height: 300px;\"\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we do the motion correction, let's plot a middle axial slice from the first and last volumes of a data set to which we've added extra \"movement\". This will let us see the problem of movement exaggerated. Let's start by loading a scan that hasn't been preprocessed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make a figure with two subplots, side by side in a single row. It will contain images of the middle axial slice from the first and last volume:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plot_slice(data_nopreproc[0], 15, 'axial', title='Raw First Volume', colormap='gray')\n",
    "plt.subplot(1,2,2)\n",
    "plot_slice(data_nopreproc[-1], 15, 'axial', title='Raw Last Volume', colormap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the difference between the first and last volume by subtracting the two volumes and plotting the resulting difference. Let's see that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,5))\n",
    "data_nopreproc_diff = data_nopreproc[0] - data_nopreproc[-1]\n",
    "plot_slice(data_nopreproc_diff, 15, 'axial', title='Difference between first and last volume (Motion Corrected  Data)', colormap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the brain is rotated and translated from the first to the last volume. Let's see what motion correction can do!\n",
    "\n",
    "Motion correction needs a **reference volume**, to which all of the other volumes will be aligned. Usual choices are the first, middle, last or mean volume. Let's select the first volume as the refernce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_volume = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use `nipype` to do the motion correction. The first step to using `nipype` is to create an object that will do the desired preprocessing step. We'll be using `nipype` as a wrapper around `FSL` today, and so we'll use the `MCFLIRT` object, which is FSL's motion correction algorithm. Let's create it and setup some parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_correct = fsl.MCFLIRT()\n",
    "motion_correct.inputs.in_file = filename_nopreproc\n",
    "motion_correct.inputs.ref_vol = reference_volume\n",
    "motion_correct.inputs.save_plots = True\n",
    "motion_correct.inputs.save_rms = True\n",
    "motion_correct.inputs.stats_imgs = True\n",
    "motion_correct._cmd = 'fsl5.0-' + motion_correct._cmd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we simply tell the motion correction algorithm to run.\n",
    "\n",
    "**NOTE:** This will take 10-20 seconds to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_mc = motion_correct.run()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a solid visual idea of what the motion correction algorithm did, let's replot the middle slice from the first and last volumes of the motion corrected data. First we'll load it, getting the filename from the `results_mc` object returned from the motion correction algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_motion = result_mc.outputs.out_file\n",
    "img_motion = nibabel.load(filename_motion)\n",
    "data_motion = img_motion.get_data().T\n",
    "data_motion.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot those slices.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.subplot(1,2,1)\n",
    "plot_slice(data_motion[0], 15, 'axial', title='Motion Corrected First Volume', colormap='gray')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plot_slice(data_motion[-1], 15, 'axial', title='Motion Corrected Last Volume', colormap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare the difference image we saw above, with a new one calculated one the motion corrected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.subplot(1,2,1)\n",
    "data_motion_diff = data_motion[0] - data_motion[-1]\n",
    "plot_slice(data_motion_diff, 15, 'axial', title='Difference volume (Original Data)', colormap='gray')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plot_slice(data_nopreproc_diff, 15, 'axial', title='Difference volume (Motion Corrected  Data)', colormap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks much better!\n",
    "\n",
    "The motion correction algorithm also quantifies how much movement there was across the course of the run. We can use another FSL tool, via `nipype`, to make a nice plot of those so-called **movement parameters** (parameters because many neuroscientists use them as control parameters during the analysis to try and account for the effects of motion). Let's create a motion plotter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the time series showing the estimated rotation\n",
    "plotter = fsl.PlotMotionParams()\n",
    "plotter.inputs.in_file = result_mc.outputs.par_file\n",
    "plotter.inputs.in_source = 'fsl'\n",
    "plotter._cmd = 'fsl5.0-' + plotter._cmd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The motion correction algorithm corrects for movement that is rotation, and that is translation. First, let's tell the plotter to make a plot of the rotation in all three spatial axes `(Z,Y,X)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter.inputs.plot_type = 'rotations'\n",
    "result_rot = plotter.run()\n",
    "\n",
    "file = open(result_rot.outputs.out_file, \"rb\")\n",
    "image = file.read()\n",
    "file.close()\n",
    "Image(value=image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we interpret this plot? What do you see? It looks like there was **HUGE** rotation in the Z plane that progressed linearly from the start of the scan to the end. That's exactly what we added artifically to the data to exaggerate the error caused by motion, and the capacity of motion correction to fix the error.\n",
    "\n",
    "Now let's tell the plotter to plot the translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter.inputs.plot_type = 'translations'\n",
    "result_trans = plotter.run()\n",
    "\n",
    "file = open(result_trans.outputs.out_file, \"rb\")\n",
    "image = file.read()\n",
    "file.close()\n",
    "Image(value=image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what do we see in this translation plot?\n",
    "Likewise, we see a huge translation in the x, and a smaller translation in y axes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout Session\n",
    "\n",
    "1\\. Now practice motion correction on some data that is much more reasonable, a run worth of data that hasn't been rotated to see what that looks like. Give the motion correction object the filename stored in `filename_nopreproc_original` to run the motion correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Now create the plot showing the difference between the first and last volume before and after motion correction for this second run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slice Time Correction\n",
    "\n",
    "We've learned that a volume takes about 2 seconds to collect. A lot happens in the brain in 2 seconds, and so the state of the tissue being recorded in the first axial slice is very different than in the last slice almost 2 seconds later. This introduces noise across voxels, not within a voxel as motion does. When doing an analysis which compares activation across voxels, we need to correct for this difference in time so that we are comparing apples to apples.\n",
    "\n",
    "Source of noise being addressed:\n",
    "* Cross voxel differences in timing\n",
    "\n",
    "There are several ways that researchers have addressed this issue. Traditionally BOLD data would be **interpolated** in time to put all the data on the same time frame. **Interpolation** is an operation that makes an educated guess at what your data looks like at a temporal or spatial point you haven't measured. To do this, it first draws a line (or higher-order polynomial) between the data points you have measured, and then uses the value on that line at the spatial or temporal point you want to guess. See the image below.\n",
    "\n",
    "<img src=\"figures/interpolation.png\" align=\"left\"  style=\"height: 300px;\"\\>\n",
    "\n",
    "The current opinion of many neuroscientists is that this type of slice time correction can cause unwanted side-effects due to the interpolation. More advanced techniques are generally used, but we will review the interpolation method here to give you an idea of the problem and a possible solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data from all the voxels in a given axial slice is collected at the same time - it just becomes less blurry during the milliseconds it takes to collect the data. Before we do the slice time correction, let's look at the time series of several voxels from different axial slices, which we've modified to exagerate the problem that slice timing attempts to solve.\n",
    "\n",
    "Let's use slicing to every other voxel across the Z axis, at a point somewhere in the middle of the brain, excluding the top and bottom 5 slices. Then we'll plot the timeseries for those voxels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_multi_axial = data_nopreproc_corrected[:, 5:-5:, 50, 50]\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "plt.plot(timeseries_multi_axial)\n",
    "_ = plt.title('Axial voxel time series')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a **HUGE** HRF that we've added to the data artificially. Let's zoom in on that region to see if we can notice the shift across alices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_multi_axial = data_nopreproc_corrected[:16, 5:-5:, 50, 50]\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "plt.plot(timeseries_multi_axial, 'x-')\n",
    "_ = plt.title('Axial voxel time series')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's still kinda hard to see the shift. Let's plot them all at the same location by subtracting out the first value of each time series to see if that makes it better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_multi_axial = data_nopreproc_corrected[:16, 5:-5:2, 50, 50]\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "plt.plot(timeseries_multi_axial - timeseries_multi_axial[0], 'x-')\n",
    "_ = plt.title('Axial voxel time series')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice? You can see that the HRF peaks at slightly shifted locations in time (along the x-axis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the slice time correction, and we need to define two parameters to do so (although many more exist). The first is whether the data was collected using an **interleaved** sequence, which we discussed in the section on motion artifacts above. The second is the length of the TR, in seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interleaved = False\n",
    "tr = 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create the slice timer, just as we created the motion correction object, and set the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_timer = fsl.SliceTimer()\n",
    "slice_timer.inputs.in_file = filename_nopreproc_corrected\n",
    "slice_timer.inputs.interleaved = interleaved\n",
    "slice_timer.time_repetition = tr\n",
    "slice_timer._cmd = 'fsl5.0-' + slice_timer._cmd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's simply run the slice time correction.\n",
    "\n",
    "**NOTE:** This takes about 20-30 seconds to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_slicetime = slice_timer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the slice time corrected data, and replot the same voxels to see if they changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_slicetime = result_slicetime.outputs.slice_time_corrected_file\n",
    "img_slicetime = nibabel.load(filename_slicetime)\n",
    "data_slicetime = img_slicetime.get_data().T\n",
    "data_slicetime.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_multi_axial_slicetime = data_slicetime[:, 5:-5:, 50, 50]\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "plt.plot(timeseries_multi_axial_slicetime)\n",
    "_ = plt.title('Axial voxel time series Slice Time Corrected')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout Session\n",
    "1\\. The cell above this created a plot of the slice time corrected data across all of time. Now create a second line plot that compares just the first 16 TRs, as we did for the data before slice timing. How does it look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "#Now they look almost identical!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Filtering (Detrending)\n",
    "\n",
    "Within the BOLD signal there are several sources of noise at low frequencies (~100 second or more). There are several ways to remove these unwanted signals, one of which is **temporal filtering**. **Temporal filtering** is a technique which elimantes all low-frequency signals below a given frequency, in this case 1/100 **hertz** (cycles/per second). Any **complex signal** (such as a single voxel's BOLD timeseries) can be broken down into it's composite **simple signals**, each of which is a sine wave at a specific frequency. The image below shows 2 simple waves (in blue and red), and the resulting complex wave (in green) created by adding them together. \n",
    "\n",
    "<img src=\"figures/Frequencies.gif\" align=\"left\"  style=\"height: 300px;\"\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Temporal filtering** uses a process known as a **Fourier transformation** to break down a complex wave into its composite simple waves. It then eliminates all the simple waves below a given frequency and reassembles a new complex wave from the remaining simple waves. This new complex wave has no simple wave components below the given frequency, or **threshold**. This type of filtering is know as **high-pass filtering** because it lets only the high-frequency waves \"pass-through\" into the new wave.\n",
    "\n",
    "Researchers use temporal filtering to remove low-frequency waves that result primarily from **scanner drift**, which is a term which refers to an increase in the BOLD signal over time that is caused by the superconducting magnet's field drifting over time. You can think of the MRI machine as needing to \"warm up\" before it delivers consistent data. Unfortunately for us, that warm-up period is often longer than an entire scan (or run).\n",
    "\n",
    "The image on the left shows a voxel timeseries with severe scanner drift, and on the right after it's been corrected using a high-pass filter.\n",
    "<img src=\"figures/Temporal-Detrending.png\" align=\"left\"  style=\"height: 300px;\"\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signal Frequency Basics\n",
    "\n",
    "Before applying high-pass filtering to real fMRI data, let's get a better grasp of how simple waves combine to form complex waves, and how high-pass filtering can be used to remove the low fequency simple waves.\n",
    "\n",
    "First, let's create some data that have some simple waves at various frequencies. To do that we're going to need a **helper function**. Don't worry about the code inside, we'll just be concerned with using the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sine(sinefreq):\n",
    "    fs = 200 # 200 frames per second\n",
    "    duration = 2 #seconds\n",
    "    T = duration\n",
    "    nsamples = fs * T\n",
    "    w = 2. * np.pi * sinefreq\n",
    "    t_sine = np.linspace(0, T, nsamples, endpoint=False)\n",
    "    y_sine = np.sin(w * t_sine)\n",
    "    return y_sine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the above function to create data at 3 frequencies: 1, 5 and 10 hertz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplewave_1hz = create_sine(1)\n",
    "simplewave_2hz = create_sine(2.5)\n",
    "simplewave_10hz = create_sine(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot these three waves as time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "plt.plot(simplewave_1hz, label='1hz')\n",
    "plt.plot(simplewave_2hz, label='2.5hz')\n",
    "plt.plot(simplewave_10hz, label='10hz')\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add all these simple waves together to create a complex wave. To do this we simply add the arrays together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complexwave = simplewave_1hz + simplewave_2hz + simplewave_10hz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 5))\n",
    "_ = plt.plot(complexwave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply a highpass filter to this complex wave. We'll need another helper function, and again don't worry about the details of this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "def highpass_filter(data, cutoff, order=5):\n",
    "    fs = 200 # 200 frames per second\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = signal.butter(order, normal_cutoff, btype='high', analog=False)\n",
    "    y = signal.filtfilt(b, a, data)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since highpass filters remove the low-frequency components of the signal and keep the high frequency components of the signal, let's try and recover just the highest signal in the complex wave, the 10hz signal. To do that, we'll set the threshold to halfway between the frewquency to keep, and the frequency to filter out. In this case we want to keep the 10hz signal, and filter out the 1hz and 2.5hz frequencies, so we'll set the threshold at 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highpass_filtered_wave_10 = highpass_filter(complexwave, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's plot it to see if it worked. We'll plot both the original 10hz signal, and the new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "plt.plot(simplewave_10hz, label='Original 10HZ')\n",
    "plt.plot(highpass_filtered_wave_10, label='Filtered')\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks pretty good! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout Session\n",
    "\n",
    "1\\. What would happen if you high pass filtered the complex wave using a much lower frequency threshold? Or a much higher threshold? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal filtering of FMRI Data\n",
    "\n",
    "Now let's apply the same principle to fMRI data to remove the **scanner drift**. First let's plot a time series from a single voxel so we can get idea of what the drift looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "plt.plot(data_slicetime[:,15,50,50])\n",
    "_ = plt.title('Voxel Before Temporal Filtering')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do the temporal filtering we'll use FSL and `nipype` again, just to be consistent with the previous preprocessing steps. We could also do this manually using the `highpas_filter` function from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the temporal filtering object\n",
    "temporal_filter = fsl.TemporalFilter()\n",
    "temporal_filter.inputs.in_file = filename_slicetime\n",
    "temporal_filter.inputs.highpass_sigma = 18.0 # 100sec filter\n",
    "temporal_filter._cmd = 'fsl5.0-' + temporal_filter._cmd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now run the temporal filtering.\n",
    "\n",
    "**NOTE:** This takes 10 or 15 seconds to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_tempfilt = temporal_filter.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the resulting temporally filtered data so we can plot it. Since temporal filtering removes the mean when doing a highpass filter, we must add the mean back into the data in order for plots to look correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_tempfilt = result_tempfilt.outputs.out_file\n",
    "img_tempfilt = nibabel.load(filename_tempfilt)\n",
    "data_tempfilt = img_tempfilt.get_data().T + data_slicetime.mean(axis=0)\n",
    "data_tempfilt.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the same sample time series as we plotted above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_slicetime = data_slicetime[:,15,50,50]\n",
    "timeseries_tempfilt = data_tempfilt[:,15,50,50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's put the two time series on the same plot so we can see how they've changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "plt.plot(timeseries_slicetime, label='Original')\n",
    "plt.plot(timeseries_tempfilt, label='Temporally Filtered')\n",
    "plt.legend()\n",
    "plt.title('Original vs Temporally Filtered Time Series')\n",
    "plt.xlabel('Time (TRs)')\n",
    "_ = plt.ylabel('BOLD Signal (a.u.)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakout Session\n",
    "\n",
    "1\\. Try changing the sigma value above by using a lower and higher sigma (5 & 50) and replot the timeseries. See how the signal changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Filtering (Smoothing) \n",
    "\n",
    "The scanner contains several sources of noise, one of which results in independent Gaussian (or normally distributed, or the bell curve) noise across all the voxels. One way to reduce this type of noise is to apply what's called a Gaussian filter across the BOLD data in the spatial dimensions. \n",
    "\n",
    "While spatial filtering has the upside of removing some of the scanner-induced Gaussian noise, there are some downsides:\n",
    "* Reduces spatial resolution\n",
    "* Can reduce signal if the activations are small in size\n",
    "\n",
    "For these reasons we will not be using spatially filtered data for the remainder of the class. \n",
    "\n",
    "The details of spatial filtering are complex and beyond the scope of this course, but we can get an intuitive feel for what spatial filtering does and what its downsides are. We will do this here. Below is a series of images of Einstein that give a visual depiction of what spatial filtering does. \n",
    "\n",
    "The image on the left is the original image, the image in the middle has had Gaussian noise added to it, and the image on the right has undergone spatial filtering using a Gaussian filter. Notice that while the noise has been removed from the image on the right, it also looks more blurry because its spatial resolution has been reduced by the filtering. For this reason spatial filtering is sometimes also called smoothing.\n",
    "\n",
    "<img src=\"figures/Einstein.jpg\" align=\"left\"  style=\"height: 300px;\"\\>\n",
    "\n",
    "<img src=\"figures/Einstein_Noise.jpg\" align=\"left\"  style=\"height: 300px;\"\\>\n",
    "\n",
    "<img src=\"figures/Einstein_Denoised.jpg\" align=\"left\"  style=\"height: 300px;\"\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Filter Intuition\n",
    "\n",
    "Let's get a better intuition for what Gaussian noise looks like when added to fMRI data, and when it's removed.\n",
    "\n",
    "First, we'll extract an axial slice from the slice time data to apply noise to. Let's have a look at it before we do anything to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_axial_tempfilt = get_slice(data_tempfilt[0], 15, 'axial')\n",
    "plt.imshow(slice_axial_tempfilt, cmap='gray')\n",
    "_ = plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add some Gaussian noise to this slice. It turns out you actually already know how to create Gaussian noise! The random data we've been created when using `np.random.randn` is actually Gaussian. The \"n\" in `randn` stands for **standard normal**, which is a name for Gaussian data that has a mean of `0` and a standard deviation of `1`. So let's use `np.random.randn` to create some Gaussian noise and add it to the slice. We want to multiply that noise by some fraction of the max value in the image, since the normal data has very small magnitudes. Let's choose to add noise that is 20% of the max value of the slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value = slice_axial_tempfilt.max()\n",
    "slice_dim = slice_axial_tempfilt.shape[0]\n",
    "noise = np.random.randn(slice_dim, slice_dim) * (0.2 * max_value)\n",
    "slice_axial_tempfilt_noise = slice_axial_tempfilt + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the two images side by side to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(11,5))\n",
    "plt.subplot(1,2,1)\n",
    "_ = plt.imshow(slice_axial_tempfilt, cmap='gray')\n",
    "plt.subplot(1,2,2)\n",
    "_ = plt.imshow(slice_axial_tempfilt_noise, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's definitely noticeable, it looks like static on old-school TVs. \n",
    "\n",
    "Now that we've seen what somewhat extreme noise looks like, let's remove it using spatial filtering. To do so we'll use `gaussian_filter`. It takes the data to use, and the size of the filter, which we'll set to 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage.filters import gaussian_filter\n",
    "slice_axial_tempfilt_filtered = gaussian_filter(slice_axial_tempfilt_noise, 4)\n",
    "\n",
    "slice_axial_tempfilt_filtered_no_noise = gaussian_filter(slice_axial_tempfilt, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot all 3 images to see the full effects of adding noise and filtering it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(11,11))\n",
    "plt.subplot(2,2,1)\n",
    "plt.imshow(slice_axial_tempfilt, cmap='gray')\n",
    "plt.subplot(2,2,2)\n",
    "plt.imshow(slice_axial_tempfilt_noise, cmap='gray')\n",
    "plt.subplot(2,2,4)\n",
    "plt.imshow(slice_axial_tempfilt_filtered, cmap='gray')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "_  = plt.imshow(slice_axial_tempfilt_filtered_no_noise, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(slice_axial_tempfilt - slice_axial_tempfilt_noise), np.linalg.norm(slice_axial_tempfilt_filtered - slice_axial_tempfilt_filtered_no_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout Session\n",
    "1\\. Play with the size of the filter. What does increasing the size do? And decreasing it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Play with adding more noise and using the same size filter. Does it make the end result better? Or worse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Filtering fMRI Data\n",
    "\n",
    "Let's apply spatial filtering to our real fMRI data. First we need to choose the parameters for spatial smoothing. As we just saw, we must choose the size of the **Gaussian filter** to use. This value is chosen based on the spatial size of the activation you expect to see. The standard is to use ~8mm for FWHM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the input parameters\n",
    "brightness_threshold = 2000.0\n",
    "fwhm = 8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create the smoothing algorithm using `nipype` and FSL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoother = fsl.SUSAN()\n",
    "smoother.inputs.in_file = result_slicetime.outputs.slice_time_corrected_file\n",
    "smoother.inputs.brightness_threshold = brightness_threshold\n",
    "smoother.inputs.fwhm = fwhm\n",
    "smoother._cmd = 'fsl5.0-' + smoother._cmd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the smoothing algorithm.\n",
    "\n",
    "**NOTE:** This takes 20-30 seconds to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_spatfilt = smoother.run() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the smoothed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_spatfilt = result_spatfilt.outputs.smoothed_file\n",
    "img_spatfilt = nibabel.load(filename_spatfilt)\n",
    "data_spatfilt = img_spatfilt.get_data().T\n",
    "data_spatfilt.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the middle axial slice of the first volume in the data before and after smoothing to see how it changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(11,5))\n",
    "plt.subplot(1,2,1)\n",
    "plot_slice(data_slicetime[0], 15, 'axial', colormap='gray', title='Before Spatial Smoothing')\n",
    "plt.subplot(1,2,2)\n",
    "_ = plot_slice(data_spatfilt[0], 15, 'axial', colormap='gray', title='After Spatial Smoothing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakout Session\n",
    "\n",
    "1. Change the `fwhm` to 25 and rerun. Then replot the first volume and see how much 'smoother' it looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Normalization (Z-Score)\n",
    "\n",
    "Within statistics and signal processing the term **normalization** refers to a number of different computations that \"adjust values measured on different scales to a notionally common scale\". That \"notionally common scale\" can be the range between zero and one, for example, meaning the maximum value in the data becomes `1` and the minimum value in the data becomes `0`. **Normalization** is done for a number of reasons ranging from simply making data more interpretable, to removing irrelevant differences in signals to allow for their joint analyses. For more on **normalization** see <a href=\"https://en.wikipedia.org/wiki/Normalization_(statistics)\">here</a>.\n",
    "\n",
    "**Temporal normalization** is simply the process of normalization applied to data in the time domain. In the case of fMRI, that means normalizing each voxel time series, across all the runs of a subject so they are on the same scale. \n",
    "\n",
    "We'll cover two types of **normalization** today:\n",
    "\n",
    "1\\. **Feature Scaling:** Puts all the data into the range [0-1] (the brackets mean inclusive). <br/>Formula:\n",
    "\n",
    "\\begin{align}\n",
    "X^{\\prime} & = \\frac{X - X_{min}}{X_{max}-X_{min}}\n",
    "\\end{align}\n",
    "where $X^{\\prime}$ is the feature scaled data, $X$ is the original data, $X_{min}$ is the minimum value of the data, and $X_{max}$ is the maximum value of the data.\n",
    "\n",
    "2\\. **Standard Scoring (Z-Scoring):** Makes the mean of the data equal `0` and the standard deviation of the data equal `1`. <br/>Formula:\n",
    "\\begin{align}\n",
    "Z & = \\frac{X - \\mu}{\\sigma}\n",
    "\\end{align}\n",
    "where $X$ is the data, $\\mu$ is the mean of the data, and $\\sigma$ is the **standard deviation** of the data.\n",
    "\n",
    "Let's get familiar with **normalization** by exploring **feature scaling** one step at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Feature scaling is perhaps the simplest form of **normalization**, and is conceptually easy to understand. The idea is to move all of the data to a known range, in this case from [0-1]. Let's create a toy array of random data to practice with, then plot it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_random = np.random.randn(20)\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "_ = plt.plot(array_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of **feature scaling** is to remove the minimum value from the data. Let's see what that does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_random_offset = array_random - array_random.min()\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "_ = plt.plot(array_random_offset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huh, the plot looks the same, so what's the difference? Removing the minimum value does not change the shape of the data, it simply **offsets** (or shifts along the y-axis) the data, making the new minimum value equal to zero. So we can see the change only in the values of the y-axis. Notice the lowest value is now `0`, when it was around `-1` before.\n",
    "\n",
    "The second step of **feature scaling** is to divide all the data by the new maximum value. Let's do that here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_random_featurescaled = array_random_offset / array_random_offset.max()\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "_ = plt.plot(array_random_featurescaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What changed this time? Again, the plot looks the same, only the scale of the data (on the y-axis) has changed. By dividing the offset data by it's new max value, we've **scaled** the data so it's new maximum is `1`. That's true because dividing a number by itself always equals one, so dividing all the numbers by the maximum means the new maximum has to be `1`. Also, since the new minimum was `0`, dividing `0` by any number always returns zero. So now our data is guaranteed to be on the range [0-1]!\n",
    "\n",
    "Important to note is that **normalization** does not change the shape of your data, it simply applies an **offset**, and then **scales** the data.\n",
    "\n",
    "We can combine both the offset and scaling into a single line of code for conveneince. Let's remind ourselves of the equation for features scaling first:\n",
    "\n",
    "\\begin{align}\n",
    "X^{\\prime} & = \\frac{X - X_{min}}{X_{max}-X_{min}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_random_featurescaled2 = (array_random - array_random.min()) / (array_random.max() - array_random.min())\n",
    "\n",
    "print(array_random_featurescaled)\n",
    "print(array_random_featurescaled2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values from both ways of feature scaling look the same, but the code this new way looks a little different than when we did it the first time. That's because in the first way we did it, we used the maximum value from the array that already had the minimum removed. Since the new way uses the original `array_random` everywhere, we have to remove the minimum from the maximum explicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout Session\n",
    "\n",
    "1\\. Create a helper function to do feature scaling. Call the function `feature_scaling_simple`. It will have a single argument, the array to feature scale called `data`, and will return the feature scaled array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Now update the function you just created by adding a second argument that designates which axis the feature scaling should be done across. Call the second argument `dim` and make it default to `0`, and call the function `feature_scaling`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Finally, create a 2-D array of random numbers with shape `(4,8)`, call it `array_random2D`. Now use the two versions of the feature scaling function on it. Do you expect the same answer, or different answers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems with Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature scaling** is very useful to put data into an easily interpretable scale, however a problem with this normalization method shows up when you have outlying values far way from the mean. What would happen if we add a very large outlying value at a single time point? Let's see!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_random_long = np.random.randn(100)\n",
    "array_random_outlier = copy.copy(array_random_long)\n",
    "array_random_outlier[80] = array_random_outlier.max()*20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize what's happening in these data sets we'll use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL IF YOU DIDN'T GET THE ANSWER IN THE BREAKOUT SESSION ABOVE\n",
    "\n",
    "# def feature_scale(data, dim=0):\n",
    "#     \"\"\"Normalize data to range of 0-1 by subtracting min, dividing by range\"\"\"\n",
    "#     data_norm = (data - data.min(axis=dim, keepdims=True)) / (data.max(axis=dim, keepdims=True) - data.min(axis=dim, keepdims=True))\n",
    "#     return data_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_random_stacked = np.stack((feature_scale(array_random_long), feature_scale(array_random_outlier)), axis=1)\n",
    "\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(array_random_stacked, 10)\n",
    "_ = plt.legend(('Random Array', 'Random Array w/Outlier'))\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(array_random_stacked)\n",
    "_ = plt.legend(('Random Array', 'Random Array w/Outlier'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the histogram, the array with the outlier has most of its data lying in a single bin of the histogram. Feature scaling does not put the data from both arrays into a similar range, because the max value is not stable (it can change a lot depending on only one data point). As we've learned, (linear) data normalization involves an *offset* operation (by subtracting off some value) and a **scaling** operation (by dividing by some value or performing some nonlinear operation). \n",
    "\n",
    "A more robust and stable way to normalize data is to subtract the **mean** of the data instead of the min, and to divide by the **standard deviation** instead of the range (max - min). As was outlined above, this is exactly what z-scoring does. Let's learn more about it now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Scoring (Z-Scoring)\n",
    "\n",
    "One of the most commonly used forms of **normalization** is to creating standard scores, or z-scores. A z-scored dataset has a mean of 0, and a standard deviation of 1. So what's a standard deviation? Let's find out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Deviation\n",
    "\n",
    "**Standard deviation** is a descriptive statistic that measures the average distance away from the mean (in either direction) of a data set. When a data set has a small standard deviation, the data points tend to be close to the mean, when the standard deviation is large the data points tend to be far from the mean. \n",
    "\n",
    "Before we jump into the equation for the standard deviation, let's get a visual intuition for what the standard deviation looks like by plotting the distribution of some random Gaussian data. We learned earlier today that `np.random.randn` actually gives us **standard normal** data, whose **distribution** is Gaussian (shaped like the Bell curve), and whose mean is `0` and standard deviation is `1`. We'll plot the data using a histogram, since that plots the **distribution** of the data. Then we'll draw a line for the mean, and two lines for standard deviation on either side of the mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stddev = np.random.randn(1000)\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "_ = plt.hist(data_stddev, bins=20)\n",
    "_ = plt.plot([data_stddev.mean(), data_stddev.mean()], [0,150], label='Mean')\n",
    "_ = plt.plot([-data_stddev.std(), -data_stddev.std()], [0,150], 'r', label='Standard Deviation')\n",
    "_ = plt.plot([data_stddev.std(), data_stddev.std()], [0,150], 'r', label='Standard Deviation')\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at how to calculate the standard deviation. The formula is a bit complicated-looking, but we'll work through it step by step to see it's actually not that bad.\n",
    "\n",
    "$$\\hat{\\sigma} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (x_i - \\overline{x})^2}$$\n",
    "\n",
    "where $\\hat{\\sigma}$ is the estimated standard deviation, $N$ is the number of samples in your data set, $x_i$ is the ith data point, and $\\overline{x}$ is the mean of the data set.\n",
    "\n",
    "When tryring to understand a formula, it can be useful to pick it apart starting with the innermost calculation, and work outwards. Let's do that here, starting with:\n",
    "$$\\sum_{i=1}^N (x_i - \\overline{x})^2$$\n",
    "\n",
    "That Greek letter ($\\sum$) is an uppercase \"sigma\" and stands for the `sum` operation. In this case it means to sum all that's in the parentheses, across all the $x_i$ values. So this expression is saying, for each data point:\n",
    "1. Subtract the mean\n",
    "2. Square that difference\n",
    "\n",
    "And then sum all those squared differences. This sum is calculated many places in statistics, and is called the **sum of squared differences**, or **sum of squares** for short. Let's do that here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = array_random\n",
    "differences = x - x.mean()\n",
    "differences_squared = differences**2\n",
    "sum_squares = differences_squared.sum()\n",
    "sum_squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can put that all into a single line of code like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_squares = np.sum((x - x.mean())**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was really the hard part! Now we take that value, divide it by $N$, and take the square root:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = x.shape[0]\n",
    "std_dev = np.sqrt(sum_squares / N)\n",
    "std_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard deviation is such a common operation that there is a function for it, `std`. Let's verify we got the same answer as the numpy function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout Session\n",
    "\n",
    "1\\. Let's look back at the function we used during the review to calculate noise in a volume. Describe the 3 calculations this function makes to calculate the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_noise(data):\n",
    "    \n",
    "    # determine the voxels that are outside the brain, \n",
    "    # assuming their value is around 300 in the first volume\n",
    "    outside_brain_mask = data[0] <= 300\n",
    "    \n",
    "    # get all the voxels that are not in the brain, for each time point\n",
    "    outside_brain_vox = data[:,outside_brain_mask]\n",
    "    \n",
    "    # find the standard deviation of the voxels outside the brain, for\n",
    "    # each volume across time\n",
    "    data_std = np.std(outside_brain_vox, axis=0)\n",
    "    \n",
    "    # take the average of all of those standard deviations\n",
    "    noise = np.mean(data_std)\n",
    "    return noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Z-Scores\n",
    "\n",
    "Now that we know what a standard deviation is, we can calculate z-scores. Remember that z-scoring makes the mean of the data `0`, and the standard deviation of the data `1`. To do that we'll use the same logic as we did with **feature scaling**. To make the mean zero, we'll **offset** (or subtract) the data by the mean. Then to make the standard deviation `1`, we'll divide that by the standard deviation. Here's the equation, which is actually very simple:\n",
    "\n",
    "\\begin{align}\n",
    "Z & = \\frac{X - \\mu}{\\sigma}\n",
    "\\end{align}\n",
    "\n",
    "Let's create z-scores from the original array of random data `array_random` here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_random_zscore = (array_random - array_random.mean()) / array_random.std()\n",
    "array_random_zscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There is also a function to do z-scoring for us already in python, it is called `zscore` and is in the module `scipy.stats`. Let's import it and use it to verify that our answer is correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "array_random_zscore2 = zscore(array_random, axis=0)\n",
    "print(array_random_zscore)\n",
    "print(array_random_zscore2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup, we did it right!\n",
    "\n",
    "Now let's see how z-scoring changed the data by creating line plots like we did when **norming** (doing normalization) with **feature scaling** above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "_= plt.plot(array_random, label='Original')\n",
    "_= plt.plot(array_random_zscore, label='Z-Scored')\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the random data array we created was actualy drawn from a **standard normal** distribution it's mean was already almost `0` and it's standard deviation almost `1`, so z-scoring didn't change things that much here! Let's move on to some real fMRI data where that won't be the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout session\n",
    "\n",
    "1\\. Z-score the 2-D `array_random2D` across rows (meaning z-score each column indepependently), and do the calculations manually (don't use the `zscore` function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z-Scoring fMRI data\n",
    "\n",
    "We've learned that values recorded by fMRI data is recorded in arbitrary units, and about issues such as **scanner drift**. For both of these reasons, and others, the raw values of fMRI BOLD data can vary drastically between different runs on the same subject in the same day. Another way to say this, is that we're really just concerned with relative differences in the BOLD signal, and not the absolute differences. **Temporal normalization** is often used to remove these meaningless differences is signal between scans of the same subject. Z-scoring is the most common form of this.\n",
    "\n",
    "Let's load 2 more scans worth of data from this subject in order to practice z-scoring with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_preproc2 = nibabel.load(\"/data/cogneuro/fMRI/categories/s01_categories_02.nii.gz\")\n",
    "data_preproc2 = img_preproc2.get_data().T\n",
    "img_preproc3 = nibabel.load(\"/data/cogneuro/fMRI/categories/s01_categories_03.nii.gz\")\n",
    "data_preproc3 = img_preproc3.get_data().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate z-scoring on real fMRI data we'll extract a single voxel time series from all 3 scans. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries1 = data_preproc[:, 6, 57, 37]\n",
    "timeseries2 = data_preproc2[:, 6, 57, 37]\n",
    "timeseries3 = data_preproc3[:, 6, 57, 37]\n",
    "timeseries3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will contatenate these time series together to have a lot of data for analysis. We'll use `np.concatenate` to make a single 1-D array of the 3 time series in a row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_total = np.concatenate((timeseries1, timeseries2, timeseries3), axis=0)\n",
    "timeseries_total.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start the z-scoring process, let's plot the data to see why we need to z-score in the first place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "_ = plt.plot(timeseries_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The signals of each run seem to vary around very different mean values! Let's visualize this by drawing some lines into the plot that represent the mean of the 3 scans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 6))\n",
    "plt.plot(timeseries_total)\n",
    "mean1 = timeseries1.mean()\n",
    "mean2 = timeseries2.mean()\n",
    "mean3 = timeseries3.mean()\n",
    "timeseries_len = timeseries1.shape[0]\n",
    "plt.plot([0, timeseries_len], [mean1, mean1])\n",
    "plt.plot([timeseries_len, timeseries_len*2], [mean2, mean2])\n",
    "_ = plt.plot([timeseries_len*2, timeseries_len*3], [mean3, mean3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take the first stage of z-scoring, to subtract the mean. We'll be z-scoring each timeseries separately, so we'll subtract the mean of each timeseries separately here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries1_nomean = timeseries1 - mean1\n",
    "timeseries2_nomean = timeseries2 - mean2\n",
    "timeseries3_nomean = timeseries3 - mean3\n",
    "timeseries_total_nomean = np.concatenate((timeseries1_nomean, timeseries2_nomean, timeseries3_nomean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the total time series with the means removed from the 3 time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "_ = plt.plot(timeseries_total_nomean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total time series now looks a lot more similar. There could still be differences in the average amount of difference from the means (which is the standard deviation!), however. Let's calculate the standard deviations and plot them usnig colored strips that center on the mean. We'll use a different color for each scan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdev1 = np.std(timeseries1_nomean)\n",
    "stdev2 = np.std(timeseries2_nomean)\n",
    "stdev3 = np.std(timeseries3_nomean)\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(timeseries_total_nomean)\n",
    "plt.fill_between([0, timeseries_len], [-stdev1, -stdev1], [stdev1, stdev1], alpha=.3)\n",
    "plt.fill_between([timeseries_len, timeseries_len*2], [-stdev2, -stdev2], [stdev2, stdev2], alpha=.3)\n",
    "_ = plt.fill_between([timeseries_len*2, timeseries_len*3], [-stdev3, -stdev3], [stdev3, stdev3], alpha=.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the heights of the 3 colored strips are different, which indicates the standard deviation differs across the 3 runs. Let's complete the z-scoring process and divide each scan's time series by the standard deviation, separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries1_zscore = timeseries1_nomean / stdev1\n",
    "timeseries2_zscore = timeseries1_nomean / stdev2\n",
    "timeseries3_zscore = timeseries1_nomean / stdev3\n",
    "timeseries_total_zscore = np.concatenate((timeseries1_zscore, timeseries2_zscore, timeseries3_zscore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we'll plot the z-scored time series along with the original concatonated time series so we can see how z-scoring changed the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "plt.subplot(2,1,1)\n",
    "_ = plt.plot(timeseries_total)\n",
    "_ = plt.title('Z-Original')\n",
    "plt.subplot(2,1,2)\n",
    "_ = plt.plot(timeseries_total_zscore)\n",
    "_ = plt.title('Z-Scored')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap: Preprocessing Steps\n",
    "\n",
    "While the goal of preprocessing remains the same for all researchers, to improve SNR, the specific steps chosen can vary. Additionally the order in which they are executed and the algorithms used to do each step can vary as well. Below is an outline of the most common preprocessing steps with a brief description of each. We will cover each of them in more detail below, except the conversion to NIFTI, since we have already discussed this in a previous lecture. \n",
    "\n",
    "1. **Conversion from Dicom to NIFTI**: Convert the MRI output Dicom file format into the NIFTI neuroimaging file format for use in the analyses packages mentioned above. \n",
    "2. **Remove Scanner Artifacts**: The scanner is a complex piece of machinery that does have occasional problems. Examining and removing data that contains \"scanner artifacts\" created by these occasional problems is important.\n",
    "3. **Motion Correction**: The subject moves during every scan, even the best trained subjects. Motion correction is the process of rotating and translating each volume of functional data so it is in the exact same place across every scan.\n",
    "5. **Slice Time Correction**: It takes ~2 seconds to collect an entire functional image of the brain, and a lot happens in the brain in that time period. Slice time correction is the process of resampling the data so that every voxel in a given volume represents the exact same point in time. \n",
    "6. **Temporal Filtering (High-Pass Filter)**: The scanner and subject are both prone to cyclical changes that can cause the signal we record to by distorted at a very low frequency (such as breathing and \"scanner drift\"). Temporal filtering removes those low-frequncy noise signals.\n",
    "7. **Spatial Filtering (Smoothing)**: When combining data across subjects, using a spatial filter can improve SNR if we assume that the signal we are looking for has the same properties as the spatial filter we use.\n",
    "8. **Temporal Normalization (Z-Score)**: The values in our fMRI data can vary in their mean and variance across runs due to processess unimportant to our analyses. It is important then to control for these differences by z-scoring each run individually.\n",
    "\n",
    "\n",
    "<img src=\"figures/fmri-preprocessing-steps-in-spm8-3-638.jpg\" align=\"left\" style=\"height: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For group statistics\n",
    "\n",
    "In addition to the preprocessing steps just mentioned, there are 2 additional steps that most researchers do in order to facilitate group level analyses, which are statistics computed across the entire group of subjects. We will not be doing either of these steps in this class, but it is important to know about these two steps since many studies use them.\n",
    "\n",
    "1. **Registration (Functional to anatomical)**: This step is similar to motion correction, as it is finding a rotation and translation of the functional data so that it is in the same place in 3D space as the anatomical scan. This is a necessary first step for **Spatial Normalization**.\n",
    "2. **Spatial Normalization (Single subject to standard 3D space)**: In order to do statistics across all the subjects, the data needs to be in the same 3D space. This is tricky because every brain is shaped slightly different. The process of spatial normalization finds a very complex (non-linear) transformation that maps every voxel in the subjects' anatomical scans into a standard 3D space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
