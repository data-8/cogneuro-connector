{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 9: Models: Simple Linear Regression and Multiple Regression\n",
    "\n",
    "## Goals\n",
    "- **Neuroscience / Neuroimaging concepts**\n",
    "    - How to model and estimate voxel responses to different conditions\n",
    "    - Simulating a signal\n",
    "    - Design Matrix\n",
    "- **Datascience / Coding concepts**\n",
    "    - Statistical Models\n",
    "    - Simple Linear Regression\n",
    "    - Cost functions: Sum of Squared Errors (SSE)\n",
    "    - Relationship between correlation and regression\n",
    "    - Multiple Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Simply run the cells below that contain all the Python modules we'll neeed, plus setup matplotlib for plotting in this jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats\n",
    "from scipy.stats import zscore\n",
    "import nibabel\n",
    "import cortex\n",
    "import os\n",
    "from nistats.hemodynamic_models import glover_hrf as create_hrf\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, widgets, FloatSlider\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set plotting defaults\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data\n",
    "Run the cell below to download files needed for this lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "zname, _ = urllib.request.urlretrieve(\"https://berkeley.box.com/shared/static/hcu0khw2spowgd1winlfi4f3si7lk1ua.zip\", \n",
    "                                      \"/home/jovyan/pedagogy_workshop_data.zip\")\n",
    "zfile = zipfile.ZipFile(zname)\n",
    "zfile.extractall(\"/home/jovyan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "Define a few helper functions that we'll use in this lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nifti(file_name, mask=None):\n",
    "    img = nibabel.load(file_name)\n",
    "    data = img.get_data().T\n",
    "    if mask is None:\n",
    "        masked_data = data\n",
    "    else:\n",
    "        masked_data = data[:, mask]\n",
    "    return masked_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_line_product(x, y, label_x='X', label_y='Y'):\n",
    "    plt.plot(x, label=label_x)\n",
    "    plt.plot(y, label=label_y)\n",
    "    plt.bar(np.arange(len(x)), x*y, label=\"Product\")\n",
    "    _ = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_error_time(x, y, slope, intercept):\n",
    "    \"\"\"Plots the whole thing as a time series instead of a point cloud\"\"\"\n",
    "    n = x.shape[0]\n",
    "    plt.plot(y, 'b')\n",
    "    plt.hlines([0], x.min(), x.max())\n",
    "    num_x = 1 if x.ndim == 1 else x.shape[1]\n",
    "    t = np.arange(n)\n",
    "    new_x = np.zeros((n,))\n",
    "    for cur_x in range(num_x):\n",
    "        cur_x = (intercept + slope[cur_x] * x[:,cur_x])\n",
    "        new_x += cur_x \n",
    "    plt.plot(new_x, 'r', lw=1)\n",
    "    plt.fill_between(t, new_x, y[:,0], color='g', alpha=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_error_scatter(x, y, slope, intercept):\n",
    "    x = x[:,0]\n",
    "    y = y[:,0]\n",
    "    x_left = x.min() - x.ptp() * .2\n",
    "    x_right = x.max() + x.ptp() * .2\n",
    "    y_est = slope * x + intercept\n",
    "    y_left, y_right = slope * x_left + intercept, slope * x_right + intercept\n",
    "    y_bottom, y_top = y.min() - y.ptp() * .2, y.max() + y.ptp() * .2\n",
    "    xs = np.vstack([x, x])\n",
    "    bars = np.vstack([y, y_est])\n",
    "    plt.hlines([0], x_left, x_right)\n",
    "    plt.vlines([0], y_bottom, y_top)\n",
    "    plt.scatter(x, y)\n",
    "    plt.plot([x_left, x_right], [y_left, y_right], 'r', lw=2)\n",
    "    plt.plot([0, 0], [0, intercept], 'm', lw=2)\n",
    "    plt.plot(xs, bars, 'g-.')\n",
    "    plt.axis([x_left, x_right, y_bottom, y_top])\n",
    "    return ((y_est - y) ** 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "def show_error_both_ways(x, y, slope, intercept):\n",
    "    if x.ndim == 1:\n",
    "        x = x.reshape(len(x),1)\n",
    "    if y.ndim == 1:\n",
    "        y = y.reshape(len(y),1)\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    gs = gridspec.GridSpec(1, 5)\n",
    "    ax = plt.subplot(gs[0, 0:2])\n",
    "    e = show_error_scatter(x, y, slope, intercept)\n",
    "    ax = plt.subplot(gs[0, 2:])\n",
    "    show_error_time(x, y, [slope], intercept)\n",
    "    print('Error: %.04f' % (e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation\n",
    "\n",
    "Last week we learned that correlation is a statistic that can be calculated to quantify the similarity of two random variables, and there are multiple ways to think about what exactly it does. We learned three:\n",
    "* Correlation is the slope of a line that best reduces the squared error between z-scored data in a scatter plot.\n",
    "* Correlation measures the extent to which two variables change together away from their respective means. \n",
    "* Correlation is the average product of the standardized scores of two variables.\n",
    "\n",
    "Let's create some \"fake\" data and then review these three ways to think about correlation. First we'll create some constants that we can change later that adjust the size of the fake data and how correlated the two vectors are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_CORRELATION = 0.6\n",
    "FAKE_N = 50\n",
    "MEAN_1 = 0\n",
    "MEAN_2 = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the fake data with the means and correlation specified above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_data = np.random.multivariate_normal([MEAN_1,MEAN_2], [[1,TARGET_CORRELATION],[TARGET_CORRELATION,1]],FAKE_N)\n",
    "fake_x = fake_data[:,0]\n",
    "fake_y = fake_data[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put that into a function since we'll use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fake_data(n, mean1, mean2, corr):\n",
    "    fake_data = np.random.multivariate_normal([mean1,mean2], [[1,corr],[corr,1]],n)\n",
    "    fake_x = fake_data[:,0]\n",
    "    fake_y = fake_data[:,1]\n",
    "    return (fake_x, fake_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We learned we can use the `np.corrcoef` function to compute the correlation between two variables. Let's use that to verify that these variables have a correlation coefficient close to what we specified (though it won't be exact due to randomness)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_fake = np.corrcoef(fake_x, fake_y)[0,1]\n",
    "corr_fake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout Session\n",
    "\n",
    "1\\. Rerun the two code cells above that create the random data and calculate their correlation. You should see that the values of the correlation can be pretty different from what we specified.\n",
    "\n",
    "2\\. Now change `FAKE_N` to be `1000` instead of `50`, and rerun the two above code cells several times. How does this change the correlations calculated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "### END STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation as the Slope of a Line that Fits Standardized (Z-Scored) Data in a Scatter Plot\n",
    "\n",
    "When we plot the data from two variables (stored in vectors) on a scatter plot, then the slope of the line that is closest to all the data points, on average, is the correlation. \n",
    "\n",
    "First we'll need to z-score both vectors, since correlation is done on standardized data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_x_zscore = scipy.stats.zscore(fake_x)\n",
    "fake_y_zscore = scipy.stats.zscore(fake_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll use a scatter plot to visualize the data of the two variables, which shows the relationship between the data points of two variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.plot(fake_x_zscore, fake_y_zscore, '.')\n",
    "plt.xlabel('Fake X')\n",
    "_ = plt.ylabel('Fake Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add a line to the plot that best fits the line. Since we know that the slope of that line is the correlation, we can plot the line by using the `fake_x` vector as the x data, and multiplying `fake_x` by the correlation for the y data. Let's see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.plot(fake_x_zscore, fake_y_zscore, '.')\n",
    "plt.plot(fake_x_zscore, fake_x_zscore*corr_fake)\n",
    "plt.xlabel('Fake X')\n",
    "_ = plt.ylabel('Fake Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important property of plotting the standardized data is that the correlation line always goes through the origin, or (0,0) point, of the plot. This is because the means of both vectors are zero (due to z-scoring), and correlation measures how two signals change together, relative to their respective means.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout Session\n",
    "\n",
    "1\\. Make a figure that is `figsize=(10,5)`, and use `plt.subplot` to make two scatter plots in the same row. The first plot should show the original fake data, and the second the z-scored fake data. Draw appropriate titles above each plot. Do they look similar or different? Why might this be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "### END STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation as Two Signals Changing Together, Relative to their Respective Means\n",
    "\n",
    "If we think of the 2 vectors to be correlated as time series data, then correlation is quantifying how similarily the two signals change at each time point relative to their means (since we z-score the data first, their means are both 0!). Thus if we plot the two z-scored vectors together in a line plot, we can get a visual idea of whether they change together or not. Let's compare the z-scored fake data vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 8))\n",
    "plt.plot(fake_x_zscore, 'g-.', label='zscored X')\n",
    "plt.plot(fake_y_zscore, 'g', label='zscored Y')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Z-Scored Data')\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They do seem to vary together fairly well, although not perfectly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout Session\n",
    "\n",
    "1\\. Create 2 new sets of fake x and y data, one with a correlation of 0.1 and with a correlation of 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "### END STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Now z-score both pairs of x and y fake data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "### END STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Finally create a line plot for each pair of x and y fake data, each on its own row of a subplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "### END STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation as the average product of standardized vectors\n",
    "\n",
    "We know that when two vectors vary together in time they are correlated. But how do we quantify this? We learned that correlation can be thought of as:\n",
    "\n",
    "> The average product of two standardized vectors.\n",
    "\n",
    "In other words, once we have z-scored data, we can simply multiply each pair of data points, and take the average of all of those products. Let's have a look at how this works both visually and algebraically for a few different correlation values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_x1, fake_y1 = create_fake_data(100, 0, 0, .1)\n",
    "fake_x2, fake_y2 = create_fake_data(100, 0, 0, .5)\n",
    "fake_x3, fake_y3 = create_fake_data(100, 0, 0, .9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And z-score all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_x1_z = zscore(fake_x1)\n",
    "fake_x2_z = zscore(fake_x2)\n",
    "fake_x3_z = zscore(fake_x3)\n",
    "fake_y1_z = zscore(fake_y1)\n",
    "fake_y2_z = zscore(fake_y2)\n",
    "fake_y3_z = zscore(fake_y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the 3 pairs of fake data, along with their products. We'll use a helper function for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,9))\n",
    "plt.subplot(3,1,1)\n",
    "plot_line_product(fake_x1_z, fake_y1_z)\n",
    "plt.title('Correlation .1')\n",
    "plt.subplot(3,1,2)\n",
    "plot_line_product(fake_x2_z, fake_y2_z)\n",
    "plt.title('Correlation .5')\n",
    "plt.subplot(3,1,3)\n",
    "plot_line_product(fake_x3_z, fake_y3_z)\n",
    "_ = plt.title('Correlation .9')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're averaging the products, when the products are all positive we can expect a high positive correlation, which we see in the bottom plot. But when they are both positive and negative they will cancel each other out, resulting in a correlation close to 0, which we see in the top plot.\n",
    "\n",
    "Now let's calculate the average pairwise product of the two vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_fake1 = np.mean(fake_x1_z*fake_y1_z)\n",
    "corr_fake2 = np.mean(fake_x2_z*fake_y2_z)\n",
    "corr_fake3 = np.mean(fake_x3_z*fake_y3_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And compare these values to what we get using `np.corrcoef`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.allclose(corr_fake1, np.corrcoef(fake_x1_z,fake_y1_z)[0,1]))\n",
    "print(np.allclose(corr_fake2, np.corrcoef(fake_x2_z,fake_y2_z)[0,1]))\n",
    "print(np.allclose(corr_fake3, np.corrcoef(fake_x3_z,fake_y3_z)[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing it all together\n",
    "\n",
    "Now let's use an interactive plot that plots two data vectors using both a scatter plot and a time series plot to see how both visualizations change together when we add more data, or use more correlated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cor_time(n):\n",
    "\n",
    "    # create a sequence for the length of the fake data\n",
    "    t = range(n)\n",
    "    \n",
    "    # subset the fake data to the size specified by the slider\n",
    "    cur_x = fake_x[:n]\n",
    "    cur_y = fake_y[:n]\n",
    "\n",
    "    # z-score (standardize) the fake data\n",
    "    cur_x_z = scipy.stats.zscore(cur_x)\n",
    "    cur_y_z = scipy.stats.zscore(cur_y)\n",
    "    \n",
    "    # take the mean of the fake data\n",
    "    mean_fake_x = np.mean(cur_x)\n",
    "    mean_fake_y = np.mean(cur_y)\n",
    "\n",
    "    # calculate the correlation of the fake data\n",
    "    fake_prods = (cur_x-mean_fake_x)*(cur_y-mean_fake_y)\n",
    "    fake_mean_prods = np.mean(fake_prods)\n",
    "    fake_prod_std = np.std(cur_x) * np.std(cur_y)\n",
    "    fake_cor = fake_mean_prods / fake_prod_std\n",
    "\n",
    "    # print out the values of the correlation\n",
    "#    print('Average of products: %.02f' % (fake_mean_prods))\n",
    "#    print('Product of std-dev: %.02f' % (fake_prod_std))\n",
    "    print('Correlation: %.02f' % (fake_cor))\n",
    "\n",
    "    # plot the line plots of the two variables\n",
    "    plt.figure(figsize=(15,6))\n",
    "    grid = plt.GridSpec(2,5)\n",
    "    plt.subplot(grid[0,2:])\n",
    "    plt.plot(t, cur_x, 'g', label='x')\n",
    "    plt.plot(t, cur_x, 'go')\n",
    "    plt.plot(t, cur_y, 'b', label='y')\n",
    "    plt.plot(t, cur_y, 'bo')\n",
    "    plt.plot(t, np.ones(n)*mean_fake_x, 'g--')\n",
    "    plt.plot(t, np.ones(n)*mean_fake_y, 'b--')\n",
    "    plt.legend()\n",
    "\n",
    "    # plot the product of the difference between each data point and their respective means\n",
    "    plt.subplot(grid[1,2:])\n",
    "    plt.plot(t, np.ones(n)*fake_mean_prods, '--')\n",
    "    plt.bar(t,fake_prods)\n",
    "\n",
    "    # make a scatter plot of the standardized data, and draw the correlation line\n",
    "    plt.subplot(grid[0:2,0:2])\n",
    "    plt.scatter(cur_x_z, cur_y_z)\n",
    "    fit = np.polyfit(cur_x_z, cur_y_z, deg=1)\n",
    "    plt.plot(cur_x_z, fit[0] * cur_x_z + fit[1], color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_plot = interactive(plot_cor_time, n=widgets.IntSlider(value=2, min=2, max=len(fake_x)))\n",
    "output = interactive_plot.children[-1]\n",
    "output.layout.height = '450px'\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation quantifies linear dependency\n",
    "\n",
    "Earlier in the lecture we were using correlation coefficient to assess how much two sets of numbers relate to each other. There are some assumptions behind this, which we will take a look at here.\n",
    "\n",
    "The general intuition, when we say **\"these quantities are correlated\"**, it means: There is some form of **dependency** between the two quantities. Meaning: knowledge of the one quantity might gain us more knowledge about the other quantity. This is a **loose definition**, but there exist mathematical ideas to quantify this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When we say *correlation*, we usually refer to *linear correlation* (also called Pearson correlation)\n",
    "Or to frame it as a question: **How much does the point cloud resemble a line?**\n",
    "\n",
    "We saw above that when 2 vectors are linearly related, without noise, they formed a perfect line, meaning a perfect positive correlation of 1. Let's see that again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(100)\n",
    "y = 2 * x + 4\n",
    "plt.scatter(x, y)\n",
    "_ = plt.axis([-3, 3, -11, 11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Correlation measured on a non-linear relation\n",
    "\n",
    "There can be many different **non-linear** ways that two vectors can relate to each other, or be dependent on each other. One simple way is when one vector is the square of the other, resulting in a parabola when plotted on a scatter plot. Let's create a `height_squared` vector that shows just that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1., 1., 51)\n",
    "y = x ** 2\n",
    "plt.plot(x, y, 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that these two vectors create a parabola when plotted on a scatter plot! While there is definitely a precise relationship between the two arrays, correlation cannot detect that relationship because it only looks for linear relationships. Let's look at what the correlation between these vectors is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically exactly zero! Let's plot the correlation line along with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_z = (x - x.mean()) / x.std()\n",
    "y_z = (y - y.mean()) / y.std()\n",
    "plt.plot(x_z, y_z, 'o')\n",
    "c = np.mean(x_z * y_z)\n",
    "plt.plot(x_z, c * y_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation coefficient of this *perfect functional relation* is **zero**. Always keep this in mind when thinking about and evaluating correlation scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Correlation Flatmaps\n",
    "\n",
    "In this section we will correlate all the voxels of a scan with the response vectors for the experiment. We will compute all of them together using array operations, and eventually even compute all of them together for stimulus types. Then we will display the results on flat maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the visual category fMRI data\n",
    "\n",
    "We'll start by loading the visual category localizer data that we've been using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data01 = load_nifti(\"/data/cogneuro/fMRI/categories/s01_categories_01.nii.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_voxel_1 = scipy.stats.zscore(data01[:, 6, 57, 37])\n",
    "data_faces = faces_voxel_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_voxel_1 = scipy.stats.zscore(data01[:, 10, 62, 40])\n",
    "data_places = places_voxel_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load the stimulus category labels and create the stimulus vectors for all the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "localizer_labels = np.load(\"/home/jovyan/catloc_experimental_conditions.npy\")\n",
    "faces_stimuli_vec = localizer_labels == 'faces'\n",
    "bodies_stimuli_vec = localizer_labels == 'body'\n",
    "places_stimuli_vec = localizer_labels == 'places'\n",
    "objects_stimuli_vec = localizer_labels == 'object'\n",
    "scrambled_stimuli_vec = localizer_labels == 'scrambled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_stimuli_vec = faces_stimuli_vec[:data_places.shape[0]]\n",
    "bodies_stimuli_vec = bodies_stimuli_vec[:data_places.shape[0]]\n",
    "places_stimuli_vec = places_stimuli_vec[:data_places.shape[0]]\n",
    "objects_stimuli_vec = objects_stimuli_vec[:data_places.shape[0]]\n",
    "scrambled_stimuli_vec = scrambled_stimuli_vec[:data_places.shape[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create an HRF, and convolve it with each of the stimulus vectors to create the response vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the HRF function\n",
    "hrf = create_hrf(tr=2.0, oversampling=1, time_length=32)\n",
    "\n",
    "# create the response vectors\n",
    "faces_response_vec = np.convolve(faces_stimuli_vec, hrf)[:len(faces_stimuli_vec)]\n",
    "bodies_response_vec = np.convolve(bodies_stimuli_vec, hrf)[:len(bodies_stimuli_vec)]\n",
    "places_response_vec = np.convolve(places_stimuli_vec, hrf)[:len(places_stimuli_vec)]\n",
    "objects_response_vec = np.convolve(objects_stimuli_vec, hrf)[:len(objects_stimuli_vec)]\n",
    "scrambled_response_vec = np.convolve(scrambled_stimuli_vec, hrf)[:len(scrambled_stimuli_vec)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And pull out the number of categories in the data, and create a vector that represents time for the data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cat = len(faces_stimuli_vec)\n",
    "t_cat = np.arange(0,(n_cat*2),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data\n",
    "\n",
    "Above we already loaded the BOLD data into the names `data1`. Let's check their shapes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data01.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's mask them to obtain the cortical voxels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = cortex.db.get_mask('s01', 'catloc', 'cortical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_masked = data01[:, mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's perform z-scoring for the first array for all the voxels separately:\n",
    "\n",
    "First, we compute the mean of each voxel time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_masked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_mean = data1_masked.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_mean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that taking the mean along axis 0 gives us a mean value for each voxel.\n",
    "\n",
    "We can also take the standard deviation along this axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_stdev = data1_masked.std(axis=0)\n",
    "data1_stdev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do the z-scoring by subtracting the mean and dividing by the standard deviation for each column of the masked data array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_zscored = (data1_masked - data1_mean) / data1_stdev\n",
    "data_zscored = data1_zscored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing correlation of one response vector to all the voxels\n",
    "\n",
    "In order to compute the correlation of a response vector to all the voxels, we will first spell out the multiplication of that one response vector to all voxels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_response_zscored = zscore(faces_response_vec)\n",
    "faces_response_times_voxels = faces_response_zscored * data_zscored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout Session\n",
    "1. Do z-scoring for the other two arrays\n",
    "2. Use `np.concatenate` or `np.vstack` to stack all three matrices vertically into an array named `data_zscored`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huh, that didn't work!\n",
    "\n",
    "To make this work, we need to turn `faces_response_zscored` into a column vector of shape `(360, 1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_response_zscored_column = faces_response_zscored.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_response_times_voxels = faces_response_zscored_column * data_zscored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That worked. Now we can take the mean along the time axis to compute the correlations with each voxel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_faces_data = faces_response_times_voxels.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_faces_data.max(), corr_faces_data.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_faces = cortex.Volume(corr_faces_data, 's01', 'catloc', vmin=-.5, vmax=.5)\n",
    "_ = cortex.quickflat.make_figure(vol_faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_faces = cortex.Volume(corr_faces_data, 's01', 'catloc', vmin=-.5, vmax=.5)\n",
    "cortex.webshow(vol_faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that the `faces_response_zscored` correlates most strongly with voxels that we have already identified as belonging to areas processing faces!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout Session\n",
    "1. Compute the same correlation for `places` in the PPA voxel, and make a flatmap showing the correlation values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "\n",
    "### END STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "We've seen that correlation is a statistical technique that quantifies the linear relationship between two vectors of data by calculating the correlation coefficient, $r$. One way to think about correlation is that it finds a line that best fits the data when the standardized (z-scored) data is plotted in a scatter plot. **Linear regression** is also a statistical technique that finds the line that best fits data when plotted in a scatter plot, but it does not require that the data is standardized. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent vs. Dependent Variables\n",
    "Another way to describe linear regression is that it finds the linear relationship between a single **dependent variable** (y) and one or more **independent variables** denoted (x). Thus **linear regression** is more powerful than correlation because it can quantify the relationship of many different **independent variables** with the **dependent variable**, whereas correlation only finds the relationship between two **variables**, neither of which is independent nor dependent. \n",
    "\n",
    "* **Dependent Variable**: This variable represents the data you've measured, and are trying to understand in terms of the independet variables. In our case, the BOLD data time series is the dependent variable.\n",
    "* **Independent Variable**: These are the one or more variables that we're using to explain the dependent variable. These are generally not measured, rather their true values are known. In our case, the response vectors of all the visual categories or motor tasks are the independent variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The linear equation: `y = a*x + b`\n",
    "\n",
    "We saw last week that we can create a data vector $y$ that is perfectly correlated with a data vector $x$ by using the linear equation: $$y = a*x+b$$\n",
    "\n",
    "Then we can add noise to $y$ to decrease that correlation. The correlation coefficient, $r$, was neither $a$ nor $b$ however. \n",
    "\n",
    "Linear regression gives us a tool to find $a$ and $b$ if the correlation between $x$ and $y$ is perfect, or to estimate the values of $a$ and $b$ that make the average of all the linearly transformed values of $x$ as close as possible to the values of $y$.\n",
    "\n",
    "So let's use some simple fake data to get a feeling for what $a$ and $b$ are using plots. We'll create 4 pairs of fake data with different values of $a$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fake_linear(n, a, b):\n",
    "    x = np.random.randn(n)\n",
    "    y = a * x + b\n",
    "    return (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_linear_x1, fake_linear_y1 = create_fake_linear(100, 1, 0)\n",
    "fake_linear_x2, fake_linear_y2 = create_fake_linear(100, 1, 4)\n",
    "fake_linear_x3, fake_linear_y3 = create_fake_linear(100, 2, 0)\n",
    "fake_linear_x4, fake_linear_y4 = create_fake_linear(100, .3, -3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the four pairs of fake data onto a plot to see how changing $a$ and $b$ changes the line.\n",
    "\n",
    "**NOTE**: Because we want to get a sense for the real slope of the line we must make sure the x and y axes are on the same scale. To do this we can use `plt.axis('square')`. If you don't do this the scales of the two axes will be automatically calculated by default and the angle of the line won't be the actual slope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,7))\n",
    "plt.plot(fake_linear_x1, fake_linear_y1, '.', label='a=1 b=0')\n",
    "plt.plot(fake_linear_x2, fake_linear_y2, '.', label='a=1 b=4')\n",
    "plt.plot(fake_linear_x3, fake_linear_y3, '.', label='a=2 b=0')\n",
    "plt.plot(fake_linear_x4, fake_linear_y4, '.', label='a=0.3 b=-3')\n",
    "plt.grid()\n",
    "plt.axis('square')\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout Session\n",
    "\n",
    "1\\. Think back to algebra class and determine what the slope and intercept are for the 4 fake data sets plotted in the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "### END STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the Slope: `a`\n",
    "\n",
    "The slope of a line is it's angle, which represents the change in y for every change of 1 in x. It's often termed \"Rise over Run\" because it can be calculated be choosing any two points on the line and dividing the difference in y-values of those two points (the Rise) by the difference in the x-values (the Run).\n",
    "\n",
    "In regression, we use the value of one or more variables (x) to predict the value of another (y). When the variables x and y are measured in standard units, the regression line for predicting y based on x has slope r and passes through the origin. Thus the equation of the regression line can be written as:\n",
    "\n",
    "$$\\hat{y} = r*x$$\n",
    "\n",
    "where $r$ is the correlation coefficient, $\\hat{y}$ is the **predicted value of y**, and when both $x$ and $y$ variables are measured in standard units (z-scored).\n",
    " \n",
    "Generally we don't want to have to z-score the data in order to find a relationship between $x$ and $y$, and so we can write the linear regression equation in terms of the original units of the data. We do this by inverting the z-score operation, namely multiplying both variables by their respective standard deviations, and adding back in their respective means:\n",
    "\n",
    "$$\\frac{\\hat{y} − \\bar{y}}{SD_y} = r * \\frac{x - \\bar{x}}{SD_x}$$\n",
    "\n",
    "The figures below give a visual representation of how the we arrive at the above equation.\n",
    "\n",
    "<img src=\"figures/regline_z.png\" align=\"left\" style=\"height: 200px;\"/>\n",
    "<img src=\"figures/regline_orig.png\" align=\"right\" style=\"height: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By rearranging the terms in the equation above, we can retrieve the equation for estimating the slope of the regresion line:\n",
    "$$slope = r*\\frac{SD_y}{SD_x}$$\n",
    " \n",
    "where $r$ is the correlation coefficient, SD_y is the standard deviation of y, and SD_x is the standard deviation of x. \n",
    "\n",
    "Let's put this equation into a function and use it to calculate the slope of the regression line for the fake data sets we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_slope(x, y):\n",
    "    r = np.corrcoef(x, y)[0,1]\n",
    "    return r*np.std(y)/np.std(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_fake1 = calc_slope(fake_linear_x1, fake_linear_y1)\n",
    "slope_fake2 = calc_slope(fake_linear_x2, fake_linear_y2)\n",
    "slope_fake3 = calc_slope(fake_linear_x3, fake_linear_y3)\n",
    "slope_fake4 = calc_slope(fake_linear_x4, fake_linear_y4)\n",
    "print(slope_fake1,slope_fake2,slope_fake3,slope_fake4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we were able to find the slopes perfectly since there was no noise! Now let's update the function to create fake data by adding noise to it, and see how we do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fake_linear(n, a, b, noise_sd=None):\n",
    "    x = np.random.randn(n)\n",
    "    y = a * x + b\n",
    "    if noise_sd is not None:\n",
    "        y += np.random.randn(n)*noise_sd\n",
    "    return (x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we'll create some fake linear data that has noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_linear_x5,fake_linear_y5 = create_fake_linear(100, 1, 0, .3)\n",
    "fake_linear_x6,fake_linear_y6 = create_fake_linear(100, 2, 0, .3)\n",
    "fake_linear_x7,fake_linear_y7 = create_fake_linear(100, .3, 0, .3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use the slope function to estimate the slope that best fits this data, but it won't be perfect because we've added noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_fake5 = calc_slope(fake_linear_x5, fake_linear_y5)\n",
    "slope_fake6 = calc_slope(fake_linear_x6, fake_linear_y6)\n",
    "slope_fake7 = calc_slope(fake_linear_x7, fake_linear_y7)\n",
    "print('True Value: %.03f, Estimated Value: %.03f' % (1, slope_fake5))\n",
    "print('True Value: %.03f, Estimated Value: %.03f' % (2, slope_fake6))\n",
    "print('True Value: %.03f, Estimated Value: %.03f' % (.3, slope_fake7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's still pretty good, the estimated values for the slope are very close to the original values! \n",
    "\n",
    "Let's plot the data along with the regression line to visualize what's happening. Note that fake data sets 5, 6 & 7 were created with an intercept of 0, so we don't need to estimate an intercept for these plots to look good. Normally you would, and we'll learn how to do that next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(fake_linear_x5, fake_linear_y5, '.')\n",
    "plt.plot(fake_linear_x5, fake_linear_x5 * slope_fake5)\n",
    "plt.axis('square')\n",
    "plt.title('Fake Data 5')\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(fake_linear_x6, fake_linear_y6, '.')\n",
    "plt.plot(fake_linear_x6, fake_linear_x6 * slope_fake6)\n",
    "plt.axis('square')\n",
    "plt.title('Fake Data 6')\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(fake_linear_x7, fake_linear_y7, '.')\n",
    "plt.plot(fake_linear_x7, fake_linear_x7 * slope_fake7)\n",
    "plt.axis('square')\n",
    "_ = plt.title('Fake Data 7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout Session\n",
    "\n",
    "1\\. Create a new pair of fake $x$ and $y$ that is 100 long called `fake_breakout_x` and `fake_breakout_y`, with `a=3.2`, `b=4.2` and `noise_sd=10`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "### END STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Now calculate the slope for a linear regression between these two new variables. How close is it to the original `a`? Why is it close or far from the original value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "### END STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the Intercept: `b`\n",
    "\n",
    "The intercept is the point on the y axis where the regression line intersects it. We can estimate it using the following equation derived in the same manner as that for the slope:\n",
    "\n",
    "$$intercept = \\bar{y} − slope*\\bar{x}$$ \n",
    "\n",
    "where $\\hat{y}$ is the mean of $y$, and $\\hat{x}$ is the mean of $x$.\n",
    "\n",
    "Let's put that equation into a helper function and use it on our fake data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_intercept(x, y):\n",
    "    return np.mean(y) - calc_slope(x, y) * np.mean(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept_fake1 = calc_intercept(fake_linear_x1, fake_linear_y1)\n",
    "intercept_fake2 = calc_intercept(fake_linear_x2, fake_linear_y2)\n",
    "intercept_fake3 = calc_intercept(fake_linear_x3, fake_linear_y3)\n",
    "intercept_fake4 = calc_intercept(fake_linear_x4, fake_linear_y4)\n",
    "\n",
    "print('Real Intercept: %.03f, Estimated Intecept: %.03f' % (0,intercept_fake1))\n",
    "print('Real Intercept: %.03f, Estimated Intecept: %.03f' % (4,intercept_fake2))\n",
    "print('Real Intercept: %.03f, Estimated Intecept: %.03f' % (0,intercept_fake3))\n",
    "print('Real Intercept: %.03f, Estimated Intecept: %.03f' % (-3,intercept_fake4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we did perfect because there was no noise in this data. Let's add some noise and see how we do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_linear_x8, fake_linear_y8 = create_fake_linear(100, 1.5, 5, .5)\n",
    "fake_linear_x9, fake_linear_y9 = create_fake_linear(100, 5, -.4, .5)\n",
    "fake_linear_x10, fake_linear_y10 = create_fake_linear(100, -2, 2, .5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll estimate both the slope and intercept now so we can plot the data with the regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept_fake8 = calc_intercept(fake_linear_x8, fake_linear_y8)\n",
    "intercept_fake9 = calc_intercept(fake_linear_x9, fake_linear_y9)\n",
    "intercept_fake10 = calc_intercept(fake_linear_x10, fake_linear_y10)\n",
    "\n",
    "slope_fake8 = calc_slope(fake_linear_x8, fake_linear_y8)\n",
    "slope_fake9 = calc_slope(fake_linear_x9, fake_linear_y9)\n",
    "slope_fake10 = calc_slope(fake_linear_x10, fake_linear_y10)\n",
    "\n",
    "print('Real Intercept: %.03f, Estimated Intecept: %.03f' % (5,intercept_fake8))\n",
    "print('Real Intercept: %.03f, Estimated Intecept: %.03f' % (-.4,intercept_fake9))\n",
    "print('Real Intercept: %.03f, Estimated Intecept: %.03f' % (2,intercept_fake10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we did a good job! Let's see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(fake_linear_x8, fake_linear_y8, '.')\n",
    "plt.plot(fake_linear_x8, fake_linear_x8 * slope_fake8 + intercept_fake8)\n",
    "plt.axis('square')\n",
    "plt.title('Fake Data 8')\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(fake_linear_x9, fake_linear_y9, '.')\n",
    "plt.plot(fake_linear_x9, fake_linear_x9 * slope_fake9 + intercept_fake9)\n",
    "plt.axis('square')\n",
    "plt.title('Fake Data 9')\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(fake_linear_x10, fake_linear_y10, '.')\n",
    "plt.plot(fake_linear_x10, fake_linear_x10 * slope_fake10 + intercept_fake10)\n",
    "plt.axis('square')\n",
    "_ = plt.title('Fake Data 10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout Session\n",
    "\n",
    "1\\. Estimate the intercept of a linear regression of `fake_breakout_x` onto `fake_breakout_y`. Is it close to the original `b`? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "### END STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship between Correlation and Simple Linear Regression\n",
    "\n",
    "As was suggested earlier, there is a very intersting relationship between correlation and the slope in a **Simple Linear Regression** (which is a regression with only 1 independent variable). \n",
    "\n",
    "If we standardize (z-score) both our x (independent) and y (dependent) variables, then the slope we find using linear regression is exactly the same as the correlation, and the intercept is 0. Let's have a look, first we'll z-score our fake data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_linear_x10_z = scipy.stats.zscore(fake_linear_x10)\n",
    "fake_linear_y10_z = scipy.stats.zscore(fake_linear_y10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate the correlation, slope and intercept for the z-scored data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_intercept10_z = calc_intercept(fake_linear_x10_z, fake_linear_y10_z)\n",
    "fake_slope10_z = calc_slope(fake_linear_x10_z, fake_linear_y10_z)\n",
    "fake_correlation10_z = np.corrcoef(fake_linear_x10_z, fake_linear_y10_z)[0,1]\n",
    "\n",
    "# print out the results\n",
    "print('Linear Regression Intercept: %.06f' % (fake_intercept10_z))\n",
    "print('Linear Regression Slope: %.06f' % (fake_slope10_z))\n",
    "print('Correlation: %.06f' % (fake_correlation10_z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout Session\n",
    "\n",
    "1\\. Now z-score `fake_breakout_x` and `fake_breakout_y` and recalculate the slope on these z-scored vectors. Confirm this gives you a correlation by calculating the correlation between the original (non-zscored) vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "### END STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the line which minimizes error\n",
    "\n",
    "Now that we have an idea of what the slope and intercept are, let's get an intuitive sense for what values will give a good \"fit\" to the data using some interactive plots. This plot has two sliders, one for the slope and one for the intercept. Moving the sliders will change the slope and intercept of the red line in the plot. The data points are draw in blue, and there are lines between each data point and the regression line in green. You want the sum of all of these lines to be as small as possible. That sum is printed as a number beneath the plot. \n",
    "\n",
    "Try to adjust the slope and intercept to find the minimum value of that sum at the bottom, which is exactly what linear regression does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_error_fake(slope, intercept):\n",
    "    plt.figure(figsize=(7,7))\n",
    "    return show_error_scatter(fake_linear_x10.reshape(-1,1), fake_linear_y10.reshape(-1,1), slope, intercept)\n",
    "\n",
    "_ = interact(show_error_fake, \n",
    "             slope=FloatSlider(value=0, min=-5, max=5), \n",
    "             intercept=FloatSlider(value=0, min=-5, max=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Linear Regression in Two Ways\n",
    "\n",
    "We learned that we can plot two vectors as time series on a line plot to get an idea of how correlated they are. We can do a very similar thing to try and guess the values for the slope and intercept of a linear regression between two vectors. This interactive plot is the same as the last one, with the addition of a time series line plot. Have a look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_linear_x10.reshape(100,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_error_fake10_both(slope, intercept):\n",
    "    return show_error_both_ways(fake_linear_x10, fake_linear_y10, slope, intercept)\n",
    "\n",
    "interactive_plot = interactive(show_error_fake10_both, \n",
    "                               slope=widgets.FloatSlider(value=0, min=-5, max=5), \n",
    "                               intercept=widgets.FloatSlider(value=0, min=-5, max=5))\n",
    "output = interactive_plot.children[-1]\n",
    "output.layout.height = '450px'\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fake_linear_x10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout Session\n",
    "\n",
    "1\\. What operations are applied to the response vector when the 'slope' and 'intersept' sliders are changed? Why does changing the values cause the error value to increase or decrease?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "### END STUDENT ANSWER"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
