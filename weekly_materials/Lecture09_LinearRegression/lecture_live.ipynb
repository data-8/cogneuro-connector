{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 9: Models: Simple Linear Regression and Multiple Regression\n",
    "\n",
    "## Goals\n",
    "- **Neuroscience / Neuroimaging concepts**\n",
    "    - How to model and estimate voxel responses to different conditions\n",
    "    - Simulating a signal\n",
    "    - Design Matrix\n",
    "- **Datascience / Coding concepts**\n",
    "    - Statistical Models\n",
    "    - Simple Linear Regression\n",
    "    - Cost functions: Sum of Squared Errors (SSE)\n",
    "    - Relationship between correlation and regression\n",
    "    - Multiple Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Simply run the cells below that contain all the Python modules we'll neeed, plus setup matplotlib for plotting in this jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats\n",
    "from scipy.stats import zscore\n",
    "import nibabel\n",
    "import cortex\n",
    "import os\n",
    "from nistats.hemodynamic_models import glover_hrf as create_hrf\n",
    "\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, widgets, FloatSlider\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set plotting defaults\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "Define a few helper functions that we'll use in this lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nifti(file_name, mask=None):\n",
    "    img = nibabel.load(file_name)\n",
    "    data = img.get_data().T\n",
    "    if mask is None:\n",
    "        masked_data = data\n",
    "    else:\n",
    "        masked_data = data[:, mask]\n",
    "    return masked_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_line_product(x, y, label_x='X', label_y='Y'):\n",
    "    plt.plot(x, label=label_x)\n",
    "    plt.plot(y, label=label_y)\n",
    "    plt.bar(np.arange(len(x)), x*y, label=\"Product\")\n",
    "    _ = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_error_time(x, y, slope, intercept):\n",
    "    \"\"\"Plots the whole thing as a time series instead of a point cloud\"\"\"\n",
    "    n = x.shape[0]\n",
    "    plt.plot(y, 'b')\n",
    "    plt.hlines([0], x.min(), x.max())\n",
    "    num_x = 1 if x.ndim == 1 else x.shape[1]\n",
    "    t = np.arange(n)\n",
    "    new_x = np.zeros((n,))\n",
    "    for cur_x in range(num_x):\n",
    "        cur_x = (intercept + slope[cur_x] * x[:,cur_x])\n",
    "        new_x += cur_x \n",
    "    plt.plot(new_x, 'r', lw=1)\n",
    "    plt.fill_between(t, new_x, y[:,0], color='g', alpha=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_error_scatter(x, y, slope, intercept):\n",
    "    x = x[:,0]\n",
    "    y = y[:,0]\n",
    "    x_left = x.min() - x.ptp() * .2\n",
    "    x_right = x.max() + x.ptp() * .2\n",
    "    y_est = slope * x + intercept\n",
    "    y_left, y_right = slope * x_left + intercept, slope * x_right + intercept\n",
    "    y_bottom, y_top = y.min() - y.ptp() * .2, y.max() + y.ptp() * .2\n",
    "    xs = np.vstack([x, x])\n",
    "    bars = np.vstack([y, y_est])\n",
    "    plt.hlines([0], x_left, x_right)\n",
    "    plt.vlines([0], y_bottom, y_top)\n",
    "    plt.scatter(x, y)\n",
    "    plt.plot([x_left, x_right], [y_left, y_right], 'r', lw=2)\n",
    "    plt.plot([0, 0], [0, intercept], 'm', lw=2)\n",
    "    plt.plot(xs, bars, 'g-.')\n",
    "    plt.axis([x_left, x_right, y_bottom, y_top])\n",
    "    return ((y_est - y) ** 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_error_both_ways(x, y, slope, intercept):\n",
    "    if x.ndim == 1:\n",
    "        x = x.reshape(len(x),1)\n",
    "    if y.ndim == 1:\n",
    "        y = y.reshape(len(y),1)\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    plt.axes([0, 0, .18, 1.])\n",
    "    e = show_error_scatter(x, y, slope, intercept)\n",
    "    plt.axes([.2, 0, .8, 1])\n",
    "    show_error_time(x, y, [slope], intercept)\n",
    "    print('Sum of squared error (SSE): %.04f' % (e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the visual category fMRI data\n",
    "\n",
    "We'll start by loading the visual category localizer data that we've been using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data01 = load_nifti(\"/data/cogneuro/fMRI/categories/s01_categories_01.nii.gz\")\n",
    "data02 = load_nifti(\"/data/cogneuro/fMRI/categories/s01_categories_02.nii.gz\")\n",
    "data03 = load_nifti(\"/data/cogneuro/fMRI/categories/s01_categories_03.nii.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_voxel_1 = scipy.stats.zscore(data01[:, 6, 57, 37])\n",
    "faces_voxel_2 = scipy.stats.zscore(data02[:, 6, 57, 37])\n",
    "faces_voxel_3 = scipy.stats.zscore(data03[:, 6, 57, 37])\n",
    "data_faces = np.concatenate((faces_voxel_1, faces_voxel_2, faces_voxel_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_voxel_1 = scipy.stats.zscore(data01[:, 10, 62, 40])\n",
    "places_voxel_2 = scipy.stats.zscore(data02[:, 10, 62, 40])\n",
    "places_voxel_3 = scipy.stats.zscore(data03[:, 10, 62, 40])\n",
    "data_places = np.concatenate((places_voxel_1, places_voxel_2, places_voxel_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load the stimulus category labels and create the stimulus vectors for all the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "localizer_labels = np.load(\"/home/jovyan/catloc_experimental_conditions.npy\")\n",
    "faces_stimuli_vec = localizer_labels == 'faces'\n",
    "bodies_stimuli_vec = localizer_labels == 'body'\n",
    "places_stimuli_vec = localizer_labels == 'places'\n",
    "objects_stimuli_vec = localizer_labels == 'object'\n",
    "scrambled_stimuli_vec = localizer_labels == 'scrambled'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create an HRF, and convolve it with each of the stimulus vectors to create the response vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the HRF function\n",
    "hrf = create_hrf(tr=2.0, oversampling=1, time_length=32)\n",
    "\n",
    "# create the response vectors\n",
    "faces_response_vec = np.convolve(faces_stimuli_vec, hrf)[:len(faces_stimuli_vec)]\n",
    "bodies_response_vec = np.convolve(bodies_stimuli_vec, hrf)[:len(bodies_stimuli_vec)]\n",
    "places_response_vec = np.convolve(places_stimuli_vec, hrf)[:len(places_stimuli_vec)]\n",
    "objects_response_vec = np.convolve(objects_stimuli_vec, hrf)[:len(objects_stimuli_vec)]\n",
    "scrambled_response_vec = np.convolve(scrambled_stimuli_vec, hrf)[:len(scrambled_stimuli_vec)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And pull out the number of categories in the data, and create a vector that represents time for the data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cat = len(faces_stimuli_vec)\n",
    "t_cat = np.arange(0,(n_cat*2),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation\n",
    "\n",
    "Last week we learned that correlation is a statistic that can be calculated to quantify the similarity of two random variables, and there are multiple ways to think about what exactly it does. We learned three:\n",
    "* Correlation is the slope of a line that best reduces the squared error between z-scored data in a scatter plot.\n",
    "* Correlation measures the extent to which two variables change together away from their respective means. \n",
    "* Correlation is the average product of the standardized scores of two variables.\n",
    "\n",
    "Let's create some \"fake\" data and then review these three ways to think about correlation. First we'll create some constants that we can change later that adjust the size of the fake data and how correlated the two vectors are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_CORRELATION = 0.6\n",
    "FAKE_N = 50\n",
    "MEAN_1 = 0\n",
    "MEAN_2 = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the fake data with the means and correlation specified above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_data = np.random.multivariate_normal([MEAN_1,MEAN_2], [[1,TARGET_CORRELATION],[TARGET_CORRELATION,1]],FAKE_N)\n",
    "fake_x = fake_data[:,0]\n",
    "fake_y = fake_data[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put that into a function since we'll use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fake_data(n, mean1, mean2, corr):\n",
    "    fake_data = np.random.multivariate_normal([mean1,mean2], [[1,corr],[corr,1]],n)\n",
    "    fake_x = fake_data[:,0]\n",
    "    fake_y = fake_data[:,1]\n",
    "    return (fake_x, fake_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We learned we can use the `np.corrcoef` function to compute the correlation between two variables. Let's use that to verify that these variables have a correlation coefficient close to what we specified (though it won't be exact due to randomness)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_fake = np.corrcoef(fake_x, fake_y)[0,1]\n",
    "corr_fake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout Session\n",
    "\n",
    "1\\. Rerun the two code cells above that create the random data and calculate their correlation. You should see that the values of the correlation can be pretty different from what we specified.\n",
    "\n",
    "2\\. Now change `FAKE_N` to be `1000` instead of `50`, and rerun the two above code cells several times. How does this change the correlations calculated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "### END STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation as the Slope of a Line that Fits Standardized (Z-Scored) Data in a Scatter Plot\n",
    "\n",
    "When we plot the data from two variables (stored in vectors) on a scatter plot, then the slope of the line that is closest to all the data points, on average, is the correlation. \n",
    "\n",
    "First we'll need to z-score both vectors, since correlation is done on standardized data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_x_zscore = scipy.stats.zscore(fake_x)\n",
    "fake_y_zscore = scipy.stats.zscore(fake_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll use a scatter plot to visualize the data of the two variables, which shows the relationship between the data points of two variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.plot(fake_x_zscore, fake_y_zscore, '.')\n",
    "plt.xlabel('Fake X')\n",
    "_ = plt.ylabel('Fake Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add a line to the plot that best fits the line. Since we know that the slope of that line is the correlation, we can plot the line by using the `fake_x` vector as the x data, and multiplying `fake_x` by the correlation for the y data. Let's see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.plot(fake_x_zscore, fake_y_zscore, '.')\n",
    "plt.plot(fake_x_zscore, fake_x_zscore*corr_fake)\n",
    "plt.xlabel('Fake X')\n",
    "_ = plt.ylabel('Fake Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important property of plotting the standardized data is that the correlation line always goes through the origin, or (0,0) point, of the plot. This is because the means of both vectors are zero (due to z-scoring), and correlation measures how two signals change together, relative to their respective means.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout Session\n",
    "\n",
    "1\\. Make a figure that is `figsize=(10,5)`, and use `plt.subplot` to make two scatter plots in the same row. The first plot should show the original fake data, and the second the z-scored fake data. Draw appropriate titles above each plot. Do they look similar or different? Why might this be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "### END STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation as Two Signals Changing Together, Relative to their Respective Means\n",
    "\n",
    "If we think of the 2 vectors to be correlated as time series data, then correlation is quantifying how similarily the two signals change at each time point relative to their means (since we z-score the data first, their means are both 0!). Thus if we plot the two z-scored vectors together in a line plot, we can get a visual idea of whether they change together or not. Let's compare the z-scored fake data vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 8))\n",
    "plt.plot(fake_x_zscore, 'g-.', label='zscored X')\n",
    "plt.plot(fake_y_zscore, 'g', label='zscored Y')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Z-Scored Data')\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They do seem to vary together fairly well, although not perfectly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout Session\n",
    "\n",
    "1\\. Create 2 new sets of fake x and y data, one with a correlation of 0.1 and with a correlation of 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "### END STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Now z-score both pairs of x and y fake data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "### END STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Finally create a line plot for each pair of x and y fake data, each on its own row of a subplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "### END STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation as the average product of standardized vectors\n",
    "\n",
    "We know that when two vectors vary together in time they are correlated. But how do we quantify this? We learned that correlation can be thought of as:\n",
    "\n",
    "> The average product of two standardized vectors.\n",
    "\n",
    "In other words, once we have z-scored data, we can simply multiply each pair of data points, and take the average of all of those products. Let's have a look at how this works both visually and algebraically for a few different correlation values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_x1, fake_y1 = create_fake_data(100, 0, 0, .1)\n",
    "fake_x2, fake_y2 = create_fake_data(100, 0, 0, .5)\n",
    "fake_x3, fake_y3 = create_fake_data(100, 0, 0, .9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And z-score all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_x1_z = zscore(fake_x1)\n",
    "fake_x2_z = zscore(fake_x2)\n",
    "fake_x3_z = zscore(fake_x3)\n",
    "fake_y1_z = zscore(fake_y1)\n",
    "fake_y2_z = zscore(fake_y2)\n",
    "fake_y3_z = zscore(fake_y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the 3 pairs of fake data, along with their products. We'll use a helper function for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,9))\n",
    "plt.subplot(3,1,1)\n",
    "plot_line_product(fake_x1_z, fake_y1_z)\n",
    "plt.title('Correlation .1')\n",
    "plt.subplot(3,1,2)\n",
    "plot_line_product(fake_x2_z, fake_y2_z)\n",
    "plt.title('Correlation .5')\n",
    "plt.subplot(3,1,3)\n",
    "plot_line_product(fake_x3_z, fake_y3_z)\n",
    "_ = plt.title('Correlation .9')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're averaging the products, when the products are all positive we can expect a high positive correlation, which we see in the bottom plot. But when they are both positive and negative they will cancel each other out, resulting in a correlation close to 0, which we see in the top plot.\n",
    "\n",
    "Now let's calculate the average pairwise product of the two vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_fake1 = np.mean(fake_x1_z*fake_y1_z)\n",
    "corr_fake2 = np.mean(fake_x2_z*fake_y2_z)\n",
    "corr_fake3 = np.mean(fake_x3_z*fake_y3_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And compare these values to what we get using `np.corrcoef`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.allclose(corr_fake1, np.corrcoef(fake_x1_z,fake_y1_z)[0,1]))\n",
    "print(np.allclose(corr_fake2, np.corrcoef(fake_x2_z,fake_y2_z)[0,1]))\n",
    "print(np.allclose(corr_fake3, np.corrcoef(fake_x3_z,fake_y3_z)[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing it all together\n",
    "\n",
    "Now let's use an interactive plot that plots two data vectors using both a scatter plot and a time series plot to see how both visualizations change together when we add more data, or use more correlated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cor_time(n):\n",
    "\n",
    "    # create a sequence for the length of the fake data\n",
    "    t = range(n)\n",
    "    \n",
    "    # subset the fake data to the size specified by the slider\n",
    "    cur_x = fake_x[:n]\n",
    "    cur_y = fake_y[:n]\n",
    "\n",
    "    # z-score (standardize) the fake data\n",
    "    cur_x_z = scipy.stats.zscore(cur_x)\n",
    "    cur_y_z = scipy.stats.zscore(cur_y)\n",
    "    \n",
    "    # take the mean of the fake data\n",
    "    mean_fake_x = np.mean(cur_x)\n",
    "    mean_fake_y = np.mean(cur_y)\n",
    "\n",
    "    # calculate the correlation of the fake data\n",
    "    fake_prods = (cur_x-mean_fake_x)*(cur_y-mean_fake_y)\n",
    "    fake_mean_prods = np.mean(fake_prods)\n",
    "    fake_prod_std = np.std(cur_x) * np.std(cur_y)\n",
    "    fake_cor = fake_mean_prods / fake_prod_std\n",
    "\n",
    "    # print out the values of the correlation\n",
    "    print('Average of products: %.02f' % (fake_mean_prods))\n",
    "    print('Product of std-dev: %.02f' % (fake_prod_std))\n",
    "    print('Correlation: %.02f' % (fake_cor))\n",
    "\n",
    "    # plot the line plots of the two variables\n",
    "    plt.figure(figsize=(15,6))\n",
    "    grid = plt.GridSpec(2,5)\n",
    "    plt.subplot(grid[0,2:])\n",
    "    plt.plot(t, cur_x, 'g', label='x')\n",
    "    plt.plot(t, cur_x, 'go')\n",
    "    plt.plot(t, cur_y, 'b', label='y')\n",
    "    plt.plot(t, cur_y, 'bo')\n",
    "    plt.plot(t, np.ones(n)*mean_fake_x, 'g--')\n",
    "    plt.plot(t, np.ones(n)*mean_fake_y, 'b--')\n",
    "    plt.legend()\n",
    "\n",
    "    # plot the product of the difference between each data point and their respective means\n",
    "    plt.subplot(grid[1,2:])\n",
    "    plt.plot(t, np.ones(n)*fake_mean_prods, '--')\n",
    "    plt.bar(t,fake_prods)\n",
    "\n",
    "    # make a scatter plot of the standardized data, and draw the correlation line\n",
    "    plt.subplot(grid[0:2,0:2])\n",
    "    plt.scatter(cur_x_z, cur_y_z)\n",
    "    fit = np.polyfit(cur_x_z, cur_y_z, deg=1)\n",
    "    plt.plot(cur_x_z, fit[0] * cur_x_z + fit[1], color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_plot = interactive(plot_cor_time, n=widgets.IntSlider(value=2, min=2, max=len(fake_x)))\n",
    "output = interactive_plot.children[-1]\n",
    "output.layout.height = '450px'\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation quantifies linear dependency\n",
    "\n",
    "Earlier in the lecture we were using correlation coefficient to assess how much two sets of numbers relate to each other. There are some assumptions behind this, which we will take a look at here.\n",
    "\n",
    "The general intuition, when we say **\"these quantities are correlated\"**, it means: There is some form of **dependency** between the two quantities. Meaning: knowledge of the one quantity might gain us more knowledge about the other quantity. This is a **loose definition**, but there exist mathematical ideas to quantify this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When we say *correlation*, we usually refer to *linear correlation* (also called Pearson correlation)\n",
    "Or to frame it as a question: **How much does the point cloud resemble a line?**\n",
    "\n",
    "We saw above that when 2 vectors are linearly related, without noise, they formed a perfect line, meaning a perfect positive correlation of 1. Let's see that again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(100)\n",
    "y = 2 * x + 4\n",
    "plt.scatter(x, y)\n",
    "_ = plt.axis([-3, 3, -11, 11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Correlation measured on a non-linear relation\n",
    "\n",
    "There can be many different **non-linear** ways that two vectors can relate to each other, or be dependent on each other. One simple way is when one vector is the square of the other, resulting in a parabola when plotted on a scatter plot. Let's create a `height_squared` vector that shows just that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1., 1., 51)\n",
    "y = x ** 2\n",
    "plt.plot(x, y, 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that these two vectors create a parabola when plotted on a scatter plot! While there is definitely a precise relationship between the two arrays, correlation cannot detect that relationship because it only looks for linear relationships. Let's look at what the correlation between these vectors is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically exactly zero! Let's plot the correlation line along with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_z = (x - x.mean()) / x.std()\n",
    "y_z = (y - y.mean()) / y.std()\n",
    "plt.plot(x_z, y_z, 'x')\n",
    "c = np.mean(x_z * y_z)\n",
    "plt.plot(x_z, c * y_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation coefficient of this *perfect functional relation* is **zero**. Always keep this in mind when thinking about and evaluating correlation scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Correlation Flatmaps\n",
    "\n",
    "In this section we will correlate all the voxels of a scan with the response vectors for the experiment. We will compute all of them together using array operations, and eventually even compute all of them together for stimulus types. Then we will display the results on flat maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data\n",
    "\n",
    "Above we already loaded the BOLD data into the names `data1, data2, data3`. Let's check their shapes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data01.shape, data02.shape, data03.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's mask them to obtain the cortical voxels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = cortex.db.get_mask('s01', 'catloc', 'cortical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_masked = data01[:, mask]\n",
    "data2_masked = data02[:, mask]\n",
    "data3_masked = data03[:, mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's perform z-scoring for the first array for all the voxels separately:\n",
    "\n",
    "First, we compute the mean of each voxel time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_masked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_mean = data1_masked.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_mean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that taking the mean along axis 0 gives us a mean value for each voxel.\n",
    "\n",
    "We can also take the standard deviation along this axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_stdev = data1_masked.std(axis=0)\n",
    "data1_stdev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do the z-scoring by subtracting the mean and dividing by the standard deviation for each column of the masked data array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_zscored = (data1_masked - data1_mean) / data1_stdev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout Session\n",
    "1. Do z-scoring for the other two arrays\n",
    "2. Use `np.concatenate` or `np.vstack` to stack all three matrices vertically into an array named `data_zscored`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "### END STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing correlation of one response vector to all the voxels\n",
    "\n",
    "In order to compute the correlation of a response vector to all the voxels, we will first spell out the multiplication of that one response vector to all voxels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_response_zscored = zscore(faces_response_vec)\n",
    "faces_response_times_voxels = faces_response_zscored * data_zscored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huh, that didn't work!\n",
    "\n",
    "To make this work, we need to turn `faces_response_zscored` into a column vector of shape `(360, 1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_response_zscored_column = faces_response_zscored.reshape(360, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_response_times_voxels = faces_response_zscored_column * data_zscored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That worked. Now we can take the mean along the time axis to compute the correlations with each voxel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_faces_data = faces_response_times_voxels.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_faces_data.max(), corr_faces_data.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_faces = cortex.Volume(corr_faces_data, 's01', 'catloc')\n",
    "cortex.quickflat.make_figure(vol_faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that the `faces_response_zscored` correlates most strongly with voxels that we have already identified as belonging to areas processing faces!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout Session\n",
    "1. Compute the same correlation for `places` in the PPA voxel, and make a flatmap showing the correlation values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "\n",
    "### END STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: How does the HRF improve correlation?\n",
    "Now that we have a metric that can quantify how similar two signals are (correlation), let's look and see how much more similar the response vectors that we calculated two weeks ago are to the BOLD data than the stimulus vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_faces_stimuli = np.corrcoef(data_faces, faces_stimuli_vec)[0,1]\n",
    "corr_faces_response = np.corrcoef(data_faces, faces_response_vec)[0,1]\n",
    "\n",
    "print('Correlation between stimulus vector and BOLD data: %.06f' % (corr_faces_stimuli))\n",
    "print('Correlation between response vector and BOLD data: %.06f' % (corr_faces_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that considering the HRF does indeed improve the correlation for this voxel! Let's "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "We've seen that correlation is a statistical technique that quantifies the linear relationship between two vectors of data by calculating the correlation coefficient, $r$. One way to think about correlation is that it finds a line that best fits the data when the standardized (z-scored) data is plotted in a scatter plot. **Linear regression** is also a statistical technique that finds the line that best fits data when plotted in a scatter plot, but it does not require that the data is standardized. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent vs. Dependent Variables\n",
    "Another way to describe linear regression is that it finds the linear relationship between a single **dependent variable** (y) and one or more **independent variables** denoted (x). Thus **linear regression** is more powerful than correlation because it can quantify the relationship of many different **independent variables** with the **dependent variable**, whereas correlation only finds the relationship between two **variables**, neither of which is independent nor dependent. \n",
    "\n",
    "* **Dependent Variable**: This variable represents the data you've measured, and are trying to understand in terms of the independet variables. In our case, the BOLD data time series is the dependent variable.\n",
    "* **Independent Variable**: These are the one or more variables that we're using to explain the dependent variable. These are generally not measured, rather their true values are known. In our case, the response vectors of all the visual categories or motor tasks are the independent variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The linear equation: `y = a*x + b`\n",
    "\n",
    "We saw last week that we can create a data vector $y$ that is perfectly correlated with a data vector $x$ by using the linear equation: $$y = a*x+b$$\n",
    "\n",
    "Then we can add noise to $y$ to decrease that correlation. The correlation coefficient, $r$, was neither $a$ nor $b$ however. \n",
    "\n",
    "Linear regression gives us a tool to find $a$ and $b$ if the correlation between $x$ and $y$ is perfect, or to estimate the values of $a$ and $b$ that make the average of all the linearly transformed values of $x$ as close as possible to the values of $y$.\n",
    "\n",
    "So let's use some simple fake data to get a feeling for what $a$ and $b$ are using plots. We'll create 4 pairs of fake data with different values of $a$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fake_linear(n, a, b):\n",
    "    x = np.random.randn(n)\n",
    "    y = a * x + b\n",
    "    return (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_linear_x1, fake_linear_y1 = create_fake_linear(100, 1, 0)\n",
    "fake_linear_x2, fake_linear_y2 = create_fake_linear(100, 1, 4)\n",
    "fake_linear_x3, fake_linear_y3 = create_fake_linear(100, 2, 0)\n",
    "fake_linear_x4, fake_linear_y4 = create_fake_linear(100, .3, -3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the four pairs of fake data onto a plot to see how changing $a$ and $b$ changes the line.\n",
    "\n",
    "**NOTE**: Because we want to get a sense for the real slope of the line we must make sure the x and y axes are on the same scale. To do this we can use `plt.axis('square')`. If you don't do this the scales of the two axes will be automatically calculated by default and the angle of the line won't be the actual slope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,7))\n",
    "plt.plot(fake_linear_x1, fake_linear_y1, '.', label='a=1 b=0')\n",
    "plt.plot(fake_linear_x2, fake_linear_y2, '.', label='a=1 b=4')\n",
    "plt.plot(fake_linear_x3, fake_linear_y3, '.', label='a=2 b=0')\n",
    "plt.plot(fake_linear_x4, fake_linear_y4, '.', label='a=0.3 b=-3')\n",
    "plt.grid()\n",
    "plt.axis('square')\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout Session\n",
    "\n",
    "1\\. Think back to algebra class and determine what the slope and intercept are for the 4 fake data sets plotted in the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "### END STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the Slope: `a`\n",
    "\n",
    "The slope of a line is it's angle, which represents the change in y for every change of 1 in x. It's often termed \"Rise over Run\" because it can be calculated be choosing any two points on the line and dividing the difference in y-values of those two points (the Rise) by the difference in the x-values (the Run).\n",
    "\n",
    "In regression, we use the value of one or more variables (x) to predict the value of another (y). When the variables x and y are measured in standard units, the regression line for predicting y based on x has slope r and passes through the origin. Thus the equation of the regression line can be written as:\n",
    "\n",
    "$$\\hat{y} = r*x$$\n",
    "\n",
    "where $r$ is the correlation coefficient, $\\hat{y}$ is the **predicted value of y**, and when both $x$ and $y$ variables are measured in standard units (z-scored).\n",
    " \n",
    "Generally we don't want to have to z-score the data in order to find a relationship between $x$ and $y$, and so we can write the linear regression equation in terms of the original units of the data. We do this by inverting the z-score operation, namely multiplying both variables by their respective standard deviations, and adding back in their respective means:\n",
    "\n",
    "$$\\frac{\\hat{y} − \\bar{y}}{SD_y} = r * \\frac{x - \\bar{x}}{SD_x}$$\n",
    "\n",
    "The figures below give a visual representation of how the we arrive at the above equation.\n",
    "\n",
    "<img src=\"figures/regline_z.png\" align=\"left\" style=\"height: 200px;\"/>\n",
    "<img src=\"figures/regline_orig.png\" align=\"right\" style=\"height: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By rearranging the terms in the equation above, we can retrieve the equation for estimating the slope of the regresion line:\n",
    "$$slope = r*\\frac{SD_y}{SD_x}$$\n",
    " \n",
    "where $r$ is the correlation coefficient, SD_y is the standard deviation of y, and SD_x is the standard deviation of x. \n",
    "\n",
    "Let's put this equation into a function and use it to calculate the slope of the regression line for the fake data sets we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_slope(x, y):\n",
    "    r = np.corrcoef(x, y)[0,1]\n",
    "    return r*np.std(y)/np.std(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_fake1 = calc_slope(fake_linear_x1, fake_linear_y1)\n",
    "slope_fake2 = calc_slope(fake_linear_x2, fake_linear_y2)\n",
    "slope_fake3 = calc_slope(fake_linear_x3, fake_linear_y3)\n",
    "slope_fake4 = calc_slope(fake_linear_x4, fake_linear_y4)\n",
    "print(slope_fake1,slope_fake2,slope_fake3,slope_fake4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we were able to find the slopes perfectly since there was no noise! Now let's update the function to create fake data by adding noise to it, and see how we do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fake_linear(n, a, b, noise_sd=None):\n",
    "    x = np.random.randn(n)\n",
    "    y = a * x + b\n",
    "    if noise_sd is not None:\n",
    "        y += np.random.randn(n)*noise_sd\n",
    "    return (x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we'll create some fake linear data that has noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_linear_x5,fake_linear_y5 = create_fake_linear(100, 1, 0, .3)\n",
    "fake_linear_x6,fake_linear_y6 = create_fake_linear(100, 2, 0, .3)\n",
    "fake_linear_x7,fake_linear_y7 = create_fake_linear(100, .3, 0, .3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use the slope function to estimate the slope that best fits this data, but it won't be perfect because we've added noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_fake5 = calc_slope(fake_linear_x5, fake_linear_y5)\n",
    "slope_fake6 = calc_slope(fake_linear_x6, fake_linear_y6)\n",
    "slope_fake7 = calc_slope(fake_linear_x7, fake_linear_y7)\n",
    "print('True Value: %.03f, Estimated Value: %.03f' % (1, slope_fake5))\n",
    "print('True Value: %.03f, Estimated Value: %.03f' % (2, slope_fake6))\n",
    "print('True Value: %.03f, Estimated Value: %.03f' % (.3, slope_fake7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's still pretty good, the estimated values for the slope are very close to the original values! \n",
    "\n",
    "Let's plot the data along with the regression line to visualize what's happening. Note that fake data sets 5, 6 & 7 were created with an intercept of 0, so we don't need to estimate an intercept for these plots to look good. Normally you would, and we'll learn how to do that next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(fake_linear_x5, fake_linear_y5, '.')\n",
    "plt.plot(fake_linear_x5, fake_linear_x5 * slope_fake5)\n",
    "plt.axis('square')\n",
    "plt.title('Fake Data 5')\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(fake_linear_x6, fake_linear_y6, '.')\n",
    "plt.plot(fake_linear_x6, fake_linear_x6 * slope_fake6)\n",
    "plt.axis('square')\n",
    "plt.title('Fake Data 6')\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(fake_linear_x7, fake_linear_y7, '.')\n",
    "plt.plot(fake_linear_x7, fake_linear_x7 * slope_fake7)\n",
    "plt.axis('square')\n",
    "_ = plt.title('Fake Data 7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout Session\n",
    "\n",
    "1\\. Create a new pair of fake $x$ and $y$ that is 100 long called `fake_breakout_x` and `fake_breakout_y`, with `a=3.2`, `b=4.2` and `noise_sd=10`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "### END STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Now calculate the slope for a linear regression between these two new variables. How close is it to the original `a`? Why is it close or far from the original value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "### END STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the Intercept: `b`\n",
    "\n",
    "The intercept is the point on the y axis where the regression line intersects it. We can estimate it using the following equation derived in the same manner as that for the slope:\n",
    "\n",
    "$$intercept = \\bar{y} − slope*\\bar{x}$$ \n",
    "\n",
    "where $\\hat{y}$ is the mean of $y$, and $\\hat{x}$ is the mean of $x$.\n",
    "\n",
    "Let's put that equation into a helper function and use it on our fake data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_intercept(x, y):\n",
    "    return np.mean(y) - calc_slope(x, y) * np.mean(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept_fake1 = calc_intercept(fake_linear_x1, fake_linear_y1)\n",
    "intercept_fake2 = calc_intercept(fake_linear_x2, fake_linear_y2)\n",
    "intercept_fake3 = calc_intercept(fake_linear_x3, fake_linear_y3)\n",
    "intercept_fake4 = calc_intercept(fake_linear_x4, fake_linear_y4)\n",
    "\n",
    "print('Real Intercept: %.03f, Estimated Intecept: %.03f' % (0,intercept_fake1))\n",
    "print('Real Intercept: %.03f, Estimated Intecept: %.03f' % (4,intercept_fake2))\n",
    "print('Real Intercept: %.03f, Estimated Intecept: %.03f' % (0,intercept_fake3))\n",
    "print('Real Intercept: %.03f, Estimated Intecept: %.03f' % (-3,intercept_fake4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we did perfect because there was no noise in this data. Let's add some noise and see how we do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_linear_x8, fake_linear_y8 = create_fake_linear(100, 1.5, 5, .5)\n",
    "fake_linear_x9, fake_linear_y9 = create_fake_linear(100, 5, -.4, .5)\n",
    "fake_linear_x10, fake_linear_y10 = create_fake_linear(100, -2, 2, .5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll estimate both the slope and intercept now so we can plot the data with the regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept_fake8 = calc_intercept(fake_linear_x8, fake_linear_y8)\n",
    "intercept_fake9 = calc_intercept(fake_linear_x9, fake_linear_y9)\n",
    "intercept_fake10 = calc_intercept(fake_linear_x10, fake_linear_y10)\n",
    "\n",
    "slope_fake8 = calc_slope(fake_linear_x8, fake_linear_y8)\n",
    "slope_fake9 = calc_slope(fake_linear_x9, fake_linear_y9)\n",
    "slope_fake10 = calc_slope(fake_linear_x10, fake_linear_y10)\n",
    "\n",
    "print('Real Intercept: %.03f, Estimated Intecept: %.03f' % (5,intercept_fake8))\n",
    "print('Real Intercept: %.03f, Estimated Intecept: %.03f' % (-.4,intercept_fake9))\n",
    "print('Real Intercept: %.03f, Estimated Intecept: %.03f' % (2,intercept_fake10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we did a good job! Let's see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(fake_linear_x8, fake_linear_y8, '.')\n",
    "plt.plot(fake_linear_x8, fake_linear_x8 * slope_fake8 + intercept_fake8)\n",
    "plt.axis('square')\n",
    "plt.title('Fake Data 8')\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(fake_linear_x9, fake_linear_y9, '.')\n",
    "plt.plot(fake_linear_x9, fake_linear_x9 * slope_fake9 + intercept_fake9)\n",
    "plt.axis('square')\n",
    "plt.title('Fake Data 9')\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(fake_linear_x10, fake_linear_y10, '.')\n",
    "plt.plot(fake_linear_x10, fake_linear_x10 * slope_fake10 + intercept_fake10)\n",
    "plt.axis('square')\n",
    "_ = plt.title('Fake Data 10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout Session\n",
    "\n",
    "1\\. Estimate the intercept of a linear regression of `fake_breakout_x` onto `fake_breakout_y`. Is it close to the original `b`? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "### END STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship between Correlation and Simple Linear Regression\n",
    "\n",
    "As was suggested earlier, there is a very intersting relationship between correlation and the slope in a **Simple Linear Regression** (which is a regression with only 1 independent variable). \n",
    "\n",
    "If we standardize (z-score) both our x (independent) and y (dependent) variables, then the slope we find using linear regression is exactly the same as the correlation, and the intercept is 0. Let's have a look, first we'll z-score our fake data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_linear_x10_z = scipy.stats.zscore(fake_linear_x10)\n",
    "fake_linear_y10_z = scipy.stats.zscore(fake_linear_y10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate the correlation, slope and intercept for the z-scored data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_intercept10_z = calc_intercept(fake_linear_x10_z, fake_linear_y10_z)\n",
    "fake_slope10_z = calc_slope(fake_linear_x10_z, fake_linear_y10_z)\n",
    "fake_correlation10_z = np.corrcoef(fake_linear_x10_z, fake_linear_y10_z)[0,1]\n",
    "\n",
    "# print out the results\n",
    "print('Linear Regression Intercept: %.06f' % (fake_intercept10_z))\n",
    "print('Linear Regression Slope: %.06f' % (fake_slope10_z))\n",
    "print('Correlation: %.06f' % (fake_correlation10_z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout Session\n",
    "\n",
    "1\\. Now z-score `fake_breakout_x` and `fake_breakout_y` and recalculate the slope on these z-scored vectors. Confirm this gives you a correlation by calculating the correlation between the original (non-zscored) vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "### END STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the line which minimizes error\n",
    "\n",
    "Now that we have an idea of what the slope and intercept are, let's get an intuitive sense for what values will give a good \"fit\" to the data using some interactive plots. This plot has two sliders, one for the slope and one for the intercept. Moving the sliders will change the slope and intercept of the red line in the plot. The data points are draw in blue, and there are lines between each data point and the regression line in green. You want the sum of all of these lines to be as small as possible. That sum is printed as a number beneath the plot. \n",
    "\n",
    "Try to adjust the slope and intercept to find the minimum value of that sum at the bottom, which is exactly what linear regression does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_error_fake(slope, intercept):\n",
    "    return show_error_scatter(fake_linear_x10.reshape(-1,1), fake_linear_y10.reshape(-1,1), slope, intercept)\n",
    "\n",
    "_ = interact(show_error_fake, \n",
    "             slope=FloatSlider(value=0, min=-5, max=5), \n",
    "             intercept=FloatSlider(value=0, min=-5, max=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Linear Regression in Two Ways\n",
    "\n",
    "We learned that we can plot two vectors as time series on a line plot to get an idea of how correlated they are. We can do a very similar thing to try and guess the values for the slope and intercept of a linear regression between two vectors. This interactive plot is the same as the last one, with the addition of a time series line plot. Have a look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_linear_x10.reshape(100,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_error_fake10_both(slope, intercept):\n",
    "    return show_error_both_ways(fake_linear_x10, fake_linear_y10, slope, intercept)\n",
    "\n",
    "interactive_plot = interactive(show_error_fake10_both, \n",
    "                               slope=widgets.FloatSlider(value=0, min=-5, max=5), \n",
    "                               intercept=widgets.FloatSlider(value=0, min=-5, max=5))\n",
    "output = interactive_plot.children[-1]\n",
    "output.layout.height = '250px'\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout Session\n",
    "\n",
    "1\\. What operations are applied to the response vector when the 'slope' and 'intersept' sliders are changed? Why does changing the values cause the error value to increase or decrease?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "### END STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression as a Model\n",
    "\n",
    "Linear Regression is a type of statistical model that relates the $x$ (independent variable) to the $y$ (dependent variable. But what is a statistical model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a \"model\"?\n",
    "\n",
    "A **mathematical model** is a description of a system using mathematical concepts and language, usually in the form of one or more equations. A **statistical model** is a type of mathmatical model that includes assumptions about the system it is modeling, in the form of probability distributions of the random varables in the model.\n",
    "\n",
    "Burnham & Anderson state: \"A model is a simplification or approximation of reality and hence will not reflect all of reality\" - whence the saying \"all models are wrong\". And that's why it's called a model. Think of a model car. Model cars look like a car, but are a simplification of a real car, not the real thing. All of science creates mathmatical models to formally describe the systems they study in a simple way.\n",
    "\n",
    "So that's a more formal definition, but what is the intuition? Intuitivel, a model defines the types of relationships between the variables being modeled, and should be able to make a prediction about the outcome of system it is modeling. In our case, we are trying to model brain activity as recorded by fMRI. So the model of a voxel consists of a mathmatical equation and probablility distribution of the data. That model describes both the set of variables that affect the voxels' output, and the amount of affect each of those variables has on the output.\n",
    "\n",
    "Let's use the category localizer that we've been using in the last few lectures as an example of a statistical linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model to the data to get the coefficients\n",
    "\n",
    "We've seen how to calculate the intercept and the slope separately, but there is actually an easier way to calculate all the **weights**, which are the values for the intercept, and slope. In the language of linear regression the slope (or slopes when there are multiple $x$ as we'll see later) are called the **coefficients**. We say that we **fit** the entire linear model, which calculates the intercept and all the coefficients at one time. \n",
    "\n",
    "In order to fit the linear model we've created, we need to get more formal about what that model is. We've already seen the linear equation, but when we write the linear model equation there is an additional term, called the error term. Let's have a look:\n",
    "\n",
    "\\begin{align}\n",
    "y_j =  w_0 + w_1 * x_j +\\epsilon_j\n",
    "\\end{align}\n",
    "\n",
    "where\n",
    "- $y_j$ is the value of the BOLD signal at time=j\n",
    "- $w_0$ is the intercept term\n",
    "- $w_1$ is the slope\n",
    "- $x_j$ is the value of the response vector at time=j\n",
    "- $\\epsilon$ is the error in the model\n",
    "\n",
    "We need the error term because noise exists in our data, and so our model is not perfect. As we've seen when finding the slope and intercept, when we fit our model we cannot exactly recover $w_0$ and $w_1$. However, we would like to find a solution for $w_0$ and $w_1$ that minimizes the following error between the predicted values and the real values of the BOLD signal, as we saw in the plots above. This error value is the **Sum of Squared Errors (SSE)**, and we would like to minimize it as much as possible. Below is the equation for the sum of squared errors:\n",
    "\n",
    "\\begin{align}\n",
    "SSE = \\sum_{j = 1}^N (y_j - X_j W)^2\n",
    "\\end{align}\n",
    "\n",
    "The equations for the slope and intercept that we saw above actually minimuze the $SSE$ and give us $w_0$ and $w_1$ To show how to minimize this equation for the entire model involves calculas and linear algebra which are beyond this course, but once we do that minimization, we arrive at the **Least Squares** solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the slope and intercept with `LinearRegression`\n",
    "Now we'll use an object called `LinearRegression()` from the `sk-learn` python module, which implements exactly the above solution to fit a linear regression model. To use it is fairly simply. \n",
    "\n",
    "1. First, create the `LinearRegression()` object. It has some arguments, but the defaults are just what we need for this class, so you don't have to pass any in.\n",
    "2. Use the `fit()` function of this object, and pass it the x (independent) and y (dependent) variables you want to model. It will fit the model and store the values of $w_0$ and $w_1$ as two properties of the object, called `intercept_` and `coef_` respectively.\n",
    "\n",
    "Let's try this out. First we'll import the `LinearRegression` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `LinearRegression` object requires that both $x$ and $y$ are 2-d matrices, so we'll need to reshape the 1-D vectors we've been using into 2-D matrices. This is easily done using `.reshape(-1,1)`. Remember from an earlier lecture that passing `-1` to `reshape` tells it to automatically calculate the size of that dimension, and since we're telling it we want 1 column (2nd axes), it will turn the `-1` into the length of the vector. Let's do that here for both x and y. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_linear_x10_2D = fake_linear_x10.reshape(-1,1) \n",
    "fake_linear_y10_2D = fake_linear_y10.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create a linear regression object, and use it to fit the fake x and y 2-D data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model1 = LinearRegression()\n",
    "reg_model1.fit(fake_linear_x10_2D, fake_linear_y10_2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see if the intercept and slope we estimated by fitting the linear model are the same as those we calculated using our functions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model Intercept: %.04f, and slope: %.06f' % (reg_model1.intercept_, reg_model1.coef_))\n",
    "print('Functions Intercept: %.04f, and slope: %.06f' % (intercept_fake10, slope_fake10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And they are!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout Session\n",
    "\n",
    "1\\. Fit a linear regression model to the `fake_linear_x9` and `fake_linear_y9`. Print out the intercept and coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "### END STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted Values and Calculating SSE for a Linear Model\n",
    "\n",
    "We now know that Linear Regression minimizes the Sum of Squared Error (SSE), which means that we can use it to get an idea of how well the model fits the data. Remember the equation for SSE is:\n",
    "\n",
    "$$SSE = \\sum_{j = 1}^N (y_j - X_j W)^2$$\n",
    "\n",
    "Let's see how to calculate it and get an idea of exactly what it is. The first step in calculating SSE is to figure out the predicted values of $y$, %\\hat{y}%. We learned above that the predicted values of y are achieved by applying the linear equation $x$:\n",
    "\n",
    "$$\\hat{y} = a * x + b$$ \n",
    "\n",
    "Let's calculate this using the slope and intercept we calculated with our functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_linear_y10_hat = slope_fake10*fake_linear_x10 + intercept_fake10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot these predicted values (\"y hat\"), along with the original data to see how they relate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.plot(fake_linear_x10, fake_linear_y10, 'xk', label='Original')\n",
    "plt.plot(fake_linear_x10, fake_linear_y10_hat, '.r', label='Predicted')\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you noticed that the predicted values seem to fall directly on the regression line, you're correct! The regression line is the best guess we have for every value of $x$, and so every predicted value of $x$ is the value of the regression line at that point in $x$! Let's prove this by plotting the predicted values and the regression line itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.plot(fake_linear_x10, fake_linear_x10 * slope_fake10 + intercept_fake10, 'k', label='Regression Line')\n",
    "plt.plot(fake_linear_x10, fake_linear_y10_hat, '.r', label='Predicted')\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that the `LinearRegression` object has a function that will calculate the predicted values for us. It is conveniently called `.predict` and it takes the $x$ values you want to predict using the model. Then we'll test to make sure we get the same answer that we just calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_linear_y10_pred = reg_model1.predict(fake_linear_x10_2D).reshape(-1)\n",
    "np.allclose(fake_linear_y10_pred, fake_linear_y10_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the predicted values calculating SSE is straightforward. The name Sum of Squared Errors very accurately describes the calculations involved. Remember that our model included an error term called $\\epsilon$. If we rewrite the linear model, we can find out how to calculate the error:\n",
    "\n",
    "$$ \\epsilon = y - (a * x + b)$$\n",
    "\n",
    "And we can substitute the equation for $\\hat{y}$ in to get the following:\n",
    "\n",
    "$$ \\epsilon = y - \\hat{y}$$\n",
    "\n",
    "So to get the Sum of the Squared Errors, we calculate the error, square each value, and sum them up! Let's do that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sse_fake_linear10 = np.sum((fake_linear_y10-fake_linear_y10_pred)**2)\n",
    "sse_fake_linear10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That number might look familiar. If you were able to find the best solution for the slope and intersect in the interactive plot above, this is the same error number that would have been printed beneath the plot!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout Session\n",
    "\n",
    "1\\. Calculate the predicted values for the linear model of `fake_linear_x9`, and use them to calculate the SSE for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "### END STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression for real fMRI data\n",
    "\n",
    "Now let's apply linear models to some real fMRI data. We'll fit the faces response vector to our FFA voxel and see how we do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model_faces = LinearRegression()\n",
    "reg_model_faces.fit(faces_response_vec, data_faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh-Oh, why didn't that work? Remember that we need to reshape vectors to 1-D matrices. This is annoying now, but will become useful once we want to find regression models to multiple voxels at the same time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model_faces = LinearRegression()\n",
    "reg_model_faces.fit(faces_response_vec.reshape(-1,1), data_faces.reshape(-1,1))\n",
    "reg_model_faces.intercept_, reg_model_faces.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the SSE for this faces model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_faces_pred_faces = reg_model_faces.predict(faces_response_vec.reshape(-1,1))[:,0]\n",
    "faces_sse = np.sum((data_faces - data_faces_pred_faces) **2)\n",
    "faces_sse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the data and the regression line to see how well it fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.plot(faces_response_vec, faces_response_vec * reg_model_faces.coef_[0] + reg_model_faces.intercept_[0], 'k', label='Regression Line')\n",
    "plt.plot(faces_response_vec, data_faces, '.r', label='Face Data')\n",
    "plt.xlabel('Faces Response')\n",
    "plt.ylabel('BOLD Signal')\n",
    "plt.title('FFA Voxel Regression')\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout Session\n",
    "\n",
    "1\\. Fit a linear regression model to the PPA voxel BOLD data using the `places` response vector. Plot the data as a scatter plot, and the regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "### END STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression\n",
    "\n",
    "The real power of regression comes when we have more than one **independent variable** we want to associate with the **dependent variable**.\n",
    "\n",
    "The case of multiple linear regression is a natural extension of the simple linear regression case. In multiple regression, we have multiple independent variables (x), and still a single dependent variable (y). Now the equation describing each time point of data looks like this: \n",
    "\n",
    "\\begin{align}\n",
    "y_j =  w_0 + w_1 x^1_j + w_2 x^2_j ... + w_3 x^d_j +\\epsilon_j\n",
    "\\end{align}\n",
    "\n",
    "We can rewrite this second equation for multiple regression using a more succint notation:\n",
    "\n",
    "\\begin{align}\n",
    "Y =  {\\bf X} * W +\\epsilon\n",
    "\\end{align}\n",
    "\n",
    "where\n",
    "- $Y$ is the n x 1 array of BOLD data\n",
    "- ${\\bf X}$ is the n x 2 matrix were the first column is all 1s for the intercept and the second column is the response vector\n",
    "- ${W}$ is 2 x 1, where the first value is $w_0$ and the second value is $w_1$\n",
    "- $\\epsilon$ is the n x 1 vector or errors, or residuals\n",
    "\n",
    "We can write the solution to this multiple regression model as the following equation, which is what we will use to fit our model. This is just for your own information, you don't need to know this equation for any exams.:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat W = ({\\bf X}^\\top{\\bf X})^{-1}{\\bf X}^\\top Y\\\\\n",
    "\\end{align}\n",
    "\n",
    "As it turns out, the equation that minimizes the Sum of Squared Errors (SSE) is exactly the same for the multiple regression case as it is for the simple linear regression case, so we can use the same `LinearRegression` function for both! Let's explore this..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending the Face Model: Modeling Faces and Bodies\n",
    "\n",
    "Since we know that the subject saw more than just images of faces, it makes sense that we would want to model more than just the response to faces in order to get a model that better explains the BOLD data (y). Now let's see what happens if we include the bodies response vector in the model as well..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuition using time series plots\n",
    "\n",
    "First let's plot the FFA BOLD data along with the response vectors for the faces and bodies to remind ourselves what the signal looks like when bodies were shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(18, 6))\n",
    "_ = plt.plot(data_faces, label = 'Bold Data')\n",
    "_ = plt.plot(faces_response_vec, label = 'Faces')\n",
    "_ = plt.plot(bodies_response_vec, label = 'Bodies')\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create a 2-D $x$ matrix, which is called a **design matrix**, from the response vectors for bodies and faces. We'll use `np.stack` to do that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "designmat_faces_bodies = np.stack((faces_response_vec, bodies_response_vec), axis=1)\n",
    "designmat_faces_bodies.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And reshape the FFA voxel BOLD data into a 2-D matrix with one column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_faces_2D = data_faces.reshape(len(data_faces),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_error_faces_bodies(intercept, slope_faces, slope_bodies):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    show_error_time(designmat_faces_bodies, \n",
    "                    data_faces_2D, \n",
    "                    [slope_faces, slope_bodies], \n",
    "                    intercept)\n",
    "\n",
    "interactive_plot = interactive(show_error_faces_bodies, \n",
    "                               intercept=widgets.FloatSlider(value=0, min=-5, max=5), \n",
    "                               slope_faces=widgets.FloatSlider(value=0, min=-5, max=5), \n",
    "                               slope_bodies=widgets.FloatSlider(value=0, min=-5, max=5))\n",
    "output = interactive_plot.children[-1]\n",
    "output.layout.height = '350px'\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model\n",
    "\n",
    "Now let's fit a multiple regression model using both the faces and the bodies response vectors independent variables. We just learned that the same equation estimates the coefficients for simple and multiple linear regression, so we can use the same `LinearRegression` object to fit this multiple regression model. Instead of reshaping a single response vector into a 2-D matrix, we'll use the design matrix we just created. We'll use the reshaped FFA voxel BOLD data and fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model_face_body = LinearRegression()\n",
    "reg_model_face_body.fit(designmat_faces_bodies, data_faces_2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the coefficient for faces from this model and from the first regression model of faces that we built. There are 2 slope coefficients now, and since the faces response vector was in the first column, the first coefficient is for faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Faces Coefficient with Bodies: %.03f' % (reg_model_face_body.coef_[0][0]))\n",
    "print('Faces Coefficient Only: %.03f' % (reg_model_faces.coef_[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two coefficients are similar, but not the same! We'll learn more about why this is later on. \n",
    "\n",
    "For now let's predict the original data using this faces and bodies model, and then calculate the sum of squared error (SSE) of this new model. Then we can compare this SSE value with the SSE we calculated above for the faces only model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_faces_pred_facebody = reg_model_face_body.predict(designmat_faces_bodies)[:,0]\n",
    "faces_bodies_sse = np.sum((data_faces - data_faces_pred_facebody)**2)\n",
    "faces_bodies_sse, faces_sse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the SSE for the faces bodies model is lower than that for the faces only model we can conclude that including the bodies response vector into the model improves the fit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout Session\n",
    "\n",
    "1\\. Fit a multiple linear regression model to the PPA voxel that has 2 independent variables, the `scenes` and `object` response vectors. Calculate the predicted values, and use them to calculate the SSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "### END STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Full Model with All the Categories\n",
    "\n",
    "We saw that the model that incorporated bodies and faces did a better job than the model which just incorporated faces. Let's see what happens when we include the response vectors that describe all the stimuli that the subject saw.\n",
    "\n",
    "First we need to create the design matrix for the all the stimulus types. We'll use `np.stack` again to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "designmat_full = np.stack((faces_response_vec, bodies_response_vec, places_response_vec, objects_response_vec, scrambled_response_vec), axis=1)\n",
    "designmat_full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll fit a multiple regression model in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model_full = LinearRegression()\n",
    "reg_model_full.fit(designmat_full, data_faces_2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's predict and calculate the SSE again, so we can compare it with the faces bodies model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_faces_pred_full = reg_model_full.predict(designmat_full)[:,0]\n",
    "full_sse = np.sum((data_faces-data_faces_pred_full)**2)\n",
    "print('Just using Faces and bodies, SSE is: %.02f, adding all categories SSE is: %.02f' % (faces_bodies_sse, full_sse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the SSE is slightly lower when adding all of the response vectors, but not by much! What matters in statistcs is whether this is **significantly** lower. We'll learn about significance testing later in the semester."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout Session\n",
    "\n",
    "1\\. Fit the full model (of all 5 stimulus types) to the PPA voxel. Does the SSE increase dramatically relative to the SSE from the model of just scenes and objects?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "### END STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So Why Do We Need Multiple Regression?\n",
    "\n",
    "We just saw that simple and multiple linear regression don't always estimate the same value for coefficients of the same $x$ variable (the faces response). So why would we want to use multiple regression instead of estimate the coffecicents of a number of simple linear regression models?\n",
    "\n",
    "The answer is that when there is a correlation between the $x$ variables, multiple regression can account for that correlation, whereas doing several simple regressions cannot. In other words, multiple regression controls for correlation in the independent variables that lead to a more accurate estimation of all the slopes.\n",
    "\n",
    "Let's take a look at an example to see how this works. We'll create 2 independent variables that are correlated, and then use the linear equation to create the y data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_multi_x1, fake_multi_x2 = create_fake_data(100, 0, 0, .5)\n",
    "intercept_real1 = 0\n",
    "coef_real1 = 1.5\n",
    "coef_real2 = -2\n",
    "fake_multiple_y1 = intercept_real1 + fake_multi_x1 * coef_real1 + fake_multi_x2 * coef_real2 \n",
    "fake_multiple_y1 += np.random.randn(100) * .3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use 2 simple linear models to estimate the two coefficeints separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model_simple1 = LinearRegression()\n",
    "reg_model_simple1.fit(fake_multi_x1.reshape(-1,1), fake_multiple_y1.reshape(-1,1))\n",
    "reg_model_simple2 = LinearRegression()\n",
    "reg_model_simple2.fit(fake_multi_x2.reshape(-1,1), fake_multiple_y1.reshape(-1,1))\n",
    "print('Real Coefficient 1: %.03f, Simple Estimated Coefficient 1: %.03f' % (coef_real1, reg_model_simple1.coef_[0][0]))\n",
    "print('Real Coefficient 2: %.03f, Simple Estimated Coefficient 2: %.03f' % (coef_real2, reg_model_simple2.coef_[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not so great. Now let's see what happens when we put both independent variables in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "designmat_fake_multiple = np.stack((fake_multi_x1, fake_multi_x2), axis=1)\n",
    "reg_model_multi1 = LinearRegression()\n",
    "reg_model_multi1.fit(designmat_fake_multiple, fake_multiple_y1.reshape(-1,1))\n",
    "print('Real Coefficient 1: %.03f, Simple Estimated Coefficient 1: %.03f' % (coef_real1, reg_model_multi1.coef_[0][0]))\n",
    "print('Real Coefficient 2: %.03f, Simple Estimated Coefficient 2: %.03f' % (coef_real2, reg_model_multi1.coef_[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's more like it! \n",
    "\n",
    "So the multiple regression model can account for the correlation between the two independent variables, and therefore find a much better estimate of the coefficients! That's why it's important to include all the independent variables you think may influces the dependent variable. "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
