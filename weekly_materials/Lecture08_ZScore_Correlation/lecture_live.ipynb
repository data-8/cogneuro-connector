{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 8: Temporal Normalization (Z-Score) and Correlation\n",
    "\n",
    "## Goals\n",
    "\n",
    "- **Neuroscience / Neuroimaging concepts**\n",
    "    - Correlating neural signals with experimental design\n",
    "- **Datascience / Coding concepts**\n",
    "    - Normalization\n",
    "    - Z-scoring\n",
    "    - Conceptual Interpretation of Correlation\n",
    "    - Visual Intepretation of correlation\n",
    "    - Algebraic Interpretation of correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key terms and concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Simply run the cells below that contain all the Python modules we'll neeed, plus setup matplotlib for plotting in this jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import nibabel\n",
    "import cortex\n",
    "import os\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function for loading data\n",
    "Remember coding a function for loading brain data two homeworks ago? Here we can reuse it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nifti(file_name, mask=None):\n",
    "    img = nibabel.load(file_name)\n",
    "    data = img.get_data().T\n",
    "    if mask is None:\n",
    "        masked_data = data\n",
    "    else:\n",
    "        masked_data = data[:, mask]\n",
    "    return masked_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution\n",
    "\n",
    "Last lecture we learned about event related designs in comparison to block designs. We learned about how it becomes absolutely critical to take the hemodynamic response lag into account if our experiment utilizes rapid successions of events. \n",
    "\n",
    "We learned that the operation by which we can go from a time series representing the experimental events (blocks or short events) to a timeseries that resembles the BOLD response is *convolution* of the experimental time series with the *hemodynamic response function (HRF)*.\n",
    "\n",
    "Let's take another quick look at this to refresh our memory.\n",
    "\n",
    "We'll first load the labels of our category localizer, convolve the blocks with the HRF, then load a voxel time series from FFA and one from PPA and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_labels = np.load(\"/home/jovyan/catloc_experimental_conditions.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data from the midterm and extract some voxel time series which we know vary with the stimulus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/home/jovyan/s01_categories_all_z.nii\"\n",
    "midterm_data = load_nifti(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify the voxel location for a face selective voxel in FFA, and then read it into `z, y, x` coordinates, and use these to extract the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_voxel_loc = (6, 57, 37)\n",
    "\n",
    "z, y, x = face_voxel_loc\n",
    "faces_timeseries = midterm_data[:, z, y, x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_timeseries.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, this time series is currently not centered around zero, as we can see if we take the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_timeseries.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's subtract this mean out to center the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_timeseries_centered = faces_timeseries - faces_timeseries.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the two time series as line plots to do a quick comparison of this time series to the stimulus vector for faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(faces_timeseries_centered)\n",
    "plt.plot(category_labels == 'faces')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also create a hemodynamic response function, which we will use to convert the stimulus vector to the response vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nistats.hemodynamic_models import glover_hrf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrf = glover_hrf(tr=2, oversampling=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the homework, you wrote a function to perform convolution. Let's recall what convolution does and take another look at the function.\n",
    "\n",
    "Convolution combines two signals, for example the stimulus vector and the hrf, by moving one signal across the other and incrementing an output array by the product of the two at every shift.\n",
    "\n",
    "It is used, for example, to compute the response of a system with certain assumed properties (linear and time-invariant), to a stimulus.\n",
    "\n",
    "Let's take another close look at the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution(stimulus, impulse_response):\n",
    "    # output length for a full convolution will be longer than the stimulus\n",
    "    out_length = len(stimulus) + len(impulse_response) - 1\n",
    "    # preallocate output\n",
    "    output = np.zeros(out_length)\n",
    "    # iterate over entries of stimulus\n",
    "    for i in range(len(stimulus)):\n",
    "        # get current stimulus value\n",
    "        cur_stim_val = stimulus[i]\n",
    "        # multiply impulse response by this stimulus value\n",
    "        multiplied_response = cur_stim_val * impulse_response\n",
    "        # add multiplied response to output starting at position i\n",
    "        output[i:i + len(impulse_response)] += multiplied_response\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_stimulus = category_labels == 'faces'\n",
    "faces_response = convolution(faces_stimulus, hrf)\n",
    "faces_stimulus.shape, faces_response.shape, hrf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(faces_timeseries_centered, label='BOLD Signal')\n",
    "plt.plot(faces_stimulus, '.-.', label='Stimulus')\n",
    "plt.plot(faces_response, lw=3, label='Response')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we do the convolution the other way round?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_response_2 = convolution(hrf, faces_stimulus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(faces_response_2, lw=3)\n",
    "plt.plot(faces_response, 'y-.', lw=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are the same! It doesn't matter which parameter goes first in convolution!\n",
    "\n",
    "We shorten the vector to have it be the same size as the stimulus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_response = faces_response[:len(faces_stimulus)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakout session\n",
    "Let's display the model time series and the data time series for a place voxel. The instructions will mimic what we have done in this lecture up to now, so you will be able to copy and paste code.\n",
    "\n",
    "1. Set the name `place_voxel_loc` to `(10, 62, 40)`. This is the location of the place-sensitive voxel of interest\n",
    "2. Use `place_voxel_loc` as above to extract the time series and call it `places_timeseries`\n",
    "4. Create a name `places_stimulus` which is `True` whenever a place is shown to the subject.\n",
    "5. Create `places_response` using `np.convolve` or `convolution` with `places_stimulus` and the `hrf`.\n",
    "6. Shorten `places_response` to the length of the `places_stimulus` by cutting off points at the end.\n",
    "6. Create a figure and plot `places_timeseries` and `places_response`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Normalization (Z-Score)\n",
    "\n",
    "Now that we know how to make a response vector that is a good prediction of what the BOLD signal would look like when responding to a given stimulus (or task) type, we want to quantify how well that response vector characterizes the BOLD signal with numbers. To do this we're going to use correlation. From one perspective, the first step in correlation is to z-score both the response vector and the BOLD signal data. Let's learn about temporal normalization and z-scoring before we dive into correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "Within statistics and signal processing the term **normalization** refers to a number of different computations that \"adjust values measured on different scales to a notionally common scale\". That \"notionally common scale\" can be the range between zero and one, for example, meaning the maximum value in the data becomes `1` and the minimum value in the data becomes `0`. **Normalization** is done for a number of reasons ranging from simply making data more interpretable, to removing irrelevant differences in signals to allow for their joint analyses. For more on **normalization** see <a href=\"https://en.wikipedia.org/wiki/Normalization_(statistics)\">here</a>.\n",
    "\n",
    "**Temporal normalization** is simply the process of normalization applied to data in the time domain. In the case of fMRI, that means normalizing each voxel time series, across all the runs of a subject so they are on the same scale. \n",
    "\n",
    "We'll cover two types of **normalization** today:\n",
    "\n",
    "1\\. **Feature Scaling:** Puts all the data into the range [0-1] (the brackets mean inclusive). <br/>Formula:\n",
    "\n",
    "\\begin{align}\n",
    "X^{\\prime} & = \\frac{X - X_{min}}{X_{max}-X_{min}}\n",
    "\\end{align}\n",
    "where $X^{\\prime}$ is the feature scaled data, $X$ is the original data, $X_{min}$ is the minimum value of the data, and $X_{max}$ is the maximum value of the data.\n",
    "\n",
    "2\\. **Standard Scoring (Z-Scoring):** Makes the mean of the data equal `0` and the standard deviation of the data equal `1`. <br/>Formula:\n",
    "\\begin{align}\n",
    "Z & = \\frac{X - \\mu}{\\sigma}\n",
    "\\end{align}\n",
    "where $X$ is the data, $\\mu$ is the mean of the data, and $\\sigma$ is the **standard deviation** of the data.\n",
    "\n",
    "Let's get familiar with **normalization** by exploring **feature scaling** one step at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Feature scaling is perhaps the simplest form of **normalization**, and is conceptually easy to understand. The idea is to move all of the data to a known range, in this case from [0-1]. Let's create a toy array of random data to practice with, then plot it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_random = np.random.randn(20)\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "_ = plt.plot(array_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of **feature scaling** is to remove the minimum value from the data. Let's see what that does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_random_offset = array_random - array_random.min()\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "_ = plt.plot(array_random_offset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huh, the plot looks the same, so what's the difference? Removing the minimum value does not change the shape of the data, it simply **offsets** (or shifts along the y-axis) the data, making the new minimum value equal to zero. So we can see the change only in the values of the y-axis. Notice the lowest value is now `0`, when it was around `-1` before.\n",
    "\n",
    "The second step of **feature scaling** is to divide all the data by the new maximum value. Let's do that here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_random_featurescaled = array_random_offset / array_random_offset.max()\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "_ = plt.plot(array_random_featurescaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What changed this time? Again, the plot looks the same, only the scale of the data (on the y-axis) has changed. By dividing the offset data by it's new max value, we've **scaled** the data so it's new maximum is `1`. That's true because dividing a number by itself always equals one, so dividing all the numbers by the maximum means the new maximum has to be `1`. Also, since the new minimum was `0`, dividing `0` by any number always returns zero. So now our data is guaranteed to be within the range [0-1]!\n",
    "\n",
    "Important to note is that **normalization** does not change the \"shape\" of your time series - it simply applies an **offset**, and then **scales** the data.\n",
    "\n",
    "We can combine both the offset and scaling into a single line of code for convenience. Let's remind ourselves of the equation for features scaling first:\n",
    "\n",
    "\\begin{align}\n",
    "X^{\\prime} & = \\frac{X - X_{min}}{X_{max}-X_{min}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_random_featurescaled2 = (array_random - array_random.min()) / (array_random.max() - array_random.min())\n",
    "\n",
    "print(array_random_featurescaled)\n",
    "print(array_random_featurescaled2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values from both ways of feature scaling look the same, but the code this new way looks a little different than when we did it the first time. That's because in the first way we did it, we used the maximum value from the array that already had the minimum removed. Since the new way uses the original `array_random` everywhere, we have to remove the minimum from the maximum explicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout Session\n",
    "\n",
    "1\\. Create a helper function to do feature scaling. Call the function `feature_scaling_simple`. It will have a single argument, the array to feature scale called `data`, and will return the feature scaled array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Now update the function you just created by adding a second argument that designates which axis the feature scaling should be done across. Call the second argument `dim` and make it default to `0`, and call the function `feature_scaling`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Finally, create a 2-D array of random numbers with shape `(4,8)`, call it `array_random2D`. Now use the two versions of the feature scaling function on it. Do you expect the same answer, or different answers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An application of feature normalization: Making a floating point RGB image\n",
    "\n",
    "Remember making a color image in a previous homework? We did this using three arrays as color channels. Let's do this again here, but let's set the image up such that the color `(0, 0, 0)` is a neutral gray, `(1, 1, 1)` is white, and `(-1, -1, -1)` is black. Any continuous value in between encodes colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = np.zeros((512, 512))\n",
    "R[:128, :256] = 1\n",
    "R[-128:, :256] = -1\n",
    "fig_R = plt.figure()\n",
    "plt.imshow(R, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = np.zeros((512, 512))\n",
    "G[64:-64, 128:192] = 1\n",
    "G[64:-64, :64] = -1\n",
    "fig_G = plt.figure()\n",
    "plt.imshow(G, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.zeros_like(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RGB = np.stack((R, G, B), axis=2)\n",
    "fig_RGB = plt.figure()\n",
    "plt.imshow(RGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huh, that doesn't seem to be showing what we wanted to show!\n",
    "\n",
    "This is because `matplotlib` has very specific ways of normalizing images to be displayed as RGB-values: If they are floats, only variation between 0 and 1 is considered valid. If they are `uint8`, then it is variation between 0 and 255 (as in the 2D array homework).\n",
    "\n",
    "Let's use feature normalization to bring our RGB-image into the [0, 1] range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RGB_normalized = feature_scale_simple(RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_RGB_normalized = plt.figure()\n",
    "plt.imshow(RGB_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakout Session\n",
    "\n",
    "1. Why do the colors look subdued in this image?\n",
    "2. Fill the array `B` with a rectangle that intersects some of the rectangles in the other channels. Assign to it a value of `1` for maximum blue channel. Make another rectangle with value `-1`. Re-stack the `RGB` image and re-plot the figure to take a look. Then normalize using feature scaling and plot the normalized\n",
    "3. Change one pixel of any channel to the value -1000. Re-stack the RGB image, plot the output and then plot the normalized image.\n",
    "4. Add another pixel with value 1000 and repeat 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems with Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature scaling** is very useful to put data into an easily interpretable scale, however a problem with this normalization method shows up when you have outlying values far way from the mean. What would happen if we add a very large outlying value at a single time point? Let's see!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_random_long = np.random.randn(100)\n",
    "array_random_outlier = array_random_long.copy()\n",
    "array_random_outlier[80] = array_random_outlier.max()*20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize what's happening in these data sets we'll use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_random_stacked = np.stack((feature_scale(array_random_long), feature_scale(array_random_outlier)), axis=1)\n",
    "\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(array_random_stacked, 10)\n",
    "_ = plt.legend(('Random Array', 'Random Array w/Outlier'))\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(array_random_stacked)\n",
    "_ = plt.legend(('Random Array', 'Random Array w/Outlier'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the histogram, the array with the outlier has most of its data lying in a single bin of the histogram. Feature scaling does not put the data from both arrays into a similar range, because the max value is not stable (it can change a lot depending on only one data point). As we've learned, (linear) data normalization involves an *offset* operation (by subtracting off some value) and a **scaling** operation (by dividing by some value or performing some nonlinear operation). \n",
    "\n",
    "A more robust and stable way to normalize data is to subtract the **mean** of the data instead of the min, and to divide by the **standard deviation** instead of the range (max - min). This is exactly what z-scoring does. Let's learn more about it now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Scoring (Z-Scoring)\n",
    "\n",
    "One of the most commonly used forms of **normalization** is to create standard scores, or z-scores. A z-scored dataset has a mean of 0, and a standard deviation of 1. So what's a standard deviation? Let's find out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Deviation\n",
    "\n",
    "**Standard deviation** is a descriptive statistic that measures the average distance away from the mean (in either direction) of a data set. When a data set has a small standard deviation, the data points tend to be close to the mean, when the standard deviation is large the data points tend to be far from the mean. \n",
    "\n",
    "Before we jump into the equation for the standard deviation, let's get a visual intuition for what the standard deviation looks like by plotting the distribution of some random Gaussian data. We learned earlier today that `np.random.randn` actually gives us **standard normal** data, whose **distribution** is Gaussian (shaped like the Bell curve), and whose mean is `0` and standard deviation is `1`. We'll plot the data using a histogram, since that plots the **distribution** of the data. Then we'll draw a line for the mean, and two lines for standard deviation on either side of the mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stddev = np.random.randn(1000)\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "_ = plt.hist(data_stddev, bins=20)\n",
    "_ = plt.plot([data_stddev.mean(), data_stddev.mean()], [0,150], label='Mean')\n",
    "_ = plt.plot([-data_stddev.std(), -data_stddev.std()], [0,150], 'r', label='Standard Deviation')\n",
    "_ = plt.plot([data_stddev.std(), data_stddev.std()], [0,150], 'r', label='Standard Deviation')\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at how to calculate the standard deviation. The formula is a bit complicated-looking, but we'll work through it step by step to see it's actually not that bad.\n",
    "\n",
    "$$\\hat{\\sigma} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (x_i - \\overline{x})^2}$$\n",
    "\n",
    "where $\\hat{\\sigma}$ is the estimated standard deviation, $N$ is the number of samples in your data set, $x_i$ is the ith data point, and $\\overline{x}$ is the mean of the data set.\n",
    "\n",
    "When tryring to understand a formula, it can be useful to pick it apart starting with the innermost calculation, and work outwards. Let's do that here, starting with:\n",
    "$$\\sum_{i=1}^N (x_i - \\overline{x})^2$$\n",
    "\n",
    "That Greek letter ($\\sum$) is an uppercase \"sigma\" and stands for the `sum` operation. In this case it means to sum all that's in the parentheses, across all the $x_i$ values. So this expression is saying, for each data point:\n",
    "1. Subtract the mean\n",
    "2. Square that difference\n",
    "\n",
    "And then sum all those squared differences. This sum is calculated many places in statistics, and is called the **sum of squared differences**, or **sum of squares** for short. Let's do that here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = array_random\n",
    "differences = x - x.mean()\n",
    "differences_squared = differences**2\n",
    "sum_squares = differences_squared.sum()\n",
    "sum_squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can put that all into a single line of code like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_squares = np.sum((x - x.mean())**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was really the hard part! Now we take that value, divide it by $N$, and take the square root:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = x.shape[0]\n",
    "std_dev = np.sqrt(sum_squares / N)\n",
    "std_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard deviation is such a common operation that there is a function for it, `std`. Let's verify we got the same answer as the numpy function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout Session\n",
    "1\\. Remember when we computed the SNR of a brain scan? Let's look back at the function we used to calculate noise in a volume. Describe the 3 calculations this function makes to calculate the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_noise(data):\n",
    "    \n",
    "    # determine the voxels that are outside the brain, \n",
    "    # assuming their value is around 300 in the first volume\n",
    "    outside_brain_mask = data[0] <= 300\n",
    "    \n",
    "    # get all the voxels that are not in the brain, for each time point\n",
    "    outside_brain_vox = data[:,outside_brain_mask]\n",
    "    \n",
    "    # find the standard deviation of the voxels outside the brain, for\n",
    "    # each volume across time\n",
    "    data_std = np.std(outside_brain_vox, axis=0)\n",
    "    \n",
    "    # take the average of all of those standard deviations\n",
    "    noise = np.mean(data_std)\n",
    "    return noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Z-Scores\n",
    "\n",
    "Now that we know what a standard deviation is, we can calculate z-scores. Remember that z-scoring makes the mean of the data `0`, and the standard deviation of the data `1`. To do that we'll use the same logic as we did with **feature scaling**. To make the mean zero, we'll **offset** (or subtract) the data by the mean. Then to make the standard deviation `1`, we'll divide that by the standard deviation. Here's the equation, which is actually very simple:\n",
    "\n",
    "\\begin{align}\n",
    "Z & = \\frac{X - \\mu}{\\sigma}\n",
    "\\end{align}\n",
    "\n",
    "Let's create z-scores from the original array of random data `array_random` here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_random_zscore = (array_random - array_random.mean()) / array_random.std()\n",
    "array_random_zscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There is also a function to do z-scoring for us already in python, it is called `zscore` and is in the module `scipy.stats`. Let's import it and use it to verify that our answer is correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "array_random_zscore2 = zscore(array_random, axis=0)\n",
    "print(array_random_zscore)\n",
    "print(array_random_zscore2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup, we did it right!\n",
    "\n",
    "Now let's see how z-scoring changed the data by creating line plots like we did when **norming** (doing normalization) with **feature scaling** above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "_= plt.plot(array_random, label='Original')\n",
    "_= plt.plot(array_random_zscore, label='Z-Scored')\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the random data array we created was actualy drawn from a **standard normal** distribution its mean was already almost `0` and its standard deviation almost `1`, so z-scoring didn't change things that much here! Let's move on to some real fMRI data where that won't be the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout session\n",
    "\n",
    "1\\. Z-score the 2-D `array_random2D` along the first axis (meaning z-score each column indepependently), and do the calculations manually (don't use the `zscore` function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z-Scoring fMRI data\n",
    "\n",
    "We've learned that values recorded by fMRI data is recorded in arbitrary units, and about issues such as **scanner drift**. For both of these reasons, and others, the raw values of fMRI BOLD data can vary drastically between different runs on the same subject in the same day. Another way to say this, is that we're really just concerned with relative differences in the BOLD signal, and not the absolute differences. **Temporal normalization** is often used to remove these meaningless differences is signal between scans of the same subject. Z-scoring is the most common form of this.\n",
    "\n",
    "Let's load 2 more scans worth of data from this subject in order to practice z-scoring with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = nibabel.load(\"/data/cogneuro/fMRI/categories/s01_categories_01.nii.gz\")\n",
    "data1 = img1.get_data().T\n",
    "img2 = nibabel.load(\"/data/cogneuro/fMRI/categories/s01_categories_02.nii.gz\")\n",
    "data2 = img2.get_data().T\n",
    "img3 = nibabel.load(\"/data/cogneuro/fMRI/categories/s01_categories_03.nii.gz\")\n",
    "data3 = img3.get_data().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate z-scoring on real fMRI data we'll extract a single voxel time series from all 3 scans. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries1 = data1[:, 6, 57, 37]\n",
    "timeseries2 = data2[:, 6, 57, 37]\n",
    "timeseries3 = data3[:, 6, 57, 37]\n",
    "timeseries3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will contatenate these time series together to have a lot of data for analysis. We'll use `np.concatenate` to make a single 1-D array of the 3 time series in a row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_total = np.concatenate((timeseries1, timeseries2, timeseries3), axis=0)\n",
    "timeseries_total.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start the z-scoring process, let's plot the data to see why we need to z-score in the first place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "_ = plt.plot(timeseries_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The signals of each run seem to vary around very different mean values! Let's visualize this by drawing some lines into the plot that represent the mean of the 3 scans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 6))\n",
    "plt.plot(timeseries_total)\n",
    "mean1 = timeseries1.mean()\n",
    "mean2 = timeseries2.mean()\n",
    "mean3 = timeseries3.mean()\n",
    "timeseries_len = timeseries1.shape[0]\n",
    "plt.plot([0, timeseries_len], [mean1, mean1])\n",
    "plt.plot([timeseries_len, timeseries_len * 2], [mean2, mean2])\n",
    "_ = plt.plot([timeseries_len * 2, timeseries_len * 3], [mean3, mean3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take the first stage of z-scoring, to subtract the mean. We'll be z-scoring each timeseries separately, so we'll subtract the mean of each timeseries separately here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries1_centered = timeseries1 - mean1\n",
    "timeseries2_centered = timeseries2 - mean2\n",
    "timeseries3_centered = timeseries3 - mean3\n",
    "timeseries_total_centered = np.concatenate((timeseries1_centered, timeseries2_centered, timeseries3_centered))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the total time series with the means removed from the 3 time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "_ = plt.plot(timeseries_total_centered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total time series now looks a lot more similar. There could still be differences in the average amount of difference from the means (which is the standard deviation!), however. Let's calculate the standard deviations and plot them using colored strips that center on the mean. We'll use a different color for each scan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdev1 = np.std(timeseries1_centered)\n",
    "stdev2 = np.std(timeseries2_centered)\n",
    "stdev3 = np.std(timeseries3_centered)\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(timeseries_total_centered)\n",
    "plt.fill_between([0, timeseries_len], [-stdev1, -stdev1], [stdev1, stdev1], alpha=.3)\n",
    "plt.fill_between([timeseries_len, timeseries_len*2], [-stdev2, -stdev2], [stdev2, stdev2], alpha=.3)\n",
    "_ = plt.fill_between([timeseries_len*2, timeseries_len*3], [-stdev3, -stdev3], [stdev3, stdev3], alpha=.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the heights of the 3 colored strips are different, which indicates the standard deviation differs across the 3 runs. Let's complete the z-scoring process and divide each scan's time series by the standard deviation, separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries1_zscored = timeseries1_centered / stdev1\n",
    "timeseries2_zscored = timeseries2_centered / stdev2\n",
    "timeseries3_zscored = timeseries3_centered / stdev3\n",
    "timeseries_total_zscored = np.concatenate((timeseries1_zscored, timeseries2_zscored, timeseries3_zscored))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we'll plot the z-scored time series along with the original concatonated time series so we can see how z-scoring changed the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "plt.subplot(2,1,1)\n",
    "_ = plt.plot(timeseries_total)\n",
    "_ = plt.title('Original')\n",
    "plt.subplot(2,1,2)\n",
    "_ = plt.plot(timeseries_total_zscored)\n",
    "_ = plt.title('Z-Scored')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakout Session\n",
    "Do the same thing with the voxel in the location `places_voxel_loc`\n",
    "1. Extract `places_timeseries1` to `places_timeseries3` from `data1, data2, data3`\n",
    "2. Concatenate them and plot them\n",
    "3. Compute the means and plot the means as straight lines from 0 to 120, 120 to 240, and 240 to 360.\n",
    "4. Center each places time series, concatenate them and plot them\n",
    "5. Compute the standard deviations, and plot two lines, one above, one below the mean at the distance of the standard deviation\n",
    "6. Zscore each places time series, concatenate them and plot them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation\n",
    "\n",
    "With our understanding of z-scoring in hand we can move on to correlation, which is a technique that we will use to quantify the similarity between the response vector and the actual BOLD data. Remember that the response vector is our best guess as to how the brain will respond to the stimuli or tasks the participant is exposed to. So when we find a high correlation between response and BOLD vectors in a certain region of the brain, we can infer that this region's neurons fire in response to the stimulus or task. So what is correlation? \n",
    "\n",
    "Correlation is a statistic calculated on two variables that ranges from [-1,1] and indicates how similar the data in the two vectors are. There are multiple ways to think about it what it actually does, and here are 3 good ways to think about it:\n",
    "* Correlation is the slope of a line that best reduces the squared error between z-scored data in a scatter plot.\n",
    "* Correlation measures the extent to which two variables change together away from their respective means. \n",
    "* Correlation is the average product of the standardized scores of two variables.\n",
    "\n",
    "We'll now see these 3 ways of thinking about correlation, so that we can apply correlation to the visual category experiment we saw last lecture. The first definition above is the most common way to understand correlation, so we'll start there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation 1: as a line reducing error on a scatter plot.\n",
    "\n",
    "Plotting data on a scatter plot lets us look at the relationship between two vectors of data. If we z-score both of those vectors then there are both centered on zero, and on the same scale (where every unit is standard deviation of the data). Looking at the scatter plot, we can quantify how much changes in the first vector affect changes in the second vector by drawing a line between the data points that is, on average, closest to all the data in the y-axis. The slope of this line then tells us how much the data in the second vector changes for every unit change in the first vector. And since both vectors are z-scored, it's guaranteed that the slope of this line will be between [-1,1]. Let's find out exactly what a scatter plot is and why this is true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter plots \n",
    "\n",
    "A scatter plot is a visualization technique that plots the value of each data point in two different vectors, such as height and weight. It allows you to see the relationship between the two variables because it shows you how the data changes in one variable as the other variable changes. A scatterplot shows us the relationship between the data in the two vectors. If we see that the y-axis values vary with the x-axis values, then we can conclude some statistical dependence (but not causation!) between the two vectors.\n",
    "\n",
    "A one-colored scatterplot can be created by using the `plt.plot` function you already know, simply by specifying a marker that is a point (and not a line) in the format string after the data. Point markers can be `x, ., o, +, ^`, and others. \n",
    "\n",
    "We'll illustrate that here using some fake data first. We'll think of this data as reprsenting the height and weight of a random sample of people, and assume the mean weight is 150lbs with a standard deviation of 20lbs, and the mean height is 66 inches (5'6\") with a standard deviation of 3 inches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = np.random.randn((100))*20 + 150\n",
    "height_random = np.random.randn((100))*3 + 66"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot both of these in a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(weight, height_random, '.')\n",
    "plt.xlabel('Weight (lbs)')\n",
    "plt.ylabel('Height (inches)')\n",
    "_ = plt.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be more explicit, you can also use `marker='x', linestyle=''` to the same effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(weight, height_random, marker='.', linestyle='')\n",
    "plt.xlabel('Weight (lbs)')\n",
    "plt.ylabel('Height (inches)')\n",
    "_ = plt.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we interpret this plot? Each dot represents one data point in both the weight and height vectors. By data point, we mean that the index into both vectors is the same. So there's a single dot that represents the `0` index into the height and weight vectors, and that dot is plotted in the x-axis (horizontal) direction based on it's value in the `weight` vector, and it's y-axis (vertical) direction is based on it's value in the `height` vector. \n",
    "\n",
    "We are looking for a trend among all the data points, such that when the `weight` value increases, the `height` value seems to increase (or decrease) in a consistant manner across the data points.  This data looks somewhat circular, which is an indication that there is no relationship between the `weight` and `height` vectors.\n",
    "\n",
    "What would happen if created a scatter plot of the `weight` vector plotted against itself? Let's see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(weight, weight, marker='.', linestyle='')\n",
    "plt.xlabel('Weight (lbs)')\n",
    "plt.ylabel('Weight (lbs)')\n",
    "_ = plt.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a straight line! When two vectors have a perfect **linear relationship**, then they form a perfect straight line when plotted in a scatter plot. Any vector will always have a perfect **linear relationship** with itself, because it always moves together! Quantifying how close to a line two vectors are is one way of thinking about correlation. Let's see how that works with more complicated data now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakout Session\n",
    "Let's find something in between \"no correlation\" and \"perfect correlation\". To do this, we'll take the `weight` vector from above, and add some noise, and plot it against the original `weight` vector in a scatter plot\n",
    "\n",
    "1. Set `noise_level` to 2\n",
    "2. Using `np.random.randn` make a vector of random values of the same shape as `weight`, and multiply it by `noise_level`. Store the result in `noise`\n",
    "3. Add `noise` to `weight` and call it `weight_noisy`\n",
    "4. Make a scatterplot of `weight` and `weight_noisy`\n",
    "5. Modify `noise_level` and redo the scatter plot. Observe the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter Plot Lines in Simulated Data \n",
    "\n",
    "Let's update the weight vs height example from above, but this time we'll create a **linear relationship** between them. We'll also add some **noise** to the relationship, meaning that when `weight` increases, `height` increases, but not perfectly. What would that look like?\n",
    "\n",
    "First we'll recreate the `height` data so that there is now a linear relationship between `weight` and `height`. To do this we'll use the formala for linear relationship that you learned in algebra class: \n",
    "\n",
    "$$y = ax + b$$ \n",
    "\n",
    "where $y$ is `height` and $x$ is `weight` and $a$ represents the how much $y$ changes for every unit change in $x$, and $b$ represents how much bigger $y$ is than $x$ when $x$ equals `0`. Let's pick $a=(3/20)$ and $b=44$ so that we keep the same mean and standard deviation for `height` as we had before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = (3/20) * weight + 44\n",
    "height.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use a scatter plot to show what this relationship looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(weight, height, '.')\n",
    "plt.xlabel('Weight (lbs)')\n",
    "plt.ylabel('Height (inches)')\n",
    "_ = plt.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a straight line, and we learned above that when two variables are perfectly linearly related they will have a straight line scatter plot. We also know that a perfect increasing correlation results in a correlation coefficient of `1`. Let's see what the correlation coefficient for this relationship is by using the numpy function `np.corrcoef`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_weight_height = np.corrcoef(weight, height)\n",
    "corr_weight_height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top left and bottom right values of the 2x2 matrix above show the correlations of each vector with itself (and so will always be 1). The top right and bottom left values show the correlation of weight with height, and height with weight, respectively. Since the order doesn't matter for correlation, these two values will always be equal as well. \n",
    "\n",
    "So we see that the correlation between `weight` and `height` is indeed perfect, with a value of 1. In general, if a scatterplot is a line that is not perfectly horizontal, then this means that the variables perfectly correlate.\n",
    "\n",
    "Now let's add some noise so the relationship isn't perfect and we can see what a correlation other than zero looks like. The noise will have a mean of zero and a standard deviation of `2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height_noisy = height + np.random.randn(len(height)) * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot this data and see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(weight, height_noisy, '.')\n",
    "plt.xlabel('Weight (lbs)')\n",
    "plt.ylabel('Height (inches)')\n",
    "_ = plt.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not a straight line, but there is still an upward trend, meaning that as the values of weight increase, so do the values of height. Let's quantify this with a correlation coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_weight_height_noisy = np.corrcoef(weight, height_noisy)\n",
    "corr_weight_height_noisy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the correlation has dropped from `1` to `0.8678`. That's still a high correlation value, but it's no longer perfect. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakout Session\n",
    "Take the example from just above, vary the noise and evaluate the correlation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter Plots with Raw vs. Z-Scored Data\n",
    "\n",
    "We've been making scatter plots using \"raw\" values, meaning the original data in it's original units (pounds and inches in this example). **Scatter plots of \"raw\" values show a perfect correlation when the data form a non-horizontal line**.\n",
    "\n",
    "In the definition of correlation we gave above, we said that the scatter plot should show z-scored data, not raw data. If we z-score the 2 vectors before making a scatter plot, the rule for what constitutes a perfect correlation changes! Let's z-score our weight and height vectors and have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_z = scipy.stats.zscore(weight)\n",
    "height_z = scipy.stats.zscore(height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll plot them on a scatter plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(weight_z, height_z, '.')\n",
    "plt.xlabel('Weight (z-score)')\n",
    "plt.ylabel('Height (z-score)')\n",
    "plt.title('Z-Scored Data - No Noise')\n",
    "_ = plt.axis('equal')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(weight, height, '.')\n",
    "plt.xlabel('Weight (lbs)')\n",
    "plt.ylabel('Height (inches)')\n",
    "plt.title('Raw Data - No Noise')\n",
    "_ = plt.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the slope of the line has changed when the data is z-scored! This is because both height and weight are on the same scale (standard deviations). \n",
    "\n",
    "Now let's z-score the noisy data and plot it vs the weight data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height_noisy_z = scipy.stats.zscore(height_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(weight_z, height_noisy_z, '.')\n",
    "plt.xlabel('Weight (z-score)')\n",
    "plt.ylabel('Height (z-score)')\n",
    "plt.title('Z-Scored Data - With Noise')\n",
    "_ = plt.axis('equal')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(weight, height_noisy, '.')\n",
    "plt.xlabel('Weight (lbs)')\n",
    "plt.ylabel('Height (inches)')\n",
    "plt.title('Raw Data - With Noise')\n",
    "_ = plt.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, notice that the slope of the data has changed, for the same reason.\n",
    "\n",
    "So how do we interpret a scatter plot for correlation when the data is z-scored? **Perfect correlation happens when all points lie on the diagonal (slope=1) after z-scoring!** Also, the line for z-scored data will always go through the origin (the 0,0 point) since the data of both vectors have means of `0`.\n",
    "\n",
    "So correlation is perfect when:\n",
    "* **Raw data:** When the data forms a straight line, no matter what the slope of that line is. This is because the units for the two vectors can be different, and the slope is relative to the units.\n",
    "* **Z-Scored data:** When the data forms a stright line with a slope of 1 (45 degree line). This is because the units are now the same, so the slope has to be 1 when the correlation is perfect.\n",
    "\n",
    "Let's draw create a second set of noisy data with even more noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height_noisy2 = height + np.random.randn(len(height)) * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And find the correlation again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_weight_height_noisy2 = np.corrcoef(weight, height_noisy2)\n",
    "corr_weight_height_noisy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And z-score this new noisier data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height_noisy2_z = scipy.stats.zscore(height_noisy2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's plot the 3 z-scored relationships between weight and the 3 height values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(weight_z, height_z, '.')\n",
    "plt.plot(weight_z, weight_z*corr_weight_height[0,1],)\n",
    "plt.xlabel('Weight (z-score)')\n",
    "plt.ylabel('Height (z-score)')\n",
    "plt.title('Z-Scored Data - No Noise')\n",
    "_ = plt.axis('equal')\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(weight_z, height_noisy_z, '.')\n",
    "plt.plot(weight_z, weight_z*corr_weight_height_noisy[0,1],)\n",
    "plt.xlabel('Weight (z-score)')\n",
    "plt.ylabel('Height (z-score)')\n",
    "plt.title('Z-Scored Data - With Noise')\n",
    "_ = plt.axis('equal')\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(weight_z, height_noisy2_z, '.')\n",
    "plt.plot(weight_z, weight_z*corr_weight_height_noisy2[0,1],)\n",
    "plt.xlabel('Weight (z-score)')\n",
    "plt.ylabel('Height (z-score)')\n",
    "plt.title('Z-Scored Data - With More Noise')\n",
    "_ = plt.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does seem like these lines fit our z-scored point clouds pretty well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakout Session\n",
    "Let's look at negative correlations: Take `height_noisy_z` and flip the sign of all the values (make negative values positive, and positive values negative). Call the output `height_noisy_z_sign_flipped`. Compute the correlation with `weight`, make the scatter plot and the line plot as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter plots of real fMRI data\n",
    "Now let's make some scatter plots of the real fMRI data we've been working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(faces_response, faces_timeseries_centered, '.')\n",
    "plt.xlabel('Response (a.u.)')\n",
    "plt.ylabel('BOLD Signal (a.u.)')\n",
    "_ = plt.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there is a positive relationship between the response and the BOLD signal. The data seems to fall in vertical lines. This is because the response vector only takes on a few different values due to the HRF only having 16 values.\n",
    "\n",
    "Now let's quantify how similar they are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(faces_response, faces_timeseries_centered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a very high correlation for fMRI data that is so noisy! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakout session\n",
    "\n",
    "1. Put the places voxel time series and the places convolved model into one scatter plot.\n",
    "2. Compute their correlation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation 2: Signals Changing Together Relative to their Means\n",
    "\n",
    "Scatterplots are just one way to plot two z-scored variables together to inerpret their correlation. Since here we are working with *time series* vectors, we can also plot the response and BOLD signal time series data as line plots to see how similarily they change together over time, relative to their means.\n",
    "\n",
    "We're familiar with plotting the response and BOLD signal time series together on the same plot, but we've never plotted them together after z-scoring both. Let's see how both the centering and scaling steps of z-scoring change the perceived relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_response_centered = faces_response - faces_response.mean()\n",
    "faces_response_zscored = scipy.stats.zscore(faces_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 5))\n",
    "plt.plot(faces_response, 'r-.', label='raw response vector')\n",
    "plt.plot(timeseries_total, 'r', label='raw BOLD data')\n",
    "plt.plot(faces_response_centered, 'b-.', label='centered response vector')\n",
    "plt.plot(timeseries_total_centered, 'b', label='centered BOLD data')\n",
    "plt.plot(faces_response_zscored, 'g-.', label='zscored response vector')\n",
    "plt.plot(timeseries_total_zscored, 'g', label='zscored BOLD data')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not a very useful plot, all we see is that there is a large difference between the raw and zscored data. Since the large mean of the raw BOLD data is so large, let's only concentrate on centered and z-scored time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 5))\n",
    "plt.plot(faces_response_centered, 'b-.', label='centered response vector')\n",
    "plt.plot(timeseries_total_centered, 'b', label='centered BOLD data')\n",
    "plt.plot(faces_response_zscored, 'g-.', label='zscored response vector')\n",
    "plt.plot(timeseries_total_zscored, 'g', label='zscored BOLD data')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The merely centered, but not z-scored plot of BOLD data is dominating. Let's make another plot where we focus on the z-scored data only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 8))\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(faces_response_centered, 'b-.', label='centered response vector')\n",
    "plt.plot(timeseries_total_centered, 'b', label='centered BOLD data')\n",
    "plt.legend()\n",
    "plt.xlabel('Time (TRs)')\n",
    "plt.ylabel('Signal Amplitude (a.u.)')\n",
    "plt.title('Centered Data')\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(faces_response_zscored, 'g-.', label='zscored response vector')\n",
    "plt.plot(timeseries_total_zscored, 'g', label='zscored BOLD data')\n",
    "plt.xlabel('Time (TRs)')\n",
    "plt.ylabel('Signal Amplitude (z-scores)')\n",
    "plt.title('Z-Scored Data')\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, the z-scored response vector seems to match the BOLD signal very well once we put them on the scame scale! When the response vector goes up, the BOLD signal vector goes up, and when the response vector is zero, the BOLD signal seems to fluctuate noisily around zero. Thus plotting two z-scored vectors together let's us see how the two change together, which is another way of thinking about what correlation is capturing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakout Session\n",
    "Plot the z-scored `places` response vector and the z-scored places time series from above in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation 3: the Average Product of Z-Scored Data\n",
    "\n",
    "We've used `np.corrcoef` several times to quantify the similarity between two vectors. What does this function do?\n",
    "\n",
    "Let's take a look at how data 8 introduces correlation:\n",
    "\n",
    "        Correlation is the average of the products of the two variables, when both variables are measured in standard units.\n",
    "\n",
    "What does this mean? Two things:\n",
    "1. We need to z-score both arrays we would like to correlate\n",
    "2. We need to multiply corresponding values from the two arrays and average them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Correlation with Line Plots\n",
    "Let's try this with our face response and BOLD signal time series from above. We've already z-scored our response and BOLD signal data, but we'll introduce the calculations of correlation by doing them with algebra, and visualizing what that algebra does to the two vectors.\n",
    "\n",
    "Let's start by plotting the raw response and BOLD data together in the same plot.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 8))\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(faces_response, 'b-.', label='response vector')\n",
    "plt.plot(timeseries_total, 'b', label='BOLD data')\n",
    "plt.legend()\n",
    "plt.xlabel('Time (TRs)')\n",
    "plt.ylabel('Signal Amplitude (a.u.)')\n",
    "_ = plt.title('Raw Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's re-plot both centered vectors. By removing the mean of each of the vectors we can focus on the way that both vectors move relative to their means, because both means are now `0`.\n",
    "\n",
    "The algebra here is:\n",
    "\n",
    "$$y_{centered} = y - y_{mean}$$\n",
    "$$x_{centered} = x - x_{mean}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 8))\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(faces_response_centered, 'b-.', label='centered response vector')\n",
    "plt.plot(timeseries_total_centered, 'b', label='centered BOLD data')\n",
    "plt.legend()\n",
    "plt.xlabel('Time (TRs)')\n",
    "plt.ylabel('Signal Amplitude (a.u.)')\n",
    "_ = plt.title('Centered Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's re-plot both z-score vectors that have been scaled in addition to being centered. By scaling each of the vectors the amount of change relative to the center are in identical units for both vectors, so we can compare just the way they change on equal footing.\n",
    "\n",
    "The algebra here is:\n",
    "\n",
    "$$y_{zscore} = y_{centered} / y_{std}$$\n",
    "$$x_{zscore} = x_{centered} / x_{std}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 8))\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(faces_response_zscored, 'b-.', label='z-scored response vector')\n",
    "plt.plot(timeseries_total_zscored, 'b', label='z-scored BOLD data')\n",
    "plt.legend()\n",
    "plt.xlabel('Time (TRs)')\n",
    "plt.ylabel('Signal Amplitude (z-scores)')\n",
    "_ = plt.title('Z-Scored Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll take the product of the 2 z-scored vectors and plot that, along with the 2 z-scored vectors. If there is high correlation then this will make the large values larger because both of the vectors have large values. It will also make the smaller values smaller, because both vectors have smaller values. If there is low correlation, that means one vector goes up when the other doesn't, so the low value of the second vector will reduce the product of the two.\n",
    "\n",
    "The algebra here is:\n",
    "\n",
    "$$data_{product} = y_{centered} * x_{centered}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_product = faces_response_zscored * timeseries_total_zscored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 8))\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(faces_response_zscored, 'b-.', label='z-scored response vector')\n",
    "plt.plot(timeseries_total_zscored, 'b--', label='z-scored BOLD data')\n",
    "plt.plot(data_product, 'b', label='Data Product')\n",
    "plt.legend()\n",
    "plt.xlabel('Time (TRs)')\n",
    "plt.ylabel('Signal Amplitude (z-scores)')\n",
    "_ = plt.title('Data Product')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the $data_{product}$ is very large where the data moves high together, and very low elsewhere. Now the final step is simply to take the average of $data_{product}$.\n",
    "\n",
    "The algebra here is:\n",
    "\n",
    "$$correlation = mean(data_{product})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_stepwise = data_product.mean()\n",
    "correlation_stepwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do what we just did in steps, all in one step.  We need to use the `timeseries_total_zscored` version of the time series data, since that was z-scored one run at a time, and `np.corrcoef` will zscore across the entire vector passed in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_byhand = (scipy.stats.zscore(faces_response)* scipy.stats.zscore(timeseries_total_zscored)).mean()\n",
    "correlation_byhand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to confirm we did it right, let's compare that with the value from the `np.corrcoef` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_numpy = np.corrcoef(faces_response, timeseries_total_zscored)[0,1]\n",
    "correlation_numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They're very close, so we've done it right!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of correlation\n",
    "We just learned 3 ways to think about and visualize correlation:\n",
    "* Correlation measures the extent to which two variables change together away from their respective means. \n",
    "* Correlation is the slope of a line that best reduces the squared error between data in a scatter plot.\n",
    "* Correlation is the average product of the standardized scores of two variables.\n",
    "\n",
    "Let's do an interactive plot to see all three of them together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants to adjust how correlated and the size of the fake data\n",
    "TARGET_CORRELATION = 0.6\n",
    "FAKE_N = 30\n",
    "MEAN_1 = 0\n",
    "MEAN_2 = 10\n",
    "\n",
    "# Create some fake correlated data\n",
    "fake_data = np.random.multivariate_normal([MEAN_1,MEAN_2], [[1,TARGET_CORRELATION],[TARGET_CORRELATION,1]],FAKE_N)\n",
    "fake_x = fake_data[:,0]\n",
    "fake_y = fake_data[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cor_time(n):\n",
    "\n",
    "    # create a sequence for the length of the fake data\n",
    "    t = range(n)\n",
    "    \n",
    "    # subset the fake data to the size specified by the slider\n",
    "    cur_x = fake_x[:n]\n",
    "    cur_y = fake_y[:n]\n",
    "\n",
    "    # z-score (standardize) the fake data\n",
    "    cur_x_z = scipy.stats.zscore(cur_x)\n",
    "    cur_y_z = scipy.stats.zscore(cur_y)\n",
    "    \n",
    "    # take the mean of the fake data\n",
    "    mean_fake_x = np.mean(cur_x)\n",
    "    mean_fake_y = np.mean(cur_y)\n",
    "\n",
    "    # calculate the correlation of the fake data\n",
    "    fake_prods = (cur_x-mean_fake_x)*(cur_y-mean_fake_y)\n",
    "    fake_mean_prods = np.mean(fake_prods)\n",
    "    fake_prod_std = np.std(cur_x) * np.std(cur_y)\n",
    "    fake_cor = fake_mean_prods / fake_prod_std\n",
    "\n",
    "    # print out the values of the correlation\n",
    "    print('Average of products: %.02f' % (fake_mean_prods))\n",
    "    print('Product of std-dev: %.02f' % (fake_prod_std))\n",
    "    print('Correlation: %.02f' % (fake_cor))\n",
    "\n",
    "    # plot the line plots of the two variables\n",
    "    plt.figure(figsize=(15,6))\n",
    "    plt.subplot(3,1,1)\n",
    "    plt.plot(t, cur_x, 'g', label='x')\n",
    "    plt.plot(t, cur_x, 'go')\n",
    "    plt.plot(t, cur_y, 'b', label='y')\n",
    "    plt.plot(t, cur_y, 'bo')\n",
    "    plt.plot(t, np.ones(n)*mean_fake_x, 'g--')\n",
    "    plt.plot(t, np.ones(n)*mean_fake_y, 'b--')\n",
    "    plt.legend()\n",
    "\n",
    "    # plot the product of the difference between each data point and their respective means\n",
    "    plt.subplot(3,1,2)\n",
    "    plt.plot(t, np.ones(n)*fake_mean_prods, '--')\n",
    "    plt.bar(t,fake_prods)\n",
    "\n",
    "    # make a scatter plot of the standardized data, and draw the correlation line\n",
    "    plt.subplot(3,4,9)\n",
    "    plt.plot(cur_x_z, cur_y_z, 'o')\n",
    "    fit = np.polyfit(cur_x_z, cur_y_z, deg=1)\n",
    "    plt.plot(cur_x_z, fit[0] * cur_x_z + fit[1], color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_plot = interactive(plot_cor_time, n=widgets.IntSlider(value=2, min=2, max=len(fake_x)))\n",
    "output = interactive_plot.children[-1]\n",
    "output.layout.height = '450px'\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakout session\n",
    "\n",
    "1. Change the TARGET_CORRELATION of the fake data to 2 or 3 numbers that are larger and smaller (but between -1 and 1, which is the range of correlation values). Try one very small correlation, one negative value near 1, and one positive value near 1. How do these changes affect the magnitude and direction of the bars in the above plot? And what does that mean for the correlation value?\n",
    "\n",
    "2. Change the MEAN_2 of the fake data, making it equal to 5. Does this change the correlation? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation is a linear dependency\n",
    "\n",
    "Earlier in the lecture we were using correlation coefficient to assess how much two sets of numbers relate to each other. There are some assumptions behind this, which we will take a look at here.\n",
    "\n",
    "The general intuition, when we say **\"these quantities are correlated\"**, it means: There is some form of **dependency** between the two quantities. Meaning: knowledge of the one quantity might gain us more knowledge about the other quantity. This is a **loose definition**, but there exist mathematical ideas to quantify this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When we say *correlation*, we usually refer to *linear correlation* \n",
    "Or to frame it as a question: **How much does the point cloud resemble a line?**\n",
    "\n",
    "We saw above that when 2 vectors are linearly related, without noise, they formed a perfect line, meaning a perfect positive correlation of 1. Let's see that again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(100)\n",
    "y = 2 * x + 4\n",
    "plt.scatter(x, y)\n",
    "_ = plt.axis([-3, 3, -11, 11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Linear Correlation\n",
    "\n",
    "There can be many different **non-linear** ways that two vectors can relate to each other, or be dependent on each other. One simple way is when one vector is the square of the other, resulting in a parabola when plotted on a scatter plot. Let's create a `height_squared` vector that shows just that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1., 1., 51)\n",
    "y = x ** 2\n",
    "plt.plot(x, y, 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that these two vectors create a parabola when plotted on a scatter plot! While there is definitely a relationship between the two, correlation cannot detect that relationship because it only looks for linear relationships. Let's look at what the correlation between these vectors is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost close to zero! Let's plot the correlation line along with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_z = (x - x.mean()) / x.std()\n",
    "y_z = (y - y.mean()) / y.std()\n",
    "plt.plot(x_z, y_z, 'x')\n",
    "c = np.mean(x_z * y_z)\n",
    "plt.plot(x_z, c * y_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation coefficient of this *perfect functional relation* is **zero**. Always keep this in mind when thinking about and evaluation correlation scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Correlation Flatmaps\n",
    "\n",
    "In this section we will correlate all the voxels of a scan with the response vectors for the experiment. We will compute all of them together using array operations, and eventually even compute all of them together for stimulus types. Then we will display the results on flat maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data\n",
    "\n",
    "Above we already loaded the BOLD data into the names `data1, data2, data3`. Let's check their shapes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.shape, data2.shape, data3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's mask them to obtain the cortical voxels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = cortex.db.get_mask('s01', 'catloc', 'cortical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_masked = data1[:, mask]\n",
    "data2_masked = data2[:, mask]\n",
    "data3_masked = data3[:, mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's perform z-scoring for the first array for all the voxels separately:\n",
    "\n",
    "First, we compute the mean of each voxel time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_masked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_mean = data1_masked.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_mean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that taking the mean along axis 0 gives us a mean value for each voxel.\n",
    "\n",
    "We can also take the standard deviation along this axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_stdev = data1_masked.std(axis=0)\n",
    "data1_stdev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do the z-scoring by subtracting the mean and dividing by the standard deviation for each column of the masked data array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_zscored = (data1_masked - data1_mean) / data1_stdev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakout session\n",
    "1. Do z-scoring for the other two arrays\n",
    "2. Use `np.concatenate` or `np.vstack` to stack all three matrices vertically into an array named `data_zscored`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing correlation of one response vector to all the voxels\n",
    "\n",
    "In order to compute the correlation of a response vector to all the voxels, we will first spell out the multiplication of that one response vector to all voxels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_response_times_voxels = faces_response_zscored * data_zscored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huh, that didn't work!\n",
    "\n",
    "To make this work, we need to turn `faces_response_zscored` into a column vector of shape `(360, 1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_response_zscored_column = faces_response_zscored.reshape(360, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_response_times_voxels = faces_response_zscored_column * data_zscored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That worked. Now we can take the mean along the time axis to compute the correlations with each voxel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_faces_data = faces_response_times_voxels.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_faces_data.max(), corr_faces_data.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_faces = cortex.Volume(corr_faces_data, 's01', 'catloc')\n",
    "cortex.quickflat.make_figure(vol_faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that the `faces_response_zscored` correlates most strongly with voxels that we have already identified as belonging to areas processing faces!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakout session\n",
    "1. Compute the same correlation for `places` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
