{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Overview\n",
    "Thus far, we've been calculating statistics of our brain signals. That is, we have transformed our signal (e.g., filtering or calculating event related responses), and come up with some number to summarize it (e.g., average activity across time).\n",
    "\n",
    "However, neuroscience is about **linking the world to brain function**, and the best way to do this is to build a *model* that links the two. This is a more explicit way of defining how a change in the world results in a change in the brain.\n",
    "\n",
    "Today, we'll cover the basics of **modeling**. We'll start with correlation, move to univariate regression, and we'll finish with multivariate regression.\n",
    "\n",
    "## Goals for today\n",
    "- Neuroscience concepts\n",
    "    - Simulating a signal\n",
    "    - Correlations between neural signals\n",
    "- Coding concepts\n",
    "    - Implementing mathematical functions (e.g. ordinary least squares)\n",
    "- Datascience concepts\n",
    "    - Correlation\n",
    "    - Regression\n",
    "    - Relationship b/w them\n",
    "    - Conceptual introduction to multiple regression\n",
    "---\n",
    "\n",
    "# Correlation via simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import neurods\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# We'll begin by simulating a few signals, this will help with the intuition for what regression means\n",
    "# First, we'll create a random variable\n",
    "noise_amp = 5\n",
    "n_pts = 50\n",
    "a = 10 * np.random.random(n_pts)\n",
    "\n",
    "# Now, we'll define a \"weight\" that causes a second variable to respond to it\n",
    "weight = 2\n",
    "\n",
    "# We will add a baseline:\n",
    "baseline = 10\n",
    "\n",
    "# Finally, we'll create some noise so that it's not a perfect mapping\n",
    "noise = noise_amp * np.random.randn(n_pts)\n",
    "\n",
    "# Then let's mix them together. In this case, b is explicitly created from the values in a\n",
    "b = baseline + weight * a + noise\n",
    "\n",
    "# Let's look at the signals\n",
    "f, ax = plt.subplots()\n",
    "ax.plot(a)\n",
    "ax.plot(b)\n",
    "ax.legend({'a','b'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "> **What's the explicit relationship between these two signals?**\n",
    "\n",
    "Before we get into modeling, we'll begin with *correlation*. Look at the two plots above, they seem to be varying in similar ways. When one goes up, the other goes up, when one goes down, the other goes down. How can we quantify this?\n",
    "\n",
    "First, we'll use a scatterplot to see how related the two signals are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "ax.scatter(a, b)\n",
    "plt.xlabel('a', fontsize = 16)\n",
    "plt.ylabel('b', fontsize = 16 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Our intuition from above seem to hold here as well. When values of the x-axis are large, values on the y-axis also tend to be large. We can quantify this with the correlation coefficient.\n",
    "\n",
    "*Note: The `corrcoef` function will actually return a \"correlation matrix\". In this case, every row of `a` is correlated with every row of `b`, and displayed as a matrix. Since our variables are vectors, the output will be a 2 by 2 matrix and the 1st element of the 2nd row will be the correlation coefficient.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix, then take the 1st val of the 2nd row.\n",
    "corr = np.corrcoef(a, b)[1, 0]\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **What would happen if we increased the noise parameter when constructing the signal above?**\n",
    "\n",
    "### Constructing your own function to compute the correlation coefficient:\n",
    "\n",
    "The correlation between two variables is scale free, i.e. it is not affected by the magnitude of each variable. The correlation coefficient measures the extend to which two variables have the same behavior (they increase or decrease together). \n",
    "\n",
    "1- The first step in computing correlation is to bring both variables to the same scale. This can be done by zscoring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "a_zs = zscore(a)\n",
    "b_zs = zscore(b)\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "ax.plot(a_zs)\n",
    "ax.plot(b_zs)\n",
    "ax.legend({'a_zs','b_zs'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- The next step is to multiply elementwise the two time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = a_zs * b_zs \n",
    "\n",
    "f, ax = plt.subplots()\n",
    "ax.plot(c)\n",
    "ax.legend({'c'});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3- The last step is to get the average of the elementwise product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corr = np.mean(c)\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical expression\n",
    "\n",
    "The correlation between vectors $A$ and $B$ of length N can there be estimated as:\n",
    "$\\frac{1}{N} \\sum_{i=1}^N \\frac{ (a_i - \\mu_A) (b_i - \\mu_B)  } {\\sigma_A \\sigma_B}  = \\frac{1}{N} \\sum_{i=1}^N \\frac{ (a_i - \\mu_A) }{\\sigma_A} \\frac{(b_i - \\mu_B)  } { \\sigma_B}  = \\frac{1}{N} \\sum_{i=1}^N  a_i' b_i' $\n",
    "\n",
    "where $A'$ and $B'$ are normalized versions of $A$ and $B$ that have a 0 mean and a variance of 1.\n",
    "(in some textbooks you might find the above expression divided by N-1 instead of N, but we will not get into this subtelty here).\n",
    "\n",
    "#### Breakout session\n",
    "Write a function that:\n",
    "- takes as input two time series x and y and\n",
    "- returns their correlation coefficient.\n",
    "- You should repeat the 3 steps above (without the plots).\n",
    "- Use your function to compute the correlation of a and b from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def my_corr(x,y):\n",
    "### STUDENT ANSWER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulated fMRI data example\n",
    "\n",
    "Now let's go back to fMRI responses. Download the following dataset. The code below uses the stim_resp_plot function to plot the saved stimulus and associated data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plotting function from Lecture08 that we will continue to use\n",
    "def stim_resp_plot(t, stimulus, response, yl=(-0.2, 1.2), label_stim='Stimulus', label_resp='BOLD response (HRF)'):\n",
    "    \"\"\"Plot stimulus and response.\"\"\"\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.stem(t, stimulus, linefmt='k-', markerfmt='.', basefmt='k-', label=label_stim)\n",
    "    plt.plot(t, response, 'r.-', label=label_resp)\n",
    "    plt.ylim(yl)\n",
    "    plt.xlim([-1,t.max()+1])\n",
    "    plt.xlabel('Time (seconds)')\n",
    "    plt.ylabel('Response (arbitrary units)')\n",
    "    _ = plt.legend()\n",
    "\n",
    "#Load example data\n",
    "fileurl = 'https://www.dropbox.com/s/y9qjnno9b3vqoco/example_data_01.npz?dl=0'\n",
    "filename = 'example_data_01.npz'\n",
    "neurods.io.download_file(fileurl, filename,\n",
    "                         root_destination=os.path.abspath(os.curdir),\n",
    "                         replace = True)\n",
    "ex_data = np.load(filename)\n",
    "t = ex_data['t']\n",
    "n_tps = len(t)\n",
    "stimulus = ex_data['x']\n",
    "data_sim = ex_data['data']\n",
    "stim_resp_plot(t, stimulus, data_sim, yl=(-2, 5), label_stim='Stimulus', label_resp='Simulated data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "f, ax = plt.subplots()\n",
    "ax.scatter(stimulus, data_sim)\n",
    "plt.xlabel('stimulus', fontsize = 16)\n",
    "plt.ylabel('data_sim', fontsize = 16 )\n",
    "corr2 = my_corr(stimulus, data_sim)\n",
    "\n",
    "print(\"The correlation between the stimulus and the data is {}\".format(corr2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is going on? ==> survey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The presentation of the stimulus should create a hemodynamic response if this voxel is sensitive to that stimulus. We therefore need to convolve the stimulus first with the hemodynamic response function. \n",
    "\n",
    "But first, look at the time vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is sampled with a TR of 2 seconds! Then we need an hrf that is sampled with the same rate. This will be the same curve as before, but is sampled differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t2, hrf_2 = neurods.fmri.hrf(tr=2)\n",
    "plt.plot(t2, hrf_2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the stimulus and the convolved stimulus**\n",
    "\n",
    "Convolve the stimulus with the HRF and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv_stimulus = np.convolve(stimulus, hrf_2, mode='full')[:n_tps]\n",
    "stim_resp_plot(t, stimulus, conv_stimulus, label_stim='Stimulus', label_resp='Convolved stimulus');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zoom into the first 50 time points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stim_resp_plot(t[:100], stimulus[:100], conv_stimulus[:100],\n",
    "               label_stim='Stimulus', label_resp='Convolved stimulus');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the convolved stimulus and the simulated voxel data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stim_resp_plot(t, conv_stimulus, data_sim, yl=(-2, 5),\n",
    "               label_stim='Convolved stimulus', label_resp='Simulated data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stim_resp_plot(t[:100], conv_stimulus[:100], data_sim[:100], yl=(-2, 5),\n",
    "               label_stim='Convolved stimulus', label_resp='Simulated data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(conv_stimulus, data_sim);\n",
    "print(\"the correlation between the stimulus and the data is {}\".format(np.corrcoef(conv_stimulus, data_sim)[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see that we are able to recover a clear relationship between the stimulus and the data.\n",
    "\n",
    "What is the variance of the noise that we can guess from this plot? Had this been real data, this would have been a very clean result. Usually in fMRI we are not so lucky to have effects that are this clear. We will study in future lectures how to expand this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Observation \n",
    "So far we have measured the correlation between one stimulus and brain activity. We are also interested to find out how much this particular stimulus affects brain activity. \n",
    "\n",
    "Voxels in different parts of the brain can be differently responsive to a stimulus, or not responsive at all. In fMRI, we are interested in finding how different voxels are responding to an event or stimulus. We will therefore introduce a new parameter: $w^v$, that describes the strength with which a voxel $v$ responds to the stimulus:\n",
    "\n",
    "$ \\text{response}^v(t) = w^v \\times  \\text{convolved-stimulus} (t) $\n",
    "\n",
    "We already know how to compute $\\text{convolved-stimulus}(t)$. \n",
    "\n",
    "In the next steps, we will gradually learn how we can estimate $w^v$ from the data. We will try to find if and how responsive voxel $v$ is to a stimulus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression via simulation\n",
    "\n",
    "The simulated signals a and b are highly correlated, this is because above we've defined one signal to be a *linear function* of the other signal. In other words, like this:\n",
    "\n",
    "$signal_a = w_0 + w * signal_b + noise$\n",
    "\n",
    "How is correlation related to modeling? Basically, correlation values must vary between -1 and 1. In modeling, however, we can use an arbitrary size for the weight that defines the relationship between two signals. For example, we could change the weight value above, and the underlying correlation would always be 1.\n",
    "\n",
    "What if we wanted to recover the *actual* weight that we used, instead of a scaled correlation number? For this, we must use *regression*.\n",
    "\n",
    "In regression, we explicitly model one signal as a linear function of the other signal. The output of regression is a *weight* (not a correlation) that tells us how we can predict values of one signal using the other.\n",
    "\n",
    "Here's the equation for a linear model:\n",
    "\n",
    "$$y(t) = w_0 + w x(t)+ \\epsilon(t)$$\n",
    "\n",
    "This says: each output $y(t)$ is predicted by weighting each feature $x(t)$ with by corresponding weight $w$, and summing them together, then adding random noise $\\epsilon(t)$.\n",
    "\n",
    "Regression is a technique for inferring what these weights are, given a dataset of inputs and outputs.\n",
    "\n",
    "Let's go back to the scatter plot we had earlier of a and b:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "ax.scatter(a, b)\n",
    "plt.xlabel('a', fontsize = 16)\n",
    "plt.ylabel('b', fontsize = 16 )\n",
    "# this line adds a line of best fit:\n",
    "plt.plot(np.unique(a), np.poly1d(np.polyfit(a, b, 1))(np.unique(a)),'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression minimizes the least squares error:\n",
    "    \n",
    "$$\\min_{w_0,w} \\sum_{t = 0}^{T-1} (y(t) - w_0 - w x(t) )^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many packages to do regression in python, but we will use for now the polyfit module with a polynomial order of 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this function first a straight line through the points above\n",
    "slope, intercept = np.polyfit(a, b, 1)\n",
    "print(slope)\n",
    "print(intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to our brain data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(conv_stimulus, data_sim);\n",
    "plt.plot(np.unique(conv_stimulus), np.poly1d(np.polyfit(conv_stimulus, data_sim, 1))(np.unique(conv_stimulus)),'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we estimate from this data the magnitude of the weight $w_v$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# this function first a straight line through the points above\n",
    "slope, intercept = np.polyfit(conv_stimulus, data_sim, 1)\n",
    "print(slope)\n",
    "print(intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quiz 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw in class that we can derive the ordinary least squares (OLS) solution by taking the derivative of the objective function with respect to our parameters $W = [w_0, w]$. \n",
    "\n",
    "We define X as being a matrix with two columns: the first is a columns of 1s and the second is our input variable, (e.g. conv_stimulus). Y corresponds to the output variable (e.g. data_sim).\n",
    "\n",
    "The OLS solution is:\n",
    "\n",
    "$ W = (X^\\top X)^{-1}X^\\top Y$\n",
    "\n",
    "#### Breakout session:\n",
    "- using the inv and np.dot functions, implement the OLS solution. Your algorithm should return a vector W. \n",
    "- use the OLS solution in the cell below to print the slope and the intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "\n",
    "def my_OLS(X,Y):\n",
    "### STUDENT ANSWER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.vstack([np.ones_like(conv_stimulus),conv_stimulus]).T\n",
    "print(X.shape)\n",
    "Y = data_sim\n",
    "print(Y.shape)\n",
    "### STUDENT ANSWER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, what is units of this value? fMRI signal doesn't have a unit and can be rescaled and normalized. The weight therefore depends on how the data is normalized and is only meaningful with respect to the variance of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "What's the relationship between correlation and regression? Well, if we were to convert both the inputs and the outputs into **standard units** (AKA, so they had a mean == 0, and a variance == 1), then regression would give us the exact same answer as correlation.\n",
    "\n",
    "We have seen this before as zscoring the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Scale our variables\n",
    "X_zs = np.vstack([np.ones_like(conv_stimulus),zscore(conv_stimulus)]).T\n",
    "Y_zs = zscore(data_sim)\n",
    "W_zs = my_OLS(X_zs,Y_zs)\n",
    "print('the slope is {} and the intercept is {}'.format(W_zs[1], W_zs[0]))\n",
    "corr_standardized = my_corr(zscore(conv_stimulus),Y_zs)\n",
    "print('the correlation between the standandized conv_stimulus and response is {}'.format(corr_standardized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go back to our estimate of the weights before standardization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.vstack([np.ones_like(conv_stimulus),conv_stimulus]).T\n",
    "print(X.shape)\n",
    "Y = data_sim\n",
    "print(Y.shape)\n",
    "W = my_OLS(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that the stimulus appears again in a different run. Now it appears at times 10, 20, 30-40 and 70. The run lasts 120s.\n",
    "\n",
    "#### Breakout session\n",
    "Using the model predicted above in W, give your best prediction about how the signal would look like in that hypothetical voxel:\n",
    "- convolve the stimulus variable below with hrf_2\n",
    "- use the model W to predict activity. You will have to do a similar stacking as we did above.\n",
    "- use the stim_resp_plot function to plot the stimulus with your predicted response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stimulus_times = [10,20,30,32,34,36,38,40,70]\n",
    "t = np.arange(120)\n",
    "stimulus = np.zeros((120))\n",
    "stimulus[stimulus_times] = 1\n",
    "### STUDENT ANSWER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to use the regression model to *predict* a new output. This turns out to be really useful, and we'll cover it in future lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "What will happen when we have more than one stimulus?"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
