{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 13: False Discovery Rate, Complex Encoding Models & Decoding\n",
    "\n",
    "## Goals\n",
    "\n",
    "- **Neuroscience / Neuroimaging concepts**\n",
    "    - Significance tests for Prediction Accuracy\n",
    "    - Modeling brain responses as a function of complex stimulus features\n",
    "    - Comparing models as hypothesis testing\n",
    "- **Datascience / Coding concepts**\n",
    "    - Family Wise Error Rate\n",
    "    - False Discovery Rate\n",
    "    - Benjamini-Hochberg Procedure for FDR Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Simply run the cells below that contain all the Python modules we'll neeed, plus setup matplotlib for plotting in this jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import cortex\n",
    "import os\n",
    "import h5py\n",
    "import json\n",
    "import urllib\n",
    "import tempfile\n",
    "from scipy.stats import zscore\n",
    "from scipy.stats import norm\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from sklearn.linear_model import LinearRegression as SkLinearRegression\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set plotting defaults\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, block_size=5000, use_pinv=True, lr_args=None, fit_intercept=True, \n",
    "                 memmap_coefs=True, verbose=0):\n",
    "        self.block_size = block_size\n",
    "        self.use_pinv = use_pinv\n",
    "        self.lr_args = lr_args\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.verbose = verbose\n",
    "        self.memmap_coefs = memmap_coefs\n",
    "    \n",
    "    def zeros(self, shape, dtype='float32'):\n",
    "        if isinstance(shape, int):\n",
    "            shape = shape,\n",
    "        if self.memmap_coefs is False:\n",
    "            return np.zeros(shape, dtype=dtype)\n",
    "        else:\n",
    "            if self.memmap_coefs is True:\n",
    "                file_id, memmap_file = tempfile.mkstemp()\n",
    "                memmap_file = os.fdopen(file_id, 'rb+')\n",
    "            else:\n",
    "                # if filename specified\n",
    "                memmap_file = open(self.memmap_coefs, 'rb+')\n",
    "            memmap = np.memmap(memmap_file, dtype=dtype, shape=tuple(shape))\n",
    "            memmap[:] = 0.\n",
    "            return memmap\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        self.y_ndim = Y.ndim\n",
    "\n",
    "        if Y.ndim == 1:\n",
    "            Y = Y.reshape(-1, 1)\n",
    "\n",
    "        self.coef_ = self.zeros([Y.shape[1], X.shape[1]])\n",
    "        if self.fit_intercept:\n",
    "            self.intercept_ = self.zeros(Y.shape[1])\n",
    "\n",
    "        if self.use_pinv:\n",
    "            if self.fit_intercept:\n",
    "                X_pinv = np.linalg.pinv(np.hstack([X, np.ones_like(X[:, 0:1])]))\n",
    "            else:\n",
    "                X_pinv = np.linalg.pinv(X)\n",
    "\n",
    "            for i in range(0, Y.shape[1], self.block_size):\n",
    "                weights = X_pinv.dot(Y[:, i:i + self.block_size])\n",
    "                self.coef_[i:i + self.block_size] = weights[:X.shape[1]].T\n",
    "                if self.fit_intercept:\n",
    "                    self.intercept_[i:i + self.block_size] = weights[-1]\n",
    "                if self.verbose > 0:\n",
    "                    print(\".\", end=\"\")\n",
    "        else:\n",
    "            lr_args = {} if self.lr_args is None else lr_args\n",
    "            lr = SkLinearRegression(fit_intercept=self.fit_intercept, **lr_args)\n",
    "            for i in range(0, Y.shape[1], self.block_size):\n",
    "                lr.fit(X, Y[:, i:i + self.block_size])\n",
    "                self.coef_[i:i + self.block_size] = lr.coef_\n",
    "                if self.fit_intercept:\n",
    "                    self.intercept_[i:i + self.block_size] = lr.intercept_\n",
    "                if self.verbose > 0:\n",
    "                    print(\".\", end=\"\")\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        p = self.zeros((X.shape[0], self.coef_.shape[0]))\n",
    "        for i in range(0, self.coef_.shape[0], self.block_size):\n",
    "            p[:, i:i + self.block_size] = X.dot(self.coef_.T[:, i:i + self.block_size])\n",
    "            if hasattr(self, 'intercept_') and self.fit_intercept:\n",
    "                p[:, i:i + self.block_size] = p[:, i:i + self.block_size] + self.intercept_[i:i + self.block_size]\n",
    "        if self.y_ndim == 1:\n",
    "            p = p.ravel()\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score(x):\n",
    "    x = x - x.mean(axis=0)\n",
    "    x = x / (x.std(axis=0) + 1e-18)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlate(x, y, block_size=5000):\n",
    "    if x.ndim == 1:\n",
    "        x = x.reshape(-1, 1)\n",
    "    if y.ndim == 1:\n",
    "        y = y.reshape(-1, 1)\n",
    "    \n",
    "    output = np.zeros(x.shape[1])\n",
    "    for i in range(0, x.shape[1], block_size):\n",
    "        x_z = z_score(x[:, i:i + block_size])\n",
    "        y_z = z_score(y[:, i:i + block_size])\n",
    "        output[i:i + block_size] = np.mean(x_z*y_z, axis=0)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event Related Design Review\n",
    "\n",
    "Up until this point in the course we've been using fMRI data that came from a block designed visual localizer experiment. Several lectures ago we learned there are other ways to design an experiment, one of which being an event-related design. Today we'll use some fMRI data from an event-related design to go deeper into encoding models, but first we'll quickly review event-related designs.\n",
    "\n",
    "Block designs have relatively high levels of SNR for an fMRI design. This high level of SNR comes at a cost however; block designs are not temporally efficient, they take a long time to show just a few different types of stimuli. Since time in the MRI machine is expensive (several hundred dollars an hour), this is not an efficient way to conduct fMRI research. \n",
    "\n",
    "A better way would be to show many different stimuli (or trial) types in a [pseudo]-random order. This would allow for many different stimulus types and shorter scan times. So called fast event-related designs present a stimulus every 4 seconds or so. Below is an example image of what a fast event-related design exepriment looks like.\n",
    "\n",
    "<img src='figures/event_relatedA.PNG' style=\"width: 400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event-Related Designs and the HRF\n",
    "\n",
    "A potential problem with event-related designs is that the BOLD signal responds slowly (peaks at about 5 seconds), and so if we show many different types of stimuli in a row, the BOLD signal response from one stimulus will \"bleed over\" into the TRs of the neighboring stimuli. In other words, if we want to make an accurate mathematical model of how the brain responds to a given stimulus, we have to incorporate this delay into our model. \n",
    "See the image below for an illustration.\n",
    "\n",
    "<img src='figures/event_related.PNG' style=\"width: 400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem has a solution however, and it involves characterizing the Hemodynamic Response Function (HRF) and then using that HRF to model the BOLD data.\n",
    "\n",
    "To do this, we will borrow a concept from signal processing theory called the impulse response function. An impulse response function describes a system's response (this can be the signal that we measure using fMRI) to an external change (e.g. the stimulus or event onset). As we know, fMRI measures BOLD signal and not neuronal activity directly. Therefore, to model the hemodynamic response (BOLD signal) to an event we would like to have a function that describes what the BOLD signal looks like when it responds to a given event. This function is called the *hemodynamic response function* or HRF, and is an example of an impulse response function. \n",
    "\n",
    "A great deal of early fMRI research went into accurately describing how the BOLD signal rises, falls, and resets to baseline after an event (i.e. characaterizing the HRF). We will rely on the conclusions of this research without going into much detail about it. For an overview of hemodynamic responses in fMRI, check out [this blog post](http://mindhive.mit.edu/node/72). The practical upshot of this work is that BOLD responses have a fairly characteristic shape, which is well described by a mathematical function called the *gamma function*. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from an Event-Related fMRI Experiment\n",
    "\n",
    "Now we'll load a file containing the fMRI data from an event-related experiment in which naturalistic images of many different types of semantic categories (what is the image of) where shown to subjects while BOLD data was collected. This data has been preprocessed in a way that we haven't yet discussed. They are stored as **response amplitudes** to each of the images, in each voxel. A response amplitude is simply the estimated response to the image, after the HRF has been removed. The HRF is removed through a process called **deconvolution**, and this results in a single number for each image, the response amplitude. How deconvolution works is beyond the scope of this course, but you should understand what it results in (i.e response amplitudes).\n",
    "\n",
    "This data is stored in a new type of file called an **HD5** file. These files are used to store very large data sets, and are a useful tool in the data scientist's toolbox. You don't need to know how to load or manipulate HD5 data however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = h5py.File(\"/data/cogneuro/fMRI/images/s04_color_natims_data.hdf\")\n",
    "data_train = data_file['est'][:]\n",
    "data_test = data_file['val'][:]\n",
    "data_train.shape, data_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is a 2-D matrix where the columns are cortical voxels, and each row represents response amplitudes for the images in the experiment. The data has already been split into a training set and a  test set so that we can do cross-validation and predict the test set. There are `1260` images in the training set and `126` images in the test set. Each training image was shown 2 times to the subject, and each test image was shown `10`. \n",
    "\n",
    "Some of the voxels in this data set contain missing data (nan) because they exist near the edge of the brain and the subject moved during some of the scans. Let's create a binary mask that indicates the voxels that contain at least one `nan` in either the training or test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nanmap = np.isnan(data_train).any(axis=0) | np.isnan(data_test).any(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to z-score this data, but can't when there are `nan` values in the data. We also can't just set the `nan` values to a single value, because the zscore operation divides by the standard deviation, and if all the values of a vector are the same the standard deviation is `0`, and it is an undefined operation to divide by zero. So we'll set the voxels with `nan` values to random data, zscore the data, and set those same voxels back to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_train_column = np.random.rand(data_train.shape[0],1)\n",
    "rand_test_column = np.random.rand(data_test.shape[0],1)\n",
    "data_train[:, nanmap] = rand_train_column\n",
    "data_test[:, nanmap] = rand_test_column\n",
    "data_train = zscore(data_train, axis=0)\n",
    "data_test = zscore(data_test, axis=0)\n",
    "data_train[:, nanmap] = 0.\n",
    "data_test[:, nanmap] = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the size of the test and training data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = data_train.shape[0]\n",
    "n_test = data_test.shape[0]\n",
    "num_cortical_voxels = data_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have several encoding models we will fit to this data. The first is a simple 3 category model. Each category represents one broad semantic category. Let's load a file that contains the names of the categories (aka. features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_sem3_filename = \"/data/cogneuro/fMRI/images/color_natims_features_3cat.json\"\n",
    "features_sem3 = json.load(open(features_sem3_filename, 'r'))\n",
    "features_sem3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have humans, objects and places. Each image can contain any combination of those features, including none of them or all of them. Let's load the design matrix containing the features for all of the training and test data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "designmat_sem3_file = h5py.File(\"/data/cogneuro/fMRI/images/color_natims_features_3cat.hdf\")\n",
    "designmat_sem3_train = zscore(designmat_sem3_file['est'][:], axis=0)\n",
    "designmat_sem3_test = zscore(designmat_sem3_file['val'][:], axis=0)\n",
    "designmat_sem3_train.shape, designmat_sem3_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prediction\n",
    "\n",
    "Last week we learned that encoding models define an entire feature space that describes all the properties of the stimuli or task that the scientist thinks a given brain region might respond to. These encoding models act as hypotheses for what the brain region represents. To test those hypotheses we need to have some way to quantify how well an entire encoding model explains the changes in the BOLD signal. We saw that by using the linear model to predict the BOLD data, and then correlating the real BOLD data with the predicted data, we can get a metric of how well an entire encoding model describes the fluctations in each voxel. We called that correlation prediction accuracy. \n",
    "\n",
    "Let's review how we calculated prediction accuracy on this new event related experiment, using the 3-semantic category model as our encoding model. \n",
    "\n",
    "**NOTE** Since this data has been deconvolved, we are not predicting BOLD signals, rather response amplitudes.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Prediction Accuracy\n",
    "\n",
    "First we'll fit a linear model on the training data, using the 3-semantic encoding model's designmatrix as the independent variables, and the training response amplitudes as the dependent variables. We'll fit this model for all voxels in one function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_semantic3 = LinearRegression()\n",
    "model_semantic3.fit(designmat_sem3_train, data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the training data and correlate those predictions with the real response amplitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sem3_train = model_semantic3.predict(designmat_sem3_train)\n",
    "pred_acc_sem3_train = correlate(data_train, pred_sem3_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the prediction accuracy onto a flatmap. This data comes from a new subject, `subject=s04`, and uses a different transformation, `xfmname='color_natims'` (color natural images). We'll also show you a new colormap, called `cmap='hot'` that is very useful for plotting prediction accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_pred_acc_sem3_train = cortex.Volume(pred_acc_sem3_train, subject='s04', xfmname='color_natims', cmap='hot')\n",
    "_ = cortex.quickflat.make_figure(volume_pred_acc_sem3_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "\n",
    "We just predicted the same data on which we fit the encoding model. There's a problem with calculating prediction accuracy on the same data that you use to fit the model, and it's called **overfitting**. Because the data is noisy, each independent variable that we add generally fits some of the signal and some of the noise in that signal. We're interested in how well our model fits just the signal, but when we predict the training set we're predicting both signal and noise. To overcome this data scientists use a test set to calculate prediction accuracy on. \n",
    "\n",
    "First we'll find a voxel that is well predicted by this 3 semantic category encoding model to illustrate how overfitting differentially affects the training and test data. The voxel with the highest prediction accuracy will do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vox_idx_best_3sem = np.argmax(pred_acc_sem3_train)\n",
    "vox_idx_best_3sem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_best_3sem_train = data_train[:,vox_idx_best_3sem]\n",
    "data_best_3sem_test = data_test[:,vox_idx_best_3sem]\n",
    "data_best_3sem_train.shape, data_best_3sem_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last week we saw that adding independent variables that are just noise decreases the sum of squared errors (SSE) on the training data. This is known as **in-sample** SSE, and decreases due to overfitting to the noise in the training data. Predicting on a separate test set, and computing the **out-of-sample** SSE, works to control some of that overfitting. \n",
    "\n",
    "Let's add an increasing number of noise independent variables to the 3-category semantic encoding model and calculate both the in and out of sample error rates to see how they are differentially affected. We'll switch things up just a bit by calculating the **mean squared error (MSE)**, instead of SSE, because we want to compare in and out of sample error, and the test and training are different lengths of data. If we used SSE then the in-sample SSE would be MUCH higher than the out-of-sample SSE. \n",
    "\n",
    "**Mean Squared Error (MSE)** is just SSE divided by the number of data points:\n",
    "\n",
    "$$ \\frac{\\sum{(y-\\hat{y})^2}}{n} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_sample_MSE = np.zeros(n_test)\n",
    "out_sample_MSE = np.zeros(n_test)\n",
    "\n",
    "model_overfit = LinearRegression()\n",
    "model_overfit.fit(designmat_sem3_train,data_best_3sem_train)\n",
    "\n",
    "pred_train_best_vox = model_overfit.predict(designmat_sem3_train)\n",
    "in_sample_MSE[0] = np.sum((data_best_3sem_train-pred_train_best_vox)**2)/n_train\n",
    "\n",
    "pred_test_best_vox = model_overfit.predict(designmat_sem3_test)\n",
    "out_sample_MSE[0] = np.sum((data_best_3sem_test-pred_test_best_vox)**2)/n_test\n",
    "\n",
    "random_indep = np.random.randn(n_train+n_test, n_test)\n",
    "for i in np.arange(n_test-1):\n",
    "    designmat_sem3_train_overfit = np.concatenate((designmat_sem3_train, random_indep[:n_train,:i]), axis=1)\n",
    "    model_overfit.fit(designmat_sem3_train_overfit,data_best_3sem_train)\n",
    "\n",
    "    pred_train_overfit = model_overfit.predict(designmat_sem3_train_overfit)\n",
    "    in_sample_MSE[i+1] = np.sum((data_best_3sem_train-pred_train_overfit)**2)/n_train\n",
    "\n",
    "    designmat_sem3_test_overfit = np.concatenate((designmat_sem3_test, random_indep[n_train:,:i]), axis=1)\n",
    "    pred_test_overfit = model_overfit.predict(designmat_sem3_test_overfit)\n",
    "    out_sample_MSE[i+1] = np.sum((data_best_3sem_test-pred_test_overfit)**2)/n_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.plot(in_sample_MSE, label='In-sample MSE')\n",
    "plt.plot(out_sample_MSE, label='Out-of-sample MSE')\n",
    "plt.legend()\n",
    "plt.xlabel('# of independent variables')\n",
    "_ = plt.ylabel(\"SSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Significance of Prediction Accuracy\n",
    "\n",
    "In order to determine where an encoding model is significantly predicting brain activity we need to do some kind of hypothesis testing on each voxel's prediction accuracy. To do this we'll use the same permutation test method that we applied to find significant contrasts in previous lectures. \n",
    "\n",
    "Since the y data in this experiment is response amplitudes, and not raw BOLD data, we don't need to account for the HRF, since it's already been deconvolved from the BOLD signal. Thus, to do the permutation test, we can simply shuffle the rows of the design matrix on each permutation, making the permutation test simpler. After fitting a linear regression to each permutation, we'll calculate prediction accuracy instead of a contrast, and store that prediction accuracy in the null distribution. Let's have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_shuffle(x):\n",
    "    permute_indices = np.arange(x.shape[0])\n",
    "    np.random.shuffle(permute_indices)\n",
    "    return x[permute_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: define the number of resamples to do\n",
    "NUM_RESAMPLES = 100\n",
    "\n",
    "# Step 2: create the null distribution vector\n",
    "null_dist_pred_acc = np.zeros((NUM_RESAMPLES, num_cortical_voxels))\n",
    "\n",
    "# Step 3: loop and resample to create the null distribution\n",
    "for cur_iter in range(NUM_RESAMPLES):\n",
    "    \n",
    "    # Step 3a: resample the design matrix without replacement by shuffling the the rows \n",
    "    cur_designmat_shuffle = standard_shuffle(designmat_sem3_test)\n",
    "    \n",
    "    # Step 3b: calculate the contrast value using the current shuffled design matrix\n",
    "    cur_pred = model_semantic3.predict(cur_designmat_shuffle)\n",
    "    cur_pred_acc = correlate(data_test, cur_pred)\n",
    "    \n",
    "    # Step 3c: store the prediction accuracy map in the null distribution vector\n",
    "    null_dist_pred_acc[cur_iter, :] = cur_pred_acc\n",
    "\n",
    "# Step 4: Calculate the p-value to determine signifiance\n",
    "pred_sem3_test = model_semantic3.predict(designmat_sem3_test)\n",
    "pred_acc_sem3_test = correlate(data_test, pred_sem3_test)\n",
    "num_greater_null_pred_acc = (null_dist_pred_acc > pred_acc_sem3_test).sum(axis=0)\n",
    "p_value_pred_acc = (num_greater_null_pred_acc + 1) / (NUM_RESAMPLES+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value_pred_acc_nan = np.zeros((num_cortical_voxels))\n",
    "p_value_pred_acc_nan[:] = p_value_pred_acc\n",
    "p_value_pred_acc_nan[nanmap] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatmap of Significant Voxels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_pvalue_pred_acc = cortex.Volume(1-p_value_pred_acc_nan, subject='s04', xfmname='color_natims', vmin=.95, vmax=1, cmap='hot')\n",
    "_ = cortex.quickshow(volume_pvalue_pred_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Comparison Correction\n",
    "\n",
    "When multiple hypothesis tests are conducted the risk that you're conclusions are false increases. Since we use an alpha threshold that is not `0` (usually `.05`), there's some chance that we've said there's a real significant finding, when in fact it's not real. The more tests you do, the higher the chance that you'll incorrectly declare a a significant finding. Multiple comparison corrections are a set of techniques you can use to correct this problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonferroni Correction\n",
    "\n",
    "If multiple hypotheses are tested, the chance of a rare event increases, and therefore, the likelihood of incorrectly rejecting a null hypothesis (i.e., making a Type I error) increases. The Bonferroni correction compensates for that increase by testing each individual hypothesis at a significance level of `alpha/m`, where alpha is the desired overall alpha level and m is the number of hypotheses. For example, if a trial is testing `m=20` hypotheses with a desired `alpha = 0.05`, then the Bonferroni correction would test each individual hypothesis at `alpha = 0.05 / 20 = 0.0025`.\n",
    "\n",
    "from: https://en.wikipedia.org/wiki/Bonferroni_correction\n",
    "\n",
    "Let's use the Bonferroni correction to adjust the alpha level for all cortical voxels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "alpha_adjusted = alpha / num_cortical_voxels\n",
    "alpha_adjusted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this adjusted p-value to determine significance. Let's recreate a map that uses this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_acc_sig_Bon = p_value_pred_acc < alpha_adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_pvalue_pred_acc = cortex.Volume(pred_acc_sig_Bon, subject='s04', xfmname='color_natims', vmin=.95, vmax=1, cmap='hot')\n",
    "_ = cortex.quickshow(volume_pvalue_pred_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load p-values from Large Null Distribution\n",
    "\n",
    "Doing permutation tests takes a long time at a lot of memory, both of which we're limited by in this class. We want to have p-values that would survive a Bonferroni correction, if they are truly small enough, so we've created p-value maps for the encoding models that we'll use in class today by doing permutation tests with a large number of permuations (1 x 10^7 permutations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19 cat 1e8 https://berkeley.box.com/shared/static/imb6i0hvifh32m9zto7cbxlspoeeritm.npz\n",
    "# 3 cat 1e7 https://berkeley.box.com/shared/static/60bt29r9v1nkgb55z210yrpg2tf51ht0.npz\n",
    "# 3 cat 1e8 https://berkeley.box.com/shared/static/awbhvtg00c8km37z047qnqj3ibbqeblq.npz\n",
    "# 19 cat 1e7 https://berkeley.box.com/shared/static/z5wwm19t478yxn2uey0sr487gjsmyqxp.npz\n",
    "urllib.request.urlretrieve(\"https://berkeley.box.com/shared/static/60bt29r9v1nkgb55z210yrpg2tf51ht0.npz\",\n",
    "                           \"/home/jovyan/p_3cat_1e7.npz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_3cat_1e7_file = np.load(\"/home/jovyan/p_3cat_1e7.npz\")\n",
    "p_value_pred_acc_large = (p_3cat_1e7_file['counts'] + 1) / (p_3cat_1e7_file['cur_iter'] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the Bonferroni adjusted alpha rate to the p-values we just loaded and see if anything survives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_acc_sig_Bon_large = p_value_pred_acc_large < alpha_adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_pvalue_pred_acc_large = cortex.Volume(pred_acc_sig_Bon_large, subject='s04', xfmname='color_natims', vmin=0, vmax=1, cmap='Reds')\n",
    "_ = cortex.quickshow(volume_pvalue_pred_acc_large)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's more like it! There are some voxels where the encoding model predicts significantly. But it could be better! Let's explore another way to control for multiple comparisons called **False Discovery Rate Correction**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False Discovery Rate (FDR) Correction\n",
    "\n",
    "The Bonferroni correction we just covered is a type of multiple comparison correction called **Family Wise Error Correction (FWER)**. \n",
    "* FWER controls the alpha value so that the probability is $\\alpha$ that there is a SINGLE false positive.\n",
    "\n",
    "There is another type of multiple comparison correction called **False Dicovery Rate (FDR) correction**.\n",
    "* FDR allows for %q% proportion of the significant hypothesis tests to be false postives, meaning they're not actually true.\n",
    "\n",
    "This is a more relaxed correction, since up to 5% of the significant values can actually be false, which is comparable to accepting a 5% chance that a single hypothesis test is false (when alpha=`.05`)\n",
    "\n",
    "The **Benjamini-Hochberg procedure** is the most commonly used FDR correcion. Let's get a better intuition of what a False Discovery Rate really is, then we'll see how the Benjamini-Hochberg procedure limits FDR at a given value.\n",
    "\n",
    "[Here's a great video explanation of both FDR and Benjamini Hochberg](https://www.youtube.com/watch?v=K8LQSvtjcEo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Discovery Rate (FDR)\n",
    "To illustrate what a false discovery rate is, and how to correct for it, we'll use some simulated data. Let's imagine you've done an experiment where you record fMRI data while subjects' listen to many different types of music, one of which is heavy metal. We also will record fMRI data while the subject is resting with no music being played. Our hypotheses is:\n",
    "* Heavy metal will activate two brain regions (vs. rest) thought to be involved in emotional arousal that we'll creatively call Region A and Region B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Null Hypothesis True: Metal vs Rest in Region A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though our hypothesis is that metal will activate Region A more than rest, we're actually going to assume that this hypothesis is not true, which means that the null hypothesis that the activity is the same during metal and rest is true in all the voxels of Region A . We'll simulate this by creating a fake contrast value for each of the `1000` voxels in this example. We'll draw the contrast values from the standard normal distribution, which is a bell curve (or Gaussian) with a mean of 0 and standard deviation of 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_MUSIC_VOXELS = 1000\n",
    "\n",
    "music_contrast_regionA = np.random.randn(1000)\n",
    "_ = plt.hist(music_contrast_regionA, bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to calculate p-values for these fake contrasts values. Since we're simulating data we know that the data we've created comes from a standard normal distribution, since we coded it that way! That means we can cheat a little bit and use the standard normal distribution as the null distribution, instead of doing a permutation test. Let's create a null distribution of size `10000` for each of the `1000` fake voxels we're simulating. We'll use these distribution as if they came from a permutation test to calculate p-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_MUSIC_RESAMPLES = 10000\n",
    "music_null_dist = np.random.randn(NUM_MUSIC_RESAMPLES,NUM_MUSIC_VOXELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the p-values for the fake contrasts and plot them in a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_larger_regionA = (music_null_dist > music_contrast_regionA).sum(axis=0)\n",
    "pvals_regionA = (num_larger_regionA+1) / (NUM_MUSIC_RESAMPLES + 1)\n",
    "_ = plt.hist(pvals_regionA, bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-values from data where the null distribution is true for all hypothesis tests is uniformly distributed, meaning each p-value is just as likely as any other p-value. This is always the case when a null hypothesis is true, and is the foundation of the theory behind the false discovery rate correction technique we'll see shortly.\n",
    "\n",
    "As such, you'll notice that some p-values are below our standard alpha rate of `.05`, which means that we would reject the null hypothesis and accept the alternate hypothesis. But we know that is wrong! Another term for a significant finding is a **discovery**, and so we call these wrong conclusions **false discoveries**. Let's see how many false discoveries we have in this case by counting all the voxels with p-values below an alpha of `.05`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_discoveries_regionA = (pvals_regionA<alpha).sum()\n",
    "false_discoveries_regionA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alpha vs. Proportion of False Discoveries**\n",
    "\n",
    "This number should be close to `50`, which is exactly 5%, or `.05`, of the `1000` voxels in this simulation. Huh, that's the same as the alpha rate, why is that? When the null hypothesis is true for all of the experiments you conduct, then the proportion of false discoveries out of all the results will be approximately the same as the alpha rate. This is because the distribution of p-values is flat in this scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below shows all the possibile ways the outcome of multiple hypothesis tests can occur. The two columns indicate the ground truth (which we can never know except in these toy simulations!), either the null hypothesis is true, or the alternative hypothesis is true. The rows represent the results of a hypothesis test, either we reject the null hypothesis or accept it.\n",
    "\n",
    "We just found the number of false discoveries in our simulated experiment of region A, which is the top left cell of this table. We can see that another way describe false discoveries is **Type I Error**.\n",
    "\n",
    "<table>\n",
    "<tbody>\n",
    "    <tr>\n",
    "        <th></th>\n",
    "        <th>Null hypothesis is true (H<sub>0</sub>)</th>\n",
    "        <th>Alternative hypothesis is true (H<sub>A</sub>)</th>\n",
    "        <th>Total</th>\n",
    "    </tr>\n",
    "    <tr align=\"center\">\n",
    "        <th>Test is declared significant</th>\n",
    "        <td>False Discoveries (Type I Error)</td>\n",
    "        <td>True Discoveries</td>\n",
    "        <td>Discoveries</td>\n",
    "    </tr>\n",
    "    <tr align=\"center\">\n",
    "        <th>Test is declared non-significant</th>\n",
    "        <td>True Negatives</td>\n",
    "        <td>False Negatives (Type II Error)</td>\n",
    "        <td>Negatives</td>\n",
    "    </tr>\n",
    "    <tr align=\"center\">\n",
    "    <th>Total</th>\n",
    "        <td># of True H<sub>0</sub></td>\n",
    "        <td># of True H<sub>A</sub></td>\n",
    "        <td># of Hypothess Tested</td>\n",
    "    </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "A **False Discovery Rate (FDR)** is simply the proportion of all discoveries that are actually false. Again, in the real world we never know this, but can only estimate it based on what we know about probability. In this example we do know it, so let's calculate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regionA_FDR = false_discoveries_regionA / false_discoveries_regionA\n",
    "regionA_FDR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that was a boring calculation! Because all of the null hypotheses are actually true in this experiment, any discovery is a false discovery, so the FDR is exactly `1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternate Hypothesis True: Metal vs Rest in Region B\n",
    "\n",
    "Now we'll simulate contrasts from the metal vs rest condition in region B. To do so, we'll draw data from the standard normal distribution again, but this time we'll `3` to the data vector, giving them a mean of `3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_contrast_regionB = np.random.randn(NUM_MUSIC_VOXELS) + 3\n",
    "_ = plt.hist(music_contrast_regionB, bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in the same manner we'll calculate p-values for Region B and plot a histogram of those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_larger_regionB = (music_null_dist > music_contrast_regionB).sum(axis=0)\n",
    "pvals_regionB = (num_larger_regionB+1) / (NUM_MUSIC_RESAMPLES + 1)\n",
    "_ = plt.hist(pvals_regionB, bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see a very skewed distribution with the vast majority of the p-values near `0`, which is what we would expect since the truth is that all of these voxels really do respond more to metal than to rest.\n",
    "\n",
    "You may notice that not all of the p-values are below the alpha threshold of `.05`. These values are called **false negatives** in the table above, and are also considered **Type II Error**. The Type II Error Rate is affected both by the alpha level you choose, and by the strength of the effect you're seeing. Let's calculate the number of false negatives in this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_negatives_regionB = (pvals_regionB > .05).sum()\n",
    "false_negatives_regionB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From that value we can calculate the number of discoveries in the experiment, since the total number of hypotheses equals the number of discoveries + the number of negatives, and all the negatives in Region B are false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discoveries_regionB = NUM_MUSIC_VOXELS - false_negatives_regionB\n",
    "discoveries_regionB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we'll calculate FDR for Region B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regionb_FDR = 0 / discoveries_regionB\n",
    "regionb_FDR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, another boring calculation! Since the null hypothesis is false for all of the voxels in Region B, there are `0` false discoveries!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining Regions A & B\n",
    "\n",
    "The two examples we've provided so far for Regions A & B rarely happen in the real world. Most often when multiple hypotheses are tested some of them are actually true and others actually false. Now let's enter an alternate reality where Regions A & B are actually two sub-sections of the same region, and experimenters don't know about this division, they only know of Region X, which is the combination of both Regions A & B.\n",
    "\n",
    "Let's combine the p-values from Regions A & B to make Region X, and plot it's histogram of p-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvals_regionX = np.concatenate((pvals_regionA, pvals_regionB))\n",
    "hist_vals_regionX = plt.hist(pvals_regionX, bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see the large number of significant p-values near zero, and a large, mostly uniform distribution above it. We know that the vast majority of those high p-values come from Region A where the null hypothesis is true. Thus we can draw a horizontal line that fits those high p-values all the way across the histogram, and so values below that line that are significant we expect to be false discoveries. To draw that line we'll take the mean histogram bin count of the top half of the bins. This is an arbitrary decision just to show the idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_dist_line = hist_vals_regionX[0][10:].mean()\n",
    "plt.hist(pvals_regionX, bins=20)\n",
    "_ = plt.plot([0,1], [null_dist_line,null_dist_line])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at this histogram as containing two histograms, one above the line and one below the line, then the histogram above the line looks like Region B's histogram, and the histogram below the line like Region A's.\n",
    "\n",
    "Now let's calculate the actual FDR for Region X. To do this we'll count up all the discoveries (p-values below alpha) in Region X, and divide the false discoveries we counted in Region A by the total discoveries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discoveries_regionX = (pvals_regionX < alpha).sum()\n",
    "fdr_regionX = (false_discoveries_regionA / discoveries_regionX) * 100\n",
    "fdr_regionX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the ground truth for the FDR of Region X. Now let's use the line that we've created above to estimate the FDR for Region X. The value of the line is the estimated number of false discoveries since the line represents the number of p-values per bin that are really null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdr_regionX_estimate = (null_dist_line / discoveries_regionX) * 100\n",
    "fdr_regionX_estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty close, and this isn't even a mathematically sound estimation, we're just giving you the idea! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FDR Correction: The Benjamini-Hochberg Procedure\n",
    "\n",
    "Now that we have a concept of what FDR is, we'll explore a procedure that corrects your p-values to keep FDR below a threshold value that the experimenter decides. The **Benjamini-Hochberg (BH)** procedure does just that. You provide it with p-values from multiple hypothesis tests and a threshold value, called a **q value**, and it returns the subset of p-values that contain a **q** FDR. Here's the outline of the procedure:\n",
    "\n",
    "1. Sort all the p-values in increaseing order, from smallest to largest.\n",
    "2. Find the largest p-value such that:\n",
    "$$ P(k) \\le \\frac{k}{m}q$$\n",
    "\n",
    "   where $P(K)$ is the p-value at index $k$ in your sorted list of p-values, $m$ is the number of p-values being tested, and $q$ is the threshold you specify.\n",
    "\n",
    "3. Reject the null hypothesis (i.e. declare discoveries) for the first $k$ p-values in the sorted list.\n",
    "\n",
    "The intuition here is that for the smallest p-value in your set you'll use a Bonferroni correction, for the largest p-value in the set you'll use the original alpha, and for all values in between you'll do a linear interpolation of those values, since you are adding another p-value to your set as you go.\n",
    "\n",
    "Let's illustrate this procedure starting with a super simple example. We'll start with 10 p-values and assume the null hypothesis is true for all of them, then apply the BH procedure to these p-values to see how many survive a FDR correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvals_fake1 = np.array([.02, .15, .22, .38, .41, .52, .63, .76, .80, .91])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that one of these fake pvalues is below an alpha of `.05`, just by chance. Let's see what the BH procedure does to it. First we'll sort the p-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvals_fake1_sorted = np.sort(pvals_fake1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll calculate the adjusted **q** value that the above equation states and provide `q=.05`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = .05\n",
    "m = pvals_fake1_sorted.shape[0]\n",
    "adjusted_q = (np.arange(1, m+1) / m) * q\n",
    "adjusted_q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the fake p-values that are above this adjusted q-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvals_above_adjusted_q = pvals_fake1_sorted[pvals_fake1_sorted <= adjusted_q]\n",
    "pvals_above_adjusted_q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally find the largest fake p-value that is above the adjusted q-value line and use that as the threshold for correction. If there are no values above the adjusted q-value line, then no p-values survive the correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pvals_above_adjusted_q.size == 0:    \n",
    "    pvals_fake1_corrected = np.array([])\n",
    "else:\n",
    "    threshold = pvals_above_adjusted_q.max()\n",
    "    pvals_fake1_corrected = pvals_fake1_sorted[pvals_fake1_sorted <= threshold]\n",
    "pvals_fake1_corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that none of the p-values survive the FDR correction even though one of them (.02) was originally below the significance threshold.\n",
    "\n",
    "Let's put all of this into a function and try it on another data set where we assume all of the null hypothesis should be rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FDR(pvalues, q=.05):\n",
    "    m = len(pvalues)\n",
    "    pvalues_sorted = np.sort(pvalues)\n",
    "    adjusted_qvals = np.arange(1, m+1) / m * q\n",
    "    selected_pvals = pvalues_sorted[pvalues_sorted <= adjusted_qvals]\n",
    "    if selected_pvals.size == 0:\n",
    "        return (-1,np.zeros_like(pvalues, dtype=bool), adjusted_qvals)\n",
    "    threshold = selected_pvals.max()\n",
    "    return (threshold, pvalues[pvalues <= threshold], adjusted_qvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvals_fake2 = np.array([.0000001, .000001, .00001, .0001, .001, .01, .02, .03, .04, .05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_fake2, pvals_fake2_corrected, adjusted_qvals_fake2 = FDR(pvals_fake2)\n",
    "pvals_fake2_corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, all of these p-values survived the correction, how is that so? It's because the last adjusted q-value is always just the q level you provide, in this case `q=.05`. Since the largest p-value in this set is also `.05`, all the values are deemed significant.\n",
    "\n",
    "Thus, we could create a third fake data set that contains p-values all equal to `.05`, and all would survive the correction. Why it that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvals_fake3 = np.array([.05] * 10)\n",
    "pvals_fake3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_fake3, pvals_fake3_corrected, adjusted_qvals_fake3 = FDR(pvals_fake3)\n",
    "pvals_fake3_corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because we are willing to accept 5% false discoveries, and a 5% p-value means that there's a 5% probability that this value comes from the null distribution, so if all the values have a p-value of 5, then there's exactly a 5% FDR as well!\n",
    "\n",
    "Now let's visualize what the BH procedures does by plotting the p-values and adjusted q-values from the second fake data set in a line plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pvals_fake2, label='Fake 2 p-values')\n",
    "plt.plot(adjusted_qvals_fake2, label='Adjusted q-values')\n",
    "plt.xlabel('Sorted p-value Index')\n",
    "plt.ylabel('p-value')\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BH procedure finds the largest p-value that is below the adjusted q-value line, and deems all p-values below that to survive the FDR correction. So how does this compare with the Bonferroni correction? Let's visualize that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_Bonferroni_fake2 = alpha/pvals_fake2.shape[0]\n",
    "plt.plot(pvals_fake2, label='Fake 2 p-values')\n",
    "plt.plot(adjusted_qvals_fake2, label='Adjusted q-values')\n",
    "plt.plot([0, 10], [alpha_Bonferroni_fake2,alpha_Bonferroni_fake2], label='Bonferroni adjusted alpha')\n",
    "plt.xlabel('Sorted p-value Index')\n",
    "plt.ylabel('p-value')\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Boferroni correction calculates a single new alpha threshold, and all of the p-values have to be below this value. You can see that the Bonferroni correction is much more conservative (it doesn't let as many p-values pass correction). \n",
    "\n",
    "Let's put this plotting code into a function too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_FDR(pvals, q=.05):\n",
    "    n = len(pvals)\n",
    "    plt.plot(np.sort(pvals), label='p-values')\n",
    "    plt.plot([0,len(pvals)-1], [q/n, q], label='Adjusted q-values')\n",
    "    plt.plot([0,len(pvals)-1], [q/n, q/n], label='Bonferroni adjusted alpha')\n",
    "    plt.xlabel('# of p-values')\n",
    "    plt.xlabel('p-value (Probability)')\n",
    "    _ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Several Interesting Properties of the BH Procedure\n",
    "\n",
    "1\\. The smallest p-values don't have to be above the adjusted q-value line (or adjusted p-value below alpha), as long as one p-value is greater than the adjusted q-value line, then all the p-values smaller than that value are considered significant under an FDR correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "# Think of an exmaple of this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. As the number of significant p-values increases with a constant m, the number of accepted values increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "# Think of an exmaple of this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Adding a number h of highly significant values to a set of l high p-values can cause more than h values to be accepted under the FDR correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "# Think of an exmaple of this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FDR Correction on the Music Study\n",
    "\n",
    "Now let's apply the FDR correction to the music study, looking at the combined Region X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_regionX, pvals_regionX_corrected, adjusted_qvals_regionX = FDR(pvals_regionX)\n",
    "threshold_regionX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvals_regionX_corrected.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the threshold for correction has lowered the value to ~ `.02`, leaving about 800 something significant p-values. \n",
    "\n",
    "Let's look at a plot of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_FDR(pvals_regionX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the p-values line crosses the adjusted q-value slop just around 800, which is the number of significant values we saw above!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FDR Correction on the Encoding Model\n",
    "\n",
    "Let's return to the encoding model we fit earlier and calculate an FDR correction on those p-values. First we'll visualize the p-values and the adjusted q-value line to get an idea of what to expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,5))\n",
    "plt.plot(np.sort(p_value_pred_acc_large))\n",
    "plt.plot([0,num_cortical_voxels], [alpha/num_cortical_voxels, alpha])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_sem3, pvals_sem3_corrected, adjusted_qvals_sem3 = FDR(p_value_pred_acc_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_pred_acc_sem3_corrected = cortex.Volume(p_value_pred_acc_large < threshold_sem3, subject='s04', xfmname='color_natims', vmin=0, vmax=1, cmap='Reds')\n",
    "_ = cortex.quickflat.make_figure(volume_pred_acc_sem3_corrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks a lot more liberal than the Bonferroni correction. Let's compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_pvalue_pred_acc_large = cortex.Volume(pred_acc_sig_Bon_large, subject='s04', xfmname='color_natims', vmin=0, vmax=1, cmap='Reds')\n",
    "_ = cortex.quickshow(volume_pvalue_pred_acc_large)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complex Encoding Models\n",
    "\n",
    "Now let's look at some more complex encoding models and compare them with the simple 3-category model above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19-Category Semantic Model\n",
    "\n",
    "This model contains 19 semantic categories. Let's have a look at them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_sem19_filename = \"/data/cogneuro/fMRI/images/color_natims_features_19cat.json\"\n",
    "features_sem19 = json.load(open(features_sem19_filename, 'r'))\n",
    "features_sem19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "designmat_19_file = h5py.File(\"/data/cogneuro/fMRI/images/color_natims_features_19cat.hdf\")\n",
    "designmat_19_train = designmat_19_file['est'][:]\n",
    "designmat_19_test = designmat_19_file['val'][:]\n",
    "designmat_19_train.shape, designmat_19_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_19 = LinearRegression()\n",
    "lr_19.fit(designmat_19_train, data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_19_test = lr_19.predict(designmat_19_test)\n",
    "pred_acc_19_test = correlate(data_test, pred_19_test)\n",
    "pred_acc_19_test[pred_acc_19_test>.9] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = cortex.quickflat.make_figure(cortex.Volume(pred_acc_19_test, subject='s04', xfmname='color_natims', cmap='hot'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Model Comparison\n",
    "\n",
    "To compare the prediction accuracy performance of two encoding models we can create scatter plots where each point represents the prediction accuracy performance on the two models being compared. Let's have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pred_acc_sem3_test, pred_acc_19_test, '.')\n",
    "plt.axis('square')\n",
    "plt.plot([-.2,.6], [-.2,.6])\n",
    "plt.xlabel('3 Categories')\n",
    "_ = plt.ylabel('19 Categories')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that neurons in early visual cortex respond to contrast gradients, or edges, in images. Now we'll create an encoding model that captures this property in a crude way to see if we can find an encoding model that fits areas of early visual cortex well.\n",
    "\n",
    "First we'll load a file which contains all of the images shown to the subjects in the experiment we've been using all lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file = h5py.File(\"/data/cogneuro/fMRI/images/color_natims_images.hdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neurons in the early visual cortex that we're targeting with this model react most strongly to the intensity, or luminance, of an image, and not to it's color. For this reason, we'll convert the images to grayscale to make image processing easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_gray = np.zeros((1386, 96, 96))\n",
    "images_gray[:1260] = np.rollaxis(image_file['est'][:].mean(axis=2), 2)\n",
    "images_gray[1260:] = np.rollaxis(image_file['val'][:].mean(axis=2), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot an example image from this data set to get an idea of what it looks like, and how the gradient model works. This image has been converted to grayscale, it was originally a color image in the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images_gray[0])\n",
    "plt.gray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll calculate the gradient, or the difference between adjacent voxels, which gives us a measure of how much contrast or change there is in the image. We'll do it in both the x and y axes of the image and average the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_x = np.diff(images_gray, axis=1)\n",
    "diff_y = np.diff(images_gray, axis=2)\n",
    "diff = np.sqrt(diff_x[..., :-1] ** 2 + diff_y[:, :-1] ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the gradient image to see what information it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(diff[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can defintely see it roughly finds the \"edges\" of the image where there is a large change in the pixel intensity. Now let's smooth those differences out before we downsample the image, which is a way of \"averaging\" the information in multiple pixels. We'll create an image that is 1/100 the number of pixels by downsampling by 10 in the x and y axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed = gaussian_filter(diff, sigma=(0, 2, 2))[:, ::10, ::10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the smoothed and downsampled image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(smoothed[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't really look like much! But you can see roughly where the seal was.\n",
    "\n",
    "Now we'll use this image as an encoding model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gradient = LinearRegression()\n",
    "model_gradient.fit(smoothed[:1260].reshape(1260, -1), data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the prediction accuracy of this gradient encoding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_gradient = model_gradient.predict(smoothed[1260:].reshape(-1,100))\n",
    "pred_acc_gradient = correlate(data_test, pred_gradient)\n",
    "pred_acc_gradient[pred_acc_gradient>.9] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the predication accuracy performance of this model onto a flatmap. Where do you expect to see this model perform well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_pred_acc_gradient = cortex.Volume(pred_acc_gradient, subject='s04', xfmname='color_natims', cmap='hot')\n",
    "_ = cortex.quickflat.make_figure(volume_pred_acc_gradient, with_rois=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compare the Gradient and 19 category encoding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pred_acc_gradient, pred_acc_19_test, '.')\n",
    "plt.axis('square')\n",
    "plt.plot([-.2,.6], [-.2,.6])\n",
    "plt.xlabel('Energy')\n",
    "_ = plt.ylabel('19 Categories')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout Session\n",
    "\n",
    "1\\. Can you identify which voxels in the scatter plot you think exist in early visual cortex and which exist in later, semantic, areas of visual cortex?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding (Time Permitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until now we have been asking the question \"What can our knowledge about the stimulus predict about the brain data?\". For this, we trained a model on stimulus properties of each TR and had it predict the brain activity on new data.\n",
    "\n",
    "What if we ask \"What can our knowledge of brain activation predict about the stimulus it was being shown?\". Asking the question this way is also asking \"How much information about the stimulus is in the measured brain data and can we obtain it somehow?\". Tackling this question is often called **decoding**. It generally involves predicting an outside measure (such as the stimulus presented or the task performed or the brain state we know the subject is in [e.g. sleeping]) from the brain measurements. So in a modeling context, the brain measurements become the *independent* variable, and the property we are trying to predict becomes the *dependent* variable.\n",
    "\n",
    "**SO**: Let's see if we can use `LinearRegression` to predict the other way round!\n",
    "\n",
    "**Questions:**\n",
    "1. How do we predict each category and maybe dependencies between categories?\n",
    "2. How do we predict a category if `LinearRegression` predicts numbers?\n",
    "\n",
    "**Answers:**\n",
    "1. Here we will predict each category separately\n",
    "2. Since we are predicting presence/absence of a category, we can predict the value of the z-scored design matrix and then threshold.\n",
    "\n",
    "**Note:** Here we will be doing a very simple analysis that is typically not conducted this way: Thresholding the outcome of a linear regression is usually not the way categorical variables are predicted. One would more commonly use logistic regression (see `sklearn.linear_model.LogisticRegression`), SVMs (see `sklearn.svm.SVC`), or random forests (see `sklearn.ensemble.RandomForestClassifier`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voxel Selection\n",
    "\n",
    "Not all the voxels in the brain are going to contain information about what we are trying to predict. However, as we saw last lecture and this lecture, adding more random data to a model makes it more likely to overfit. In order to avoid the precision loss on the test set incurred by overfitting, we will pre-select voxels of which we think they might be useful in this prediction task.\n",
    "\n",
    "The type of voxel we are aiming for here are voxels that change their mean activity according to whether a concept is present or absent. Note that this is not the only way for a voxel to contain information about the stimulus (it could encode information in the difference of activity between two voxels, for example), but it is a very important type of voxel to search for and will suffice for our purposes.\n",
    "\n",
    "**How do we find these voxels?**\n",
    "Changing the mean activity according to whether a stimulus is present or not should ring a bell - this is roughly what encoding model performance can tell us about the voxel. So how about we just select the best performing voxels from an encoding model?\n",
    "\n",
    "**Strict data separation**\n",
    "We cannot use the performance of the model on the test set to select the voxels - that is cheating and called *data snooping*. So we will split our train set into a sub-train-set and a sub-test-set called `train_train` and `train_test`. We will use `4/5` of the data for model fitting and `1/5` for the evaluation.\n",
    "\n",
    "Let's get the indices for this data split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_train_indices = slice(0, 4 * 1260 // 5)  # This is for slicing in order to save memory\n",
    "train_test_indices = slice(4 * 1260 // 5, 1260) # Could also use np.arange!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the design matrices and data corresponding to these splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "designmat_sem3_train_train = designmat_sem3_train[train_train_indices]\n",
    "designmat_sem3_train_test = designmat_sem3_train[train_test_indices]\n",
    "\n",
    "data_train_train = data_train[train_train_indices]\n",
    "data_train_test = data_train[train_test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(designmat_sem3_train_train, data_train_train)\n",
    "\n",
    "train_test_pred = lr.predict(designmat_sem3_train_test)\n",
    "train_test_correlations = correlate(train_test_pred, data_train_test)\n",
    "train_test_correlations.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at these in descending order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 2))\n",
    "plt.plot(sorted(train_test_correlations)[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we thresholded at `0.15`, how many voxels would we get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_test_correlations > 0.10).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seems like a good number to start with. Let's select those voxels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cortex.quickshow(cortex.Volume(train_test_correlations > 0.10, 's04', 'color_natims', vmin=0, cmap='hot'))\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_voxels = train_test_correlations > 0.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverting the Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_decoding = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now mask out only the voxels we care about:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoding_X_train_train = data_train_train[:, selected_voxels]\n",
    "decoding_X_train_test = data_train_test[:, selected_voxels]\n",
    "\n",
    "decoding_X_test = data_test[:, selected_voxels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit category 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_decoding.fit(decoding_X_train_train, designmat_sem3_train_train[:, 1])\n",
    "predicted_categories_continuous = lr_decoding.predict(decoding_X_train_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, linear regression gives us continuous predictions, which we will need to threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(predicted_categories_continuous)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_categories = predicted_categories_continuous > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the accuracy by comparing when our predicted category is positive and when the design matrix is positive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.equal(predicted_categories, (designmat_sem3_train_test[:, 1] > 0)).mean()\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation - is this good or bad?\n",
    "\n",
    "When evaluating predictive accuracy scores, one always needs to compare to what an uninformed predictor could do:\n",
    "\n",
    "1. predict completely randomly\n",
    "2. Predict the most frequent category\n",
    "\n",
    "Let's check 2.: How often is `designmat_sem3_train_test[:, 1] > 0` anyway?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(designmat_sem3_train_test[:, 1] > 0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roughly half of the time. Our predictor is better than that! So we have inferred information from brain data about this category!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakout session\n",
    "1. Try the other two categories\n",
    "2. Try predicting on the actual test set instead of the `train_test` set.\n",
    "2. Try other voxel selection thresholds. If you are really curious, plot a diagram of accuracy versus selection threshold! Then you can select the best predictor for `train_test` and try it on the actual test set.\n",
    "3. Evaluate the performance of `sklearn.linear_model.LogisticRegression` on the same task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
