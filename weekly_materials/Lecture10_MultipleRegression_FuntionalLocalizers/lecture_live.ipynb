{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 10: Multiple Regression and Functional Localizers\n",
    "\n",
    "## Goals\n",
    "- **Neuroscience / Neuroimaging concepts**\n",
    "    - Functional Localizers\n",
    "    - Regions of Interest (ROIs)\n",
    "    - ROIs of the visual system\n",
    "- **Datascience / Coding concepts**\n",
    "    - Multiple Regression\n",
    "    - Functional Contrasts of estimated regression weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Simply run the cells below that contain all the Python modules we'll neeed, plus setup matplotlib for plotting in this jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats\n",
    "from scipy.stats import zscore\n",
    "import nibabel\n",
    "import cortex\n",
    "import os\n",
    "from nistats.hemodynamic_models import glover_hrf as create_hrf\n",
    "import nibabel\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, widgets, FloatSlider\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set plotting defaults\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "Define a few helper functions that we'll use in this lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nifti(filename, z_score=True, mask=None):\n",
    "    img = nibabel.load(filename)\n",
    "    data = img.get_data().T\n",
    "    if mask is not None:\n",
    "        data = data[:, mask]\n",
    "    if z_score:\n",
    "        data = zscore(data, axis=0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some helpers for visualization\n",
    "def show_error_time(x, y, slope, intercept):\n",
    "    \"\"\"Plots the whole thing as a time series instead of a point cloud\"\"\"\n",
    "    n = x.shape[0]\n",
    "    plt.plot(y, 'b')\n",
    "    plt.hlines([0], x.min(), x.max())\n",
    "    num_x = 1 if x.ndim == 1 else x.shape[1]\n",
    "    t = np.arange(n)\n",
    "    new_x = np.zeros((n,))\n",
    "    for cur_x in range(num_x):\n",
    "        cur_x = (intercept + slope[cur_x] * x[:,cur_x])\n",
    "        new_x += cur_x \n",
    "    plt.plot(new_x, 'r', lw=1)\n",
    "    plt.fill_between(t, new_x, y[:,0], color='g', alpha=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_error_scatter(x, y, slope, intercept):\n",
    "    x = x[:,0]\n",
    "    y = y[:,0]\n",
    "    x_left = x.min() - x.ptp() * .2\n",
    "    x_right = x.max() + x.ptp() * .2\n",
    "    y_est = slope * x + intercept\n",
    "    y_left, y_right = slope * x_left + intercept, slope * x_right + intercept\n",
    "    y_bottom, y_top = y.min() - y.ptp() * .2, y.max() + y.ptp() * .2\n",
    "    xs = np.vstack([x, x])\n",
    "    bars = np.vstack([y, y_est])\n",
    "    plt.hlines([0], x_left, x_right)\n",
    "    plt.vlines([0], y_bottom, y_top)\n",
    "    plt.scatter(x, y)\n",
    "    plt.plot([x_left, x_right], [y_left, y_right], 'r', lw=2)\n",
    "    plt.plot([0, 0], [0, intercept], 'm', lw=2)\n",
    "    plt.plot(xs, bars, 'g-.')\n",
    "    plt.axis([x_left, x_right, y_bottom, y_top])\n",
    "    return ((y_est - y) ** 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_error_both_ways(x, y, slope, intercept):\n",
    "    if x.ndim == 1:\n",
    "        x = x.reshape(len(x),1)\n",
    "    if y.ndim == 1:\n",
    "        y = y.reshape(len(y),1)\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    plt.axes([0, 0, .18, 1.])\n",
    "    e = show_error_scatter(x, y, slope, intercept)\n",
    "    plt.axes([.2, 0, .8, 1])\n",
    "    show_error_time(x, y, [slope], intercept)\n",
    "    print('Sum of squared error (SSE): %.04f' % (e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a mask\n",
    "mask = cortex.db.get_mask('s01', 'catloc', 'cortical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify filenames\n",
    "filename1 = \"/data/cogneuro/fMRI/categories/s01_categories_01.nii.gz\"\n",
    "filename2 = \"/data/cogneuro/fMRI/categories/s01_categories_02.nii.gz\"\n",
    "filename3 = \"/data/cogneuro/fMRI/categories/s01_categories_03.nii.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load, mask, zscore, and concatenate the three localizer runs\n",
    "run1 = load_nifti(filename1, z_score=True, mask=mask)\n",
    "run2 = load_nifti(filename2, z_score=True, mask=mask)\n",
    "run3 = load_nifti(filename3, z_score=True, mask=mask)\n",
    "\n",
    "# Concatenate the data\n",
    "data = np.concatenate((run1, run2, run3), axis=0)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the faces and places voxels\n",
    "data_faces = data[:, 3464]\n",
    "data_places = data[:, 10433]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the design matrix\n",
    "categories = np.load(\"/home/jovyan/catloc_experimental_conditions.npy\")\n",
    "unique_categories_no_nothing = np.unique(categories[categories != 'nothing'])\n",
    "hrf = create_hrf(tr=2, oversampling=1, time_length=32)\n",
    "stimulus_vectors = []\n",
    "designmat_full = np.zeros((len(categories), len(unique_categories_no_nothing)))\n",
    "for i in np.arange(len(unique_categories_no_nothing)):\n",
    "    unique_category = unique_categories_no_nothing[i]\n",
    "    stimulus_vectors.append(categories == unique_category)\n",
    "    designmat_full[:,i] = np.convolve(stimulus_vectors[i], hrf)[:len(stimulus_vectors[i])]\n",
    "designmat_full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "Last class we reviewed correlation and introduced linear regression in one variable. We had established that if an x-y pointcloud is shown in standard units the correlation indicates the slope of the best fitting line through the origin. We write $\\hat y$ for the points on that line and $r$ for the correlation have the relationship\n",
    "$$\\hat y = r x.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression deals with the general case, where we would like to fit a line through an arbitrary point cloud that is not necessarily centered in the origin. The diagram on the top shows the regression line in standard units. To go from standard units to the original units everything needs to be rescaled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/regline_z.png\" />\n",
    "<img src=\"figures/regline_orig.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing out the difference quotient we obtained\n",
    "$$\\frac{\\hat y - \\bar y}{SD(y)} = r\\frac{x - \\bar x}{SD(x)}$$\n",
    "\n",
    "Regression expresses $\\hat y$ as a funtion of $x$, so we reorganized the formula into\n",
    "$$\\hat y = \\bar y + SD(y)r\\frac{x - \\bar x}{SD(x)}$$\n",
    "\n",
    "Collecting terms with and without $x$ together we get \n",
    "$$\\hat y = \\frac{SD(y)}{SD(x)}r x + \\bar y - \\frac{SD(y)}{SD(x)}r\\bar x$$\n",
    "And we wrote\n",
    "$$\\hat y = \\textrm{slope}\\cdot x + \\textrm{intercept},$$\n",
    "with\n",
    "$$\\textrm{slope} = \\frac{SD(y)}{SD(x)}r$$\n",
    "$$\\textrm{intercept} = \\bar y - \\textrm{slope}\\cdot\\bar x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that these quantities, *slope* and *intercept*, computed in this way, define the line which minimizes the *Sum of Squared Errors* and *Mean Squared Error*, which basically computes how far off the line is from the point cloud on the y-axis:\n",
    "\n",
    "For each point $(x_i, y_i)$ we can write \n",
    "$$e_i = y_i - (\\textrm{slope}\\cdot x_i + \\textrm{intercept})$$\n",
    "This is called a *residual* (also known as the error term). We square the residuals $e_i$ to make them positive, then add all of them up for all the points and get\n",
    "$$SSE = \\sum_i e_i^2$$\n",
    "If you are curious, you can try to minimize this sum of squared errors by taking the derivatives with respect to slope and intercept and setting them to zero.\n",
    "\n",
    "Let's recall some functions from last class and the homework and re-create the visualization widget fitting regressions by hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply_add(one_x, many_y):\n",
    "    return np.dot(one_x, many_y)\n",
    "\n",
    "def correlate(one, many):\n",
    "    one_z = zscore(one)\n",
    "    many_z = zscore(many, axis=0)\n",
    "    product_sum = multiply_add(one_z, many_z)\n",
    "    return product_sum / len(one_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this function `correlate`, we were able to compute correlations of one response vector to *all voxels at the same time*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_response_vec = designmat_full[:, unique_categories_no_nothing == 'faces'].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_faces = correlate(faces_response_vec, data)\n",
    "corr_faces.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTERLUDE** (not exam-relevant, but useful for intuition lower down)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can modify `multiply_add` ever so slightly, and get a function that correlates **all response vectors with all voxels**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The secret lies in the TRANSPOSE:\n",
    "def multiply_add(many_x, many_y):\n",
    "    return np.dot(many_x.T, many_y)\n",
    "\n",
    "def correlate(many_x, many_y):\n",
    "    many_x_z = zscore(many_x, axis=0)\n",
    "    many_y_z = zscore(many_y, axis=0)\n",
    "    product_sum = multiply_add(many_x_z, many_y_z)\n",
    "    return product_sum / many_x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_all = correlate(designmat_full, data)\n",
    "corr_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END INTERLUDE** (end of non-exam-relevant cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression visualization\n",
    "Now let's remind ourselves of some ways of visualizing time series data and regression lines. When working with two scalar quantities, we can make a scatterplot and draw a line through it. We can also visualize the voxel time series and the fitted response vector next to each other. The latter visualization will also generalize to multiple regression.\n",
    "\n",
    "Let's take a look at both of them in this interactive plot for a faces-responsive voxel and the faces response vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_error_voxel_both(slope, intercept):\n",
    "    return show_error_both_ways(faces_response_vec, data_faces, slope, intercept)\n",
    "\n",
    "interactive_plot = interactive(show_error_voxel_both, \n",
    "                               slope=widgets.FloatSlider(value=0, min=-5, max=5), \n",
    "                               intercept=widgets.FloatSlider(value=0, min=-5, max=5))\n",
    "output = interactive_plot.children[-1]\n",
    "output.layout.height = '250px'\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After working a bit with the interactive plot, let's compute the exact values for slope and intercept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_slope(x, y):\n",
    "    sx = np.std(x)\n",
    "    sy = np.std(y)\n",
    "    r = correlate(x, y)\n",
    "    return r * sy / sx\n",
    "\n",
    "def calc_intercept(x, y):\n",
    "    mx = np.mean(x)\n",
    "    my = np.mean(y)\n",
    "    slope = calc_slope(x, y)\n",
    "    return my - slope * mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope = calc_slope(faces_response_vec, data_faces)\n",
    "intercept = calc_intercept(faces_response_vec, data_faces)\n",
    "print(slope, intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call the visualization again with these precise values. What we then see is the best fit in the sense that it minimizes the some of squared errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_error_voxel_both(slope, intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression for real fMRI data\n",
    "\n",
    "Now let's continue applying linear models to real fMRI data. As introduced in last lecture, this time we'll use a linear regression estimator from scikit-learn to get another glimpse at this software package. We'll fit the faces response vector to our FFA voxel and see how we do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model_faces = LinearRegression()\n",
    "reg_model_faces.fit(faces_response_vec, data_faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh-Oh, why didn't that work? Remember that we need to reshape vectors to 1-D matrices. This is annoying now, but will become useful once we want to find regression models to multiple voxels at the same time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model_faces = LinearRegression()\n",
    "reg_model_faces.fit(faces_response_vec.reshape(-1,1), data_faces.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, that did work now!\n",
    "\n",
    "OK. Where is the info hidden? In all scikit-learn estimators, the properties computed during `fit` are stored in names with *trailing underscore*. In this case it is `.intercept_` and `.coef_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model_faces.intercept_, reg_model_faces.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, these values correspond to `slope` and `intercept` computed above. \n",
    "\n",
    "Let's see how good this model is by calculating the SSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_faces_pred_faces = reg_model_faces.predict(faces_response_vec.reshape(-1,1))[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_sse = np.sum((data_faces - data_faces_pred_faces) **2)\n",
    "faces_sse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the data and the regression line to see how well it fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.plot(faces_response_vec, data_faces_pred_faces, 'k', label='Regression Line')\n",
    "plt.plot(faces_response_vec, data_faces, '.r', label='Face Data')\n",
    "plt.xlabel('Faces Response')\n",
    "plt.ylabel('BOLD Signal')\n",
    "plt.title('FFA Voxel Regression')\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout Session\n",
    "\n",
    "1\\. Fit a linear regression model to the PPA voxel BOLD data using the `places` response vector. Plot the data as a scatter plot, and the regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "### END STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression\n",
    "\n",
    "The real power of regression comes when we have more than one **independent variable** we want to associate with the **dependent variable**.\n",
    "\n",
    "The case of multiple linear regression is a natural extension of the simple linear regression case. In multiple regression, we have multiple independent variables ($x^i$), and still a single dependent variable (y). Now the equation describing each time point of data looks like this: \n",
    "\n",
    "\\begin{align}\n",
    "y_j =  w_0 + w_1 x^1_j + w_2 x^2_j + \\dots + w_3 x^d_j +\\varepsilon_j\n",
    "\\end{align}\n",
    "\n",
    "We can rewrite this second equation for multiple regression using a more succint notation:\n",
    "\n",
    "\\begin{align}\n",
    "Y =  {\\bf X}W + w_0 +\\varepsilon,\n",
    "\\end{align}\n",
    "\n",
    "where\n",
    "- $Y$ is the n x 1 array of BOLD data\n",
    "- ${\\bf X}$ is the n x d matrix were each of the columns is a response vector (${\\bf X}_{ji} = x^i_j$)\n",
    "- ${W}$ is d x 1, contains all the $w_i$\n",
    "- $\\varepsilon$ is the n x 1 vector of errors, or residuals\n",
    "\n",
    "We can write the solution to this multiple regression model as the following equation, which is what we will use to fit our model. This is just for your own information - you don't need to know this equation for any exams:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat W = ({\\bf X}^\\top{\\bf X})^{-1}{\\bf X}^\\top Y\\\\\n",
    "\\end{align}\n",
    "\n",
    "Even though the above formula is not necessary for exams, take a good look at it, and try to peel apart what it means. It is likely that you will encounter it more than once throughout your career. Disregard the $({\\bf X}^\\top{\\bf X})^{-1}$ for a bit and take a look at the rest (${\\bf X}^\\top Y$). Does it ring a bell? If not, refer back to the non-exam-relevant interlude from above.\n",
    "\n",
    "As it turns out, the equation that minimizes the Sum of Squared Errors (SSE) is exactly the same for the multiple regression case as it is for the simple linear regression case, so we can use the same `LinearRegression` function for both! Let's explore this..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending the Face Model: Modeling Faces and Bodies\n",
    "\n",
    "Since we know that the subject saw more than just images of faces, it makes sense that we would want to model more than just the response to faces in order to get a model that better explains the BOLD data (y). Now let's see what happens if we include the bodies response vector in the model as well..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuition using time series plots\n",
    "\n",
    "First let's plot the FFA BOLD data along with the response vectors for the faces and bodies to remind ourselves what the signal looks like when bodies were shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bodies_response_vec = designmat_full[:, unique_categories_no_nothing == 'body'].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(18, 6))\n",
    "_ = plt.plot(data_faces, label = 'Bold Data')\n",
    "_ = plt.plot(faces_response_vec, label = 'Faces')\n",
    "_ = plt.plot(bodies_response_vec, label = 'Bodies')\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create a 2-D $x$ matrix, which is called a **design matrix**, from the response vectors for bodies and faces. We'll use `np.stack` to do that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "designmat_faces_bodies = np.stack((faces_response_vec, bodies_response_vec), axis=1)\n",
    "designmat_faces_bodies.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And reshape the FFA voxel BOLD data into a 2-D matrix with one column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_faces_2D = data_faces.reshape(len(data_faces),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_error_faces_bodies(intercept, slope_faces, slope_bodies):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    show_error_time(designmat_faces_bodies, \n",
    "                    data_faces_2D, \n",
    "                    [slope_faces, slope_bodies], \n",
    "                    intercept)\n",
    "    sse = ((data_faces - \n",
    "            (designmat_faces_bodies.dot([slope_faces, slope_bodies]) + intercept)) ** 2).sum()\n",
    "    print(\"Sum of squared errors {}\".format(sse))\n",
    "\n",
    "interactive_plot = interactive(show_error_faces_bodies, \n",
    "                               intercept=widgets.FloatSlider(value=0, min=-5, max=5), \n",
    "                               slope_faces=widgets.FloatSlider(value=0, min=-5, max=5), \n",
    "                               slope_bodies=widgets.FloatSlider(value=0, min=-5, max=5))\n",
    "output = interactive_plot.children[-1]\n",
    "output.layout.height = '350px'\n",
    "interactive_plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model\n",
    "\n",
    "Now let's fit a multiple regression model using both the faces and the bodies response vectors as independent variables. We just learned that the same equation estimates the coefficients for simple and multiple linear regression, so we can use the same `LinearRegression` object to fit this multiple regression model. Instead of reshaping a single response vector into a 2-D matrix, we'll use the design matrix we just created. We'll use the reshaped FFA voxel BOLD data and fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model_face_body = LinearRegression()\n",
    "reg_model_face_body.fit(designmat_faces_bodies, data_faces_2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the coefficient for faces from this model and from the first regression model of faces that we built. There are 2 slope coefficients now, and since the faces response vector was in the first column, the first coefficient is for faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model_face_body.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Faces Coefficient with Bodies: %.03f' % (reg_model_face_body.coef_[0][0]))\n",
    "print('Faces Coefficient Only: %.03f' % (reg_model_faces.coef_[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two coefficients are similar, but not the same! We'll learn more about why this is later on. \n",
    "\n",
    "For now let's predict the original data using this faces and bodies model, and then calculate the sum of squared error (SSE) of this new model. Then we can compare this SSE value with the SSE we calculated above for the faces only model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_faces_pred_facebody = reg_model_face_body.predict(designmat_faces_bodies)[:, 0]\n",
    "faces_bodies_sse = np.sum((data_faces - data_faces_pred_facebody) ** 2)\n",
    "faces_bodies_sse, faces_sse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the SSE for the faces bodies model is lower than that for the faces only model we can conclude that including the bodies response vector into the model improves the fit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout Session\n",
    "\n",
    "1\\. Fit a multiple linear regression model to the PPA voxel that has 2 independent variables, the `places` and `object` response vectors. Calculate the predicted values, and use them to calculate the SSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "### END STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Full Model with All the Categories\n",
    "\n",
    "We saw that the model that incorporated bodies and faces did a better job than the model which just incorporated faces. Let's see what happens when we include the response vectors that describe all the stimuli that the subject saw.\n",
    "\n",
    "We'll do this for 5 voxels that are selective to the different stimulus types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxel_indices = np.array([16521, 3464, 8701, 8318, 8806])\n",
    "five_voxels = data[:, voxel_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the time series of these 5 voxels to get an idea of what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "for i in range(len(voxel_indices)):\n",
    "    timeseries = five_voxels[:, i]\n",
    "    plt.plot(timeseries + 5 * i)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll fit a multiple regression model in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model_full = LinearRegression()\n",
    "reg_model_full.fit(designmat_full, five_voxels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's predict and calculate the SSE again for the faces voxel, so we can compare it with the faces bodies model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_faces_pred_full = reg_model_full.predict(designmat_full)[:,1]\n",
    "full_sse = np.sum((data_faces-data_faces_pred_full)**2)\n",
    "print('Just using Faces and bodies, SSE is: %.02f, adding all categories SSE is: %.02f' % (faces_bodies_sse, full_sse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the SSE is slightly lower when adding all of the response vectors, but not by much! What matters in statistics is whether this is **significantly** lower. We'll learn about significance testing in the next lecture. \n",
    "\n",
    "Because adding the remaining regressors does not seem to reduce the error very much anymore, it seems unlikely that our voxel responds significantly to the added categories. However, using the full set of response vectors in regression is going to be useful when working with the full brain - there may be voxels that are very well modeled by these additional response vectors.\n",
    "\n",
    "Finally, let's plot the real BOLD data time series along with the predicted time series from using multiple regression to visualize how well the model fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = reg_model_full.predict(designmat_full)\n",
    "\n",
    "weights = reg_model_full.coef_\n",
    "intercepts = reg_model_full.intercept_\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "for i in range(len(voxel_indices)):\n",
    "    timeseries = five_voxels[:, i]\n",
    "    prediction = predictions[:, i]\n",
    "    plt.plot(timeseries + 5 * i, lw=2)\n",
    "    plt.plot(prediction + 5 * i, lw=2)\n",
    "    \n",
    "    # Now let's display the individual regressors\n",
    "    for j in range(len(unique_categories_no_nothing)):\n",
    "        plt.plot(designmat_full[:,j] * weights[i, j] + intercepts[i] + 5 * i, lw=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout Session\n",
    "\n",
    "1\\. We have just fit a full model to five voxels, one of which is the places-responsive PPA voxel from above, located at index 3 (fourth element). Use the regression model from above to compute its SSE. Does the SSE decrease dramatically relative to the SSE from the model of just scenes and objects?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "### STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Do We Need Multiple Regression?\n",
    "\n",
    "We just saw that simple and multiple linear regression don't always estimate the same value for coefficients of the same $x$ variable (the faces response). So why would we want to use multiple regression instead of estimating the coeffcicents of a number of simple linear regression models?\n",
    "\n",
    "The answer is that when there is a correlation between the $x$ variables, multiple regression can account for that correlation, whereas doing several simple regressions cannot. In other words, multiple regression controls for correlation in the independent variables. This leads to a more accurate estimation of all the slopes.\n",
    "\n",
    "[**Non-exam-relevant side note:** This is what $({\\bf X}^\\top{\\bf X})^{-1}$ does! It reweights correlations of response vectors and data according to correlation within response vectors]\n",
    "\n",
    "Let's take a look at an example to see how this works. We'll create 2 independent variables that are correlated, and then use the linear equation to create the y data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fake_multi_x1, fake_multi_x2 = create_fake_data(100, 0, 0, .5)\n",
    "fake_multi_x1 = np.zeros(100)\n",
    "fake_multi_x2 = np.zeros(100)\n",
    "fake_multi_x1[:50] = 1.\n",
    "fake_multi_x2[10:60] = 1.\n",
    "\n",
    "intercept_real1 = 0\n",
    "coef_real1 = 2\n",
    "coef_real2 = 1\n",
    "fake_multiple_y1 = intercept_real1 + fake_multi_x1 * coef_real1 + fake_multi_x2 * coef_real2 \n",
    "fake_multiple_y1 += np.random.randn(100) * .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 2))\n",
    "plt.plot(fake_multi_x1, label='Fake X1')\n",
    "plt.plot(fake_multi_x2, label='Fake X2')\n",
    "plt.plot(fake_multiple_y1, label='Fake Y')\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(fake_multi_x1, fake_multi_x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use 2 simple linear models to estimate the two coefficients separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model_simple1 = LinearRegression()\n",
    "reg_model_simple1.fit(fake_multi_x1.reshape(-1,1), fake_multiple_y1.reshape(-1,1))\n",
    "reg_model_simple2 = LinearRegression()\n",
    "reg_model_simple2.fit(fake_multi_x2.reshape(-1,1), fake_multiple_y1.reshape(-1,1))\n",
    "print('Real Coefficient 1: %.03f, Simple Estimated Coefficient 1: %.03f' % (coef_real1, reg_model_simple1.coef_[0][0]))\n",
    "print('Real Coefficient 2: %.03f, Simple Estimated Coefficient 2: %.03f' % (coef_real2, reg_model_simple2.coef_[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not so great. Now let's see what happens when we put both independent variables in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "designmat_fake_multiple = np.stack((fake_multi_x1, fake_multi_x2), axis=1)\n",
    "reg_model_multi1 = LinearRegression()\n",
    "reg_model_multi1.fit(designmat_fake_multiple, fake_multiple_y1.reshape(-1,1))\n",
    "print('Real Coefficient 1: %.03f, Simple Estimated Coefficient 1: %.03f' % (coef_real1, reg_model_multi1.coef_[0][0]))\n",
    "print('Real Coefficient 2: %.03f, Simple Estimated Coefficient 2: %.03f' % (coef_real2, reg_model_multi1.coef_[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's more like it! \n",
    "\n",
    "So the multiple regression model can account for the correlation between the two independent variables, and therefore find a much better estimate of the coefficients! That's why it's important to include all the independent variables you think may influence the dependent variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout Session\n",
    "Create three fake vectors of length 100 filled with zeros. Call them `fake_resp_1, fake_resp_2, fake_resp_3`. Next, place an `hrf` starting at location 10 in `fake_resp_1`, starting at location 12 in `fake_resp_2`, and starting at location 20 in `fake_resp_3`. Stack them as columns into a matrix `fake_resp`, and plot them. Compute a `fake_resp_y` by adding up 1 times `fake_resp_1`, 2 times `fake_resp_2` and 3 times `fake_resp_3`. Use linear  regression to perform simple linear regression with each one of the single responses, and then with `fake_resp`. Compare the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "### END STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answering Questions in Cognitive Neuroscience\n",
    "\n",
    "Multiple regression gives us a tool to determine the linear relationship between many independent variables (stimuli or tasks in fMRI experiments) and a single dependent variable (the BOLD signal). So how can we use this tool to learn about the brain? Let's remind ourselves of the types of questions that cognitive neuroscientists ask to better determine how to use this tool.\n",
    "\n",
    "Cognitive neuroscience is interested in identifying:\n",
    "* **Mental Processes and Representations and...**\n",
    "* **Localizing these onto the brain, both spatially and temporally.**\n",
    "\n",
    "The types of mental processes cognitive neuroscientists study include:\n",
    "* Attention\n",
    "* Decision-making\n",
    "* Learning\n",
    "* Memory\n",
    "* Language\n",
    "* Perception\n",
    "* Motor system\n",
    "* Social cognition\n",
    "* Emotions\n",
    "\n",
    "So we develop experiments that require the mental processes we're interested in studying, and find regions in the brain that are active when we believe that mental process should be utilized.\n",
    "\n",
    "We've been looking at perception and the motor system brain data, and so the types of questions we might be interested in asking are:\n",
    "\n",
    "* How does the visual system recognize a certain type of thing, such as faces or scenes? Are there brain regions that are active specifically to these?\n",
    "* What brain regions do visual processing in general?\n",
    "* What brain regions are used to compute and prepare body movements?\n",
    "\n",
    "Now that we know the types of questions we're interested in answering using fMRI, how can we use multiple linear regression to answer them? The weights that we get from multiple regression tell us how much $y$ increases for every increase of `1` in $x$. And since $y$ is the fMRI brain data, and $x$ are the experimental conditions, we can use these weights to determine when the brain activity increases for various experimental conditions, which quantifies the brain activity increases when the mental processes we want to study are utilized. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functional Localizers & Regions of Interest (ROIs)\n",
    "As we've just seen, the localization of cognitive proceses to specific brain regions is central to cognitive neuroscience. Thousands of researchers are working towards the goal of understanding the human brain, using many different ways of gaining knowledge and guiding investigation. Some results have been confirmed over and over again, showing that the same brain regions are involved in the same cognitive processes across many subjects and experiments. As such, these findings have established the existence of functional regions that have been given names (such as the FFA and PPA). These functional regions are broadly termed ***regions of interest*** (ROIs). Below are some examples of ROIs within the visual cortex:\n",
    "\n",
    "rOFA (Faces) in yellow, rLO (Objects) in blue, and rEBA (Bodies) in red\n",
    "<div>\n",
    "<br>\n",
    "<img src=\"figures/OFA_LO_EBA.png\" align=\"left\" style=\"height: 400px;\">\n",
    "<img src=\"figures/SingleSubject-Visual.png\" align=\"right\" style=\"height: 400px;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New studies build upon and extend these previous findings, and so we want to be able to show how our new findings relate spatially to established ROIs. ***Functional localizers*** are experiments done alongside the main study to *localize* the relevant ROIs, so the new findings can be contextualized with the previous literature. The data we've dealt with up until this point has actually been localizer data, so let's explore how to use that data to find ROIs before going on to ask more interesting questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Caveat on Intepreting ROIs\n",
    "It is **very** important to note that *functional regions* do not completely describe the processing done in a specific brain region. While at the spatial resolution that fMRI provides researchers see strong evidence for functional localization, this does not mean that a given brain region **does \"X\"** (e.g. FFA does face processing). This is just a description at a certain level of abstraction, and the computations actually being carried out in any brain region are far more complex than simple \"face processing\", or whatever the ROIs is said to do. \n",
    "\n",
    "1. At a lower level it is becoming increasingly clear that functional organization is actually continuously varying across cortex.\n",
    "2. At a higher level, functional regions interact with each other. The function of any given region might also be better described by taking this into account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional Localizer Analysis\n",
    "\n",
    "Up to this point we've used two different techniques to analyze our visual localizer data and identify cortical regions that respond to different types of visual stimuli (ROIs), such as faces. First we took the mean of all the TRs during the blocks of each type of stimulus presentation. Then we improved this analysis by accounting for the HRF and correlating response vectors with the BOLD signal. Now we'll use multiple regression to account for the overlapping HRFs from adjacent blocks of differing stimulus types. This is the way functional localizers are generally analyzed in contemporary fMRI studies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by visualizing the response design matrix so we can see that the response vectors do indeed overlap in time, and so the correlate a little bit, so we want to take that into account when fitting our linear regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "for i in range(len(unique_categories_no_nothing)):\n",
    "    label = unique_categories_no_nothing[i]\n",
    "    stimulus_timeseries = stimulus_vectors[i]\n",
    "    response_timeseries = designmat_full[:,i]\n",
    "    plt.plot(stimulus_timeseries + 1.5 * i)\n",
    "    plt.plot(response_timeseries + 1.5 * i, label=label)\n",
    "    \n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's quantify how correlated the response vectors of the design matrix are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(designmat_full.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are all correlated negatively with a correlation coeffecient value around `0.2`. How much does this correlation effect the weights estimated using a simple linear regression vs. a multiple regression model? To find out we'll fit two models, one using just the faces response vector, and the other using the full design matrix. We'll also z-score both the $x$ and $y$ of both so that they'll both have an intercept close to zero and comparable slopes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_response_vec_z = zscore(faces_response_vec)\n",
    "reg_model_faces_z = LinearRegression()\n",
    "_ = reg_model_faces_z.fit(faces_response_vec_z.reshape(-1,1), zscore(data_faces_2D))\n",
    "\n",
    "designmat_full_z = zscore(designmat_full, axis=0)\n",
    "reg_model_full_z = LinearRegression()\n",
    "_ = reg_model_full_z.fit(designmat_full_z, zscore(data_faces_2D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we've z-scored both the $x$ and $y$ data, we should expect the intercept to be around `0`, let's verify that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model_faces_z.intercept_[0], reg_model_full_z.intercept_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup, the intercepts of both models are very close to zero. Now let's compare the weights estimated in both of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model_faces_z.coef_[0,0], reg_model_full_z.coef_[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So using multiple regression results in a higher weight for the faces regressor in the FFA voxel, indicating a stronger response since we've controlled for the correlation between the 5 indepedent variables (response vectors).\n",
    "\n",
    "Now let's apply multiple linear regression to all of the cortical voxels. The `LinearRegression` object allows for this natively, let's see how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model_full_allvoxels = LinearRegression()\n",
    "_ = reg_model_full_allvoxels.fit(designmat_full_z, data)\n",
    "reg_model_full_allvoxels.coef_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and plot the weights for faces onto a flatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_faces_multi = cortex.Volume(reg_model_full_allvoxels.coef_[:,1], subject='s01', xfmname='catloc')\n",
    "_ = cortex.quickflat.make_figure(volume_faces_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weird, this flatmap looks different what we've seen when taking the average of block data or using correlation! There is more area that shows high activation (in red). Let's figure out why in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout Session\n",
    "1\\. Plot a flatmap showing the weights for the `places` weights across cortical voxels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "### END STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional contrasts\n",
    "\n",
    "The question we are trying to answer with these localizer analyses are: \n",
    "> What regions of cortex are selective for faces (or places or bodies, etc.)? \n",
    "\n",
    "So how do we answer that question using fMRI? Remember that the BOLD signal is a relative signal, meaning we always have to compare activity in one condition with activity in another condition. We might be tempted to compare the face image condition with the simple resting condition (no stimulus shown). Doing this would tell us how much the BOLD signal increases to images of faces relative to seeing nothing (rest). \n",
    "\n",
    "This type of comparison is called a **functional contrast** and is accomplished by fitting a multiple regression model to the BOLD data, and then simply subtracting the weights for condition two (rest) from condition one (faces). Let's do this know, but first we need to update our design matrix to include the `nothing` (or rest) condition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nothing_stimulus_vector = categories == 'nothing'\n",
    "nothing_response_vector = np.convolve(nothing_stimulus_vector, hrf)[:len(nothing_stimulus_vector)]\n",
    "designmat_full_nothing = np.hstack((designmat_full, nothing_response_vector.reshape(-1,1)))\n",
    "designmat_full_nothing.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fit a new multiple regression model that includes the nothing condition so we can have weights for both `faces` and `nothing` for all the cortical voxels which is what we need to create a functional contrast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model_full_nothing = LinearRegression()\n",
    "_ = reg_model_full_nothing.fit(designmat_full_nothing, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's create the functional contrast of `faces - nothing` for all the cortical voxels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrast_faces_nothing = reg_model_full_nothing.coef_[:,1] - reg_model_full_nothing.coef_[:,5]\n",
    "contrast_faces_nothing.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can plot the values of this contrast onto a flatmap to view it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_contrast_faces_nothing = cortex.Volume(contrast_faces_nothing, subject='s01', xfmname='catloc')\n",
    "_ = cortex.quickflat.make_figure(volume_contrast_faces_nothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, this flatmap looks very similar to the one we made in the last section! Now we can go about answering why it looks different than the maps we created by averaging faces blocks or doing correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout Session\n",
    "\n",
    "1\\. Why is the BOLD signal relative? Another way to ask this question is why can't we just interpret the raw values of the BOLD signal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "### END STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Create a constrat for `places - nothing` and call it `contrast_places_nothing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "### END STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Now create a flatmap showing this `places - nothing` contrast. How is it similar to the one we just created for `faces - nothing`? And how is it different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "### END STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlling Cognitive Processes \n",
    "In the question above, we asked which regions are face **selective**. Let's get more formal about what we mean by **selective** when talking about ROIs. **Selective** means that a given region responds more to faces than to other types of stimuli. We've learned that there are so called *early visual regions* which process simple (**low-level**) properties of images, such as edges or curves. Since these regions are active whenever there is any kind of visual stimulus we wouldn't want to say they are face selective just because they are active when the subject views images of faces, those regions would be active for any type of image. By comparing BOLD signal increase to images of faces relative to rest we foundactivity in early visual regions as well as the face selective regions. \n",
    "\n",
    "A more general way to say this is that we want to account for all the cognitive processes an experiment can engage except for the specific cognitive process we want to study. So in this case we simply want the cognitive process of face processing, but not general low-level visual processing. So how do we go about finding a brain region that is involved in face processing, but not any other cognitive processes? We do this by being smart about the functional contrasts we use. \n",
    "\n",
    "**Specifically, we want to compare two experimental conditions that use all of the same cognitive processes except for the specific process being studied.** \n",
    "\n",
    "In the case of this visual localizer, we can compare the faces and places conditions since both types of images will ellicit activity in the early visual regions. Let's have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_categories_no_nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrast_faces_places = reg_model_full_nothing.coef_[:,1] - reg_model_full_nothing.coef_[:,3]\n",
    "contrast_faces_places.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_contrast_faces_places = cortex.Volume(contrast_faces_places, subject='s01', xfmname='catloc')\n",
    "_ = cortex.quickflat.make_figure(volume_contrast_faces_places)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks much better! We see the face selective regions in dark red (FFA and OFA) that we saw by averaging blocks or correlating response and BOLD data vectors. This map is different, however. There are regions of dark blue that weren't there before, and much of cortex is light red. \n",
    "\n",
    "The dark blue regions are actually place selective regions, and come from the fact that we subtracted the places weights for this contrast. \n",
    "\n",
    "Now let's just remind ourselves what the correlation map looked like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_faces_bold = correlate(faces_response_vec, data)\n",
    "volume_corr_faces_bold = cortex.Volume(corr_faces_bold, subject='s01', xfmname='catloc')\n",
    "_ = cortex.quickflat.make_figure(volume_corr_faces_bold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout Session\n",
    "\n",
    "1\\. Create the `places - faces` contrast and call it `contrast_places_faces`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "### END STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Plot the flatmap for the `places - faces` contrast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "### END STUDENT ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative vs. Quantitative ROIs\n",
    "\n",
    "In all of these analysis we've simply been finding regions that have high activity for the functional contrast we're looking at, and saying that general region is the ROI in question (e.g. FFA). This is a very **qualitative** way of deciding on the location of the ROI, and we would like to be more **quantitative**. In other words, we want to be able to define some kind of threshold and say any voxel whose activity increases X amount under the contrast we will say is part of the ROI. The quantitative threshold we will use for this will be calculated by finding voxels whose increase in activity is **signifcantly** greater than zero. We will learn how to do this next week when we cover hypothesis testing and permutation testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common ROIs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Fusiform Face Area (FFA) (Kanwisher et al. 1997, http://www.jneurosci.org/content/17/11/4302)\n",
    "\n",
    "<img src='figures/F1.large.jpg'>\n",
    "**Figure caption from paper:** *\"Results of Part II. Left column, Sample stimuli used for the faces versus objects comparison as well as the two subsequent tests. Center column, Areas that produced significantly greater activation for faces than control stimuli for subject S1. a, The faces versus objects comparison was used to define a single ROI (shown in green outline for S1), separately for each subject. The time courses in the right column were produced by (1) averaging the percentage signal change across all voxels in a given subjects ROI (using the original unsmoothed data), and then (2) averaging these ROI-averages across the five subjects. F andO in a indicate face and object epochs;I and S in b indicate intact and scrambled face epochs; and F andH in c indicate face and hand epochs.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FFA via faces > places contrast\n",
    "\n",
    "Looking at unique labels, we can see that faces has index 1 and places has index 3. So let's subtract them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = reg_model_full_allvoxels.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_contrast = weights[:, 1] - weights[:, 3]\n",
    "\n",
    "_ = cortex.quickshow(cortex.Volume(face_contrast, 's01', 'catloc'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lower \"blobs\" are FFA, the bigger one higher up is OFA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Parahippocampal Place Area (Epstein et al. 1999, https://www.nature.com/articles/33402)\n",
    "\n",
    "<img src='figures/392598aa.eps.2.gif'>\n",
    "**Figure caption from paper:** *\"Results of experiment 1, demonstrating that the PPA responds selectively to scenes. **a** Examples of intact and scrambled versions of the four different types of stimuli (top), and the average percent signal change for each stimulus type in the PPA averaged over all subjects (bottom). The difference between intact and scrambled versions of each picture is a measure of the response in the PPA to each stimulus type partially unconfounded from the response to its low-level visual features. Half of the scenes were outdoor scenes of the MIT campus, and half were indoor scenes\n",
    "of\n",
    "un\n",
    "familiar\n",
    "locations.\n",
    "b\n",
    ",\n",
    "The\n",
    "tim\n",
    "e\n",
    "course\n",
    "of\n",
    "the\n",
    "per\n",
    "cent\n",
    "change\n",
    "in\n",
    "MR\n",
    "signal\n",
    "inten\n",
    "sity\n",
    "in\n",
    "the\n",
    "PP\n",
    "A\n",
    "ov\n",
    "er\n",
    "the\n",
    "pe\n",
    "riod\n",
    "of\n",
    "the\n",
    "scan.\n",
    "P\n",
    "er\n",
    "cent\n",
    "signal\n",
    "change\n",
    "was\n",
    "calcu\n",
    "lated\n",
    "individ\n",
    "ually\n",
    "for\n",
    "each\n",
    "subje\n",
    "ct\n",
    "using\n",
    "that\n",
    "subject\n",
    "s\n",
    "fixation\n",
    "activation\n",
    "as\n",
    "baseline\n",
    "and\n",
    "then\n",
    "aver\n",
    "aging\n",
    "a\n",
    "cr\n",
    "oss\n",
    "subje\n",
    "cts\n",
    "(black\n",
    "dot\n",
    "indicates\n",
    "fixation\n",
    "epochs).\n",
    "i,\n",
    "Intact\n",
    ";\n",
    "s,\n",
    "scr\n",
    "am\n",
    "bled;\n",
    "S,\n",
    "sce\n",
    "nes;\n",
    "F\n",
    ",\n",
    "faces;\n",
    "O,\n",
    "objects;\n",
    "H,\n",
    "hous\n",
    "es\"*\n",
    "\n",
    "\n",
    "<img src='figures/392598ab.eps.2.gif'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPA via places > objects contrast\n",
    "\n",
    "Places has index 3, objects has index 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_contrast = weights[:, 3] - weights[:, 2]\n",
    "\n",
    "_ = cortex.quickshow(cortex.Volume(place_contrast, 's01', 'catloc'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bottom area is PPA, the top area is RSC, the middle area is OPA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EBA via bodies > objects contrast\n",
    "\n",
    "Body has index 0, object has index 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_contrast = weights[:, 0] - weights[:, 2]\n",
    "\n",
    "_ = cortex.quickshow(cortex.Volume(body_contrast, 's01', 'catloc'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lateral Occipital Complex (LO)\n",
    "<img src='figures/loc brain.png'>\n",
    "<img src='figures/loc stimulus.png'>\n",
    "<img src='figures/nrn3747-f3.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The biggest \"blobs\" in the middle are EBA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LO via object > scrambled contrast\n",
    "\n",
    "The Lateral Occipital (LO) region is selective to objects. \n",
    "\n",
    "Object has index 2, scrambled has index 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_contrast = weights[:, 2] - weights[:, 4]\n",
    "\n",
    "_ = cortex.quickshow(cortex.Volume(object_contrast, 's01', 'catloc'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one is hard to tell for this subject as much of visual cortex is active here. LO is the region closest to the top of the image on both sides."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
