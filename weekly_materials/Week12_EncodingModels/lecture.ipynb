{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "In the last lab, we went over how we can use linear regression to estimate how much a voxel responds to each stimulus in an experiment, and how to use hypothesis testing to determine if a specific condition activates a voxel more than another condition. We saw how to estimate a t-statistic, compute a p-value and use multiple comparison correction. \n",
    "\n",
    "# Goals\n",
    "The regression models we build are predictive models. We can use them to predict the activity for each conditions. We will see in this lab how we can perform this prediction. We will also see how we can use complex stimulus that is not neatly categorized into conditions. Learning the brain responses to the different properties of the stimulus will allow us to build models that can predict the activity for new, unseen conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import neurods as nds\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel\n",
    "import cortex\n",
    "# Configure defaults for plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.aspect'] = 'auto'\n",
    "plt.rcParams['image.cmap'] = 'viridis'\n",
    "%matplotlib inline\n",
    "from scipy.stats import zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.symlink('/home/shared/cogneuro-connector/data/fmri/word_picture/','figures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "def OLS(X,Y):\n",
    "    return np.dot(inv(np.dot(X.T,X)),np.dot(X.T,Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling voxel responses\n",
    "\n",
    "We load the same data from the past two classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "basedir = os.path.join(nds.io.data_list['fmri'],'categories')\n",
    "design = np.load(os.path.join(basedir,'experiment_design.npz'))\n",
    "print('Experiment design variables: ', design.keys())\n",
    "conditions = design['conditions'].tolist()\n",
    "print('Conditions: ', conditions)\n",
    "design_run1 = design['run1']\n",
    "for i, (cond, label) in enumerate(zip(design_run1.T, conditions)):\n",
    "    plt.plot(cond+i+0.2*i, label=label, lw=2)\n",
    "plt.title('Condition labels')\n",
    "_ = plt.legend(frameon=False, bbox_to_anchor=(1.4, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the zscore function while loading the data to normalize every block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sub, xfm = 'S2', 'S2_category_auto'\n",
    "mask = cortex.db.get_mask(sub, xfm, type='cortical')\n",
    "fname = os.path.join(basedir, 'S2_categories1_{n}.nii.gz') #S2_categories1_{n}.nii.gz\n",
    "# fmri responses:\n",
    "Y = np.vstack([zscore(nds.fmri.load_data(fname.format(n=n), mask=mask, standardize=True)) for n in [1,2]])\n",
    "# stimuli:\n",
    "X = np.vstack([design[run] for run in ['run1','run2']])\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.imshow(Y)\n",
    "plt.title('Voxel responses')\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "for i, (cond, label) in enumerate(zip(X.T, conditions)):\n",
    "    plt.plot(cond+i+0.2*i, label=label, lw=2)\n",
    "plt.title('Condition labels')\n",
    "_ = plt.legend(frameon=False, bbox_to_anchor=(1.4, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a design matrix that accounts for the hemodynamic response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neurods.fmri import hrf as generate_hrf\n",
    "t_hrf, hrf_1 = generate_hrf(tr=2)\n",
    "n, d = X.shape\n",
    "\n",
    "conv_X = np.zeros_like(X)\n",
    "for i in range(d):\n",
    "    conv_X[:,i] = np.convolve(X[:,i], hrf_1)[:n]\n",
    "    \n",
    "for i, (cond, label) in enumerate(zip(conv_X.T, conditions)):\n",
    "    plt.plot(cond+i+0.2*i, label=label, lw=2)\n",
    "    \n",
    "plt.title('Condition labels')\n",
    "_ = plt.legend(frameon=False, bbox_to_anchor=(1.4, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We estimate the weights for all voxels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "weights = OLS(conv_X, Y)\n",
    "print('shape of weights is {}'.format(weights.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last lecture we estimated the mean squared error of our predictions:\n",
    "- First, we used the weights estimated above to predict the activity $\\hat {\\bf Y}$.\n",
    "- Second, we estimated the error ${\\bf Y-\\hat Y}$.\n",
    "- Then, we estimated $\\boldsymbol \\sigma^2$, the mean squared error $\\sum_{i=0}^{N-1}(Y_i - \\hat Y_i)^2$. This gave us a vector corresponding to the mean squared error at every voxel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_hat = np.dot(conv_X, weights)\n",
    "error = Y - Y_hat\n",
    "mse = np.mean((Y - Y_hat)**2, axis=0)\n",
    "vol = cortex.Volume(mse, sub, xfm, mask = mask)\n",
    "__  = cortex.quickflat.make_figure(vol)\n",
    "plt.title('mse', fontsize = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the coefficient of determination, which estimates how much of the data is being predicted: $R^2 = 1 - \\frac{\\bf \\boldsymbol \\sigma^2}{var({\\bf Y})} $.\n",
    "\n",
    "Since we have already normalized every voxel to have a variance of 1, you can simplity the computation to: $R^2 = 1 - \\bf \\boldsymbol \\sigma^2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "R2 = 1 - mse\n",
    "vol = cortex.Volume(R2, sub, xfm, mask = mask)\n",
    "__  = cortex.quickflat.make_figure(vol)\n",
    "plt.title('R^2', fontsize = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of evaluating the quality of predictions is to see how correlated they are with what we intented to predict. We can look at the time course of data for one voxels, versus the predicted activity for that voxel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vox_i = 10114 ### try other numbers like 100, 3000, 10114 ...\n",
    "plt.plot(Y_hat[:,vox_i], label = 'predicted')\n",
    "plt.plot(Y[:,vox_i], label = 'real')\n",
    "_ = plt.legend(frameon=False, bbox_to_anchor=(1.4, 1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation between vectors $A$ and $B$ of length N can be estimated as: \n",
    "\n",
    "$$\\frac{1}{N} \\sum_{i=1}^N \\frac{ (a_i - \\mu_A) (b_i - \\mu_B)  } {\\sigma_A \\sigma_B}  = \\frac{1}{N} \\sum_{i=1}^N \\frac{ (a_i - \\mu_A) }{\\sigma_A} \\frac{(b_i - \\mu_B)  } { \\sigma_B}  = \\frac{1}{N} \\sum_{i=1}^N  a_i' b_i' $$\n",
    "\n",
    "where $A'$ and $B'$ are normalized versions of $A$ and $B$ that have a 0 mean and a variance of 1.\n",
    "\n",
    "(in some textbooks you might find the above expression divided by N-1 instead of N, but we will not get into this subtelty here).\n",
    "\n",
    "To correlate two time series, we therefore effectively remove the mean and variance of each to see if they vary in the same way. We can do that by using the zscore function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Y is already mean 0 and variance 1\n",
    "plt.plot(zscore(Y_hat[:,vox_i]), label = 'predicted')\n",
    "plt.plot(Y[:,vox_i], label = 'real')\n",
    "_ = plt.legend(frameon=False, bbox_to_anchor=(1.4, 1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout session:\n",
    "\n",
    "Write a function that computes the correlation between the predicted and the real brain activity, and returns a vector with the amount of correlation for each voxel: \n",
    "\n",
    "- create new versions of the two matrices which have a column mean of 0 and a column variance of 1 (you can use the zscore function)\n",
    "- compute the **elementwise product** of the two normalized matrices\n",
    "- compute the mean of the elementwise product across the row, this is effectively the correlation\n",
    "- make a flatmap of the correlation between Y and Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# corr = compute_correlation(Y, Y_hat)\n",
    "# vol = cortex.Volume(corr, sub, xfm, mask = mask)\n",
    "# __  = cortex.quickflat.make_figure(vol)\n",
    "# plt.title('correlation', fontsize = 30)\n",
    "\n",
    "def compute_correlation(matrix_1, matrix_2):\n",
    "### STUDENT ANSWER\n",
    "    matrix_1_norm  = zscore(matrix_1)\n",
    "    matrix_2_norm  = zscore(matrix_2)\n",
    "    corr = np.mean(matrix_1_norm*matrix_2_norm, axis = 0)\n",
    "    return corr\n",
    "\n",
    "corr = compute_correlation(Y, Y_hat)\n",
    "vol = cortex.Volume(corr, sub, xfm, mask = mask)\n",
    "__  = cortex.quickflat.make_figure(vol)\n",
    "plt.title('correlation', fontsize = 30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting withheld data:\n",
    "\n",
    "We explored t-test last week. Another method for testing if the weights of a learned model are meaningful and not due only to chance is to test them to predict new, unseen data. The idea is that if the weights we estimated are indicative of how the brain responds to the experimental conditions, then we can use them to predict the brain response for new data. Here, we introduce concepts that are very important for the statistics and machine learning fields:\n",
    "\n",
    "- Training set: is the part of the data you use to estimate your model. You can use this data as you wish. We will discuss overfitting next week and see why you might want to be careful with how much of the variance of this data you want your model to predict.\n",
    "- Test set: this test should remain untouched until the very end of your analysis, where you only use it to report your results. You should never go back to your analysis and change any parameters based on the performance of your model on the test set. \n",
    "\n",
    "We did not use yet the third run of our experiment. We will load it here and use it to test the performance of our model on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub, xfm = 'S2', 'S2_category_auto'\n",
    "mask = cortex.db.get_mask(sub, xfm, type='cortical')\n",
    "fname = os.path.join(basedir, 'S2_categories1_{n}.nii.gz') #S2_categories1_{n}.nii.gz\n",
    "# fmri responses:\n",
    "Y_test = np.vstack([zscore(nds.fmri.load_data(fname.format(n=n), mask=mask, standardize=True)) for n in [3]])\n",
    "# stimuli:\n",
    "X_test = np.vstack([design[run] for run in ['run3']])\n",
    "\n",
    "n_test = X_test.shape[0]\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "for i, (cond, label) in enumerate(zip(X_test.T, conditions)):\n",
    "    plt.plot(cond+i+0.2*i, label=label, lw=2)\n",
    "plt.title('Condition labels')\n",
    "_ = plt.legend(frameon=False, bbox_to_anchor=(1.4, 1))\n",
    "\n",
    "conv_X_test = np.zeros_like(X_test)\n",
    "for i in range(d):\n",
    "    conv_X_test[:,i] = np.convolve(X_test[:,i], hrf_1)[:n_test]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakout Session\n",
    "\n",
    "Using the weights you have estimated before:\n",
    "- First, use conv_X_test to predict the activity $ {\\bf \\hat Y_{test}}$.\n",
    "- Second, estimate the error ${\\bf Y_{test}-\\hat Y_{test}}$.\n",
    "- Then, estimate $\\bf \\boldsymbol \\sigma_{test}$, the mean squared error $\\sum_{i=0}^{N-1}({Y_{test}}_i -  {\\hat Y_{test}}_i)^2$. This will give you a vector corresponding to the mean squared error at every voxel.\n",
    "- Compute the coefficient of determination, which estimates how much of the data is being predicted:\n",
    "    $R^2_{\\bf test} = 1 - \\frac{\\bf \\boldsymbol \\sigma_{test}}{var({\\bf Y_{test}})} $.\n",
    "    Since we have already normalized every voxel to have a variance of 1, you can simplity the computation to:\n",
    "    $R^2_{\\bf test} = 1 - \\bf \\boldsymbol \\sigma_{test} $\n",
    "- Produce a flatmap of $R^2_{\\bf test}$\n",
    "- Then, use the previously computed $\\boldsymbol \\sigma$ using training data to produce a flatmap of $R^2_{\\bf train} = 1 - \\bf \\boldsymbol \\sigma_{} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# start by using the variables conv_X_test and weights to estimate Y_hat_test\n",
    "# you should estimate a variable mse_test\n",
    "# then use to plot it the following:\n",
    "# vol = cortex.Volume(1-mse_test, sub, xfm, mask = mask, vmin = 0, vmax = 1)\n",
    "# __  = cortex.quickflat.make_figure(vol)\n",
    "# plt.title('R2 test', fontsize = 30)\n",
    "# then plot the training R2:\n",
    "# vol = cortex.Volume(1-mse, sub, xfm, mask = mask, vmin = 0, vmax = 1)\n",
    "# __  = cortex.quickflat.make_figure(vol)\n",
    "# plt.title('R2 train', fontsize = 30)\n",
    "\n",
    "### STUDENT ANSWER\n",
    "Y_hat_test = np.dot(conv_X_test, weights)\n",
    "error = Y_test - Y_hat_test\n",
    "mse_test = np.mean((Y_test - Y_hat_test)**2, axis=0)\n",
    "vol = cortex.Volume(1-mse_test, sub, xfm, mask = mask, vmin = 0, vmax = 1)\n",
    "__  = cortex.quickflat.make_figure(vol)\n",
    "plt.title('R2 test', fontsize = 30)\n",
    "vol = cortex.Volume(1-mse, sub, xfm, mask = mask, vmin = 0, vmax = 1)\n",
    "__  = cortex.quickflat.make_figure(vol)\n",
    "plt.title('R2 train', fontsize = 30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is the difference between the two plots?\n",
    "\n",
    "You can do the same comparisons with the correlation measure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corr = compute_correlation(Y, Y_hat)\n",
    "vol = cortex.Volume(corr, sub, xfm, mask = mask, vmin = 0, vmax = 1)\n",
    "__  = cortex.quickflat.make_figure(vol)\n",
    "plt.title('correlation', fontsize = 30)\n",
    "\n",
    "corr = compute_correlation(Y_test, Y_hat_test)\n",
    "vol = cortex.Volume(corr, sub, xfm, mask = mask, vmin = 0, vmax = 1)\n",
    "__  = cortex.quickflat.make_figure(vol)\n",
    "plt.title('correlation', fontsize = 30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the big fallacies in the fMRI litterature is to use the same data to formulate a hypothesis about the involvement of a region . You should be careful to never use the same data to make and test your hypotheses! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complex Stimuli\n",
    "\n",
    "The approach we saw so far allows us to estimate the response of each voxel to one of a few conditions. What if we want to find the response of a voxel to new, unseen conditions? What if we are interested in the variety of meanings that language can have? It will take an extremely long time to test everything single property one by one. \n",
    "\n",
    "Another approach to neuroimaging is therefore to image the brain activity while the subject sees a large number of different stimuli that vary along multiple dimensions. The idea is to cover the space of variability so that the contribution of each feature of the stimulus can be recovered from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use freely available data from the Mitchell 2008 science paper: https://www.cs.cmu.edu/afs/cs/project/theo-73/www/science2008/data.html\n",
    "\n",
    "The experiment actually consist in subjects looking at words/line drawings that are presented in isolation:\n",
    "\n",
    "<img src=\"figures/science.png\" style=\"height: 300px;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# loading data:\n",
    "basedir = os.path.join(nds.io.data_list['fmri'],'word_picture')\n",
    "name = os.path.join(basedir,'subject_1.nii.gz')\n",
    "volumes = nibabel.load(name)\n",
    "data = volumes.get_data()\n",
    "print(data.shape)\n",
    "data = data.T\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#loading mask\n",
    "name = os.path.join(basedir,'subject_1_mask.nii')\n",
    "volume = nibabel.load(name)\n",
    "mask = volume.get_data()\n",
    "print(mask.shape)\n",
    "mask = mask.T\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# flatten data to a 2D matrix\n",
    "data = data[:,mask>0]\n",
    "print(data.shape)\n",
    "\n",
    "# zscore the data\n",
    "data = zscore(data, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this package allows us to work with matlab data, which we need here to load the variables\n",
    "import scipy.io as sio\n",
    "\n",
    "# here we load the 60 words that comprise our stimuli\n",
    "words = sio.loadmat(os.path.join(basedir,'words.mat'))\n",
    "\n",
    "words = [s[0][0] for s in words['words']]\n",
    "\n",
    "print(\"Here are the stimulus words:\\n\")\n",
    "print (\" - \".join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, a stimulus was presented every 10 seconds, and the activity between 4 and 8 seconds after onset was averaged, resulting in one brain image for every stimulus presentation. Each stimulus was repeated 6 times, and the repetitions of all the stimuli was averaged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_num = 38 # change the word number\n",
    "sample_image = np.zeros_like(mask)-2\n",
    "sample_image[mask>0] = data[word_num]\n",
    "h = cortex.mosaic(sample_image) # can try with different color map: e.g. h = mosaic(image , cmap= cm.hot)\n",
    "plt.title(words[word_num],size=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset already accounts for the delay of the hemodynamic response, and therefore we should not be convolving our design matrix. We will see here how to contruct a design matrix appropriate for such an experiment.\n",
    "\n",
    "How can we represent the activity for items that do not belong into clear conditions? \n",
    "\n",
    "We could try to make each word be a condition. Ending up with 60 conditions. We see each word only once. How would that help us? We would be able to compute a contrast map between \"horse\" and \"table\", but that would not tell us much about why these differences occur, as \"horse\" and \"table\" vary in many ways. Also, learning a response per word will not allow us to know what the activity will be like for new words, such as \"goat\", \"pen\" etc.\n",
    "\n",
    "However, we know that new words have some features in common with our set of objects. What if we could learn the responses to specific properties of words (e.g. whether or not they are animate, whether or not they are edible etc...). Then we predict the activity of a novel word as a combination of the activities associated with its properties. For example, we can learn how the brain responds to objects that are manmade, inanimate, made of wood and that are used as tools, and we can estimate the brain response of \"pen\" as the combination of these responses.\n",
    "\n",
    "We will do all this in the multivariate regression framework we have used in the last labs. \n",
    "\n",
    "First, we need an annotation of the properties of these words. From looking at the list of words, it's clear that there are many properties that different sets of them share.\n",
    "\n",
    "We have access to a set of 218 questions for which every word has been labeled by multiple users on amazon mechanical Turk (Sudre et al., Neuroimage, 2012). These question were designed to represent the semantic properties of these objects. Additionally, 11 features describing the visual properties of the line drawings are also provided.\n",
    "\n",
    "The scale of the features is 1-5 with 1 being a 100% no and 5 being 100% yes.\n",
    "\n",
    "Try changing feature_i below. Try to see the different features, as well as the features 218-229 as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_data = sio.loadmat(os.path.join(basedir,'features.mat'))\n",
    "\n",
    "feature_names = feature_data['feature_names']\n",
    "features = feature_data['features']\n",
    "print(\"We have {0} features that describe the stimulus.\\n\".format(len(feature_names)))\n",
    "#print feature_names\n",
    "\n",
    "print(\"The features matrix therefore has {0} rows and {1}.\\n\".format(len(words),len(feature_names)))\n",
    "\n",
    "\n",
    "feature_i = 10\n",
    "print(\"FEATURE NUMBER {0}\".format(feature_i))\n",
    "print(feature_names[feature_i][0][0])\n",
    "for i in range(15):\n",
    "    print(words[i], features[i,feature_i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (\"Features 1 to 218\\n\")\n",
    "for i in range(15):\n",
    "    print (feature_names[i][0][0])\n",
    "print (\"...\")\n",
    "\n",
    "print (\"\\n\\nFeatures 218 to 229\\n\")\n",
    "for i in range(218,229):\n",
    "    print (feature_names[i][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's only stick to the visual features for simplicity for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = features[:,218:]\n",
    "feature_names = feature_names[:218]\n",
    "# add intercept term\n",
    "features = np.concatenate((features, np.ones([60,1])),axis = 1)\n",
    "print (\"new features size is {0}\".format(features.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Every word is therefore characterized by:\n",
    "\n",
    "1 - a vector of visual properties. For example for the \"apartment\" stimulus, Word length has a score of 4, the count of white pixels has a score of 4 etc.\n",
    "\n",
    "and\n",
    "\n",
    "2 - a 3D brain image. This is basically a set of 21764 number. Each dimension corresponds to a voxel location. We can therefore think of the brain image as a vector. \n",
    "\n",
    "See below how \"apartment\" is represented:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_i = 2\n",
    "\n",
    "print (\"\\n Word = {0}\\n\\n Features = \\n {1}\".format(words[word_i],features[word_i],size=20))\n",
    "\n",
    "vol = cortex.Volume(data[word_i],'MNI','atlas336', mask = mask)\n",
    "fig = cortex.quickflat.make_figure(vol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a matrix of 60 words x 12 features, and a matrix of 60 words x 21764 voxels. Let's focus on one voxel. We ultimately want to predict that voxel activity as a function of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# take random voxel\n",
    "random_index = 1549\n",
    "vox = np.reshape(data[:,random_index],[60,1])\n",
    "show_mat =[ features.T , vox.T]\n",
    "\n",
    "print (show_mat[1].shape)\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(20,12))\n",
    "for cnt, ax in enumerate(axes.flat):\n",
    "    im = ax.matshow(show_mat[cnt].T)\n",
    "\n",
    "# print(\" Features for all stimuli= \\n {0} \\n voxel {1} activity for all stimuli: {2}\".format(features, random_index,vox))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUILDING A PREDICTIVE MODEL\n",
    "\n",
    "### IT IS VERY IMPORTANT NOT TO USE TEST DATA IN TRAINING!!\n",
    "\n",
    "To judge if a model has learned to predict brain activity outside, we need test it on data it has not seen in training. \n",
    "\n",
    "Imagine you have a small dataset with voxel responses to features, and some of the voxels have some noise that is correlated to one of the features. The probability of such an event becomes smaller as the dataset size increases, but at low sample sizes there is a good chance of finding spurious correlations. Such a correlation actually allows you to build a model that predicts brain activity from the features, but only in that dataset, since the noise is independent of the data and will not repeat in the same way in other datasets. However, for the voxels that show a real and strong enough response to the features, you will be able to learn a model that predicts brain activity from the features, and that model should generalize to new data.\n",
    "\n",
    "This is why we always test a model on held out data that was not used in training. This allows us to judge whether the model is really predicting neural activity and not just fitted to noise in the sample.\n",
    "\n",
    "Here we separate for you the words into a test and a train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Test_index = [0,1,2,3,4,6,7,8,10,13,20,23]\n",
    "Train_index = list(set(range(60)) - set(Test_index))\n",
    "\n",
    "Train_X = features[Train_index,:]\n",
    "Train_Y = data[Train_index,:]\n",
    "print (\"shape of training features: {0}, shape of training fMRI data: {1}\".format(Train_X.shape, Train_Y.shape))\n",
    "\n",
    "Test_X = features[Test_index,:]\n",
    "Test_Y = data[Test_index,:]\n",
    "print (\"shape of testing features: {0}, shape of testing fMRI data: {1}\".format(Test_X.shape, Test_Y.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight estimation and data prediction\n",
    "\n",
    "We want to learn a function that predicts the activity for any word in terms of its features. \n",
    "\n",
    "\n",
    "#### Breakout session\n",
    "- Use the OLS function to estimate the brain response to the various features for every voxel.\n",
    "- Use the estimated weights to predict the activity for the held-out words, using Test_X.\n",
    "- Use the compute_correlation function to compute the correlation of your predicted activity and the real activity Test_Y\n",
    "- Plot a flatmap of the prediction performance. Which regions are well predicted, why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cor = compute_correlation(Test_Y, Pred_Y)\n",
    "# vol = cortex.Volume(cor, 'MNI', 'atlas336', mask=mask, vmin=0, vmax=0.6, cmap='viridis')\n",
    "# fig = cortex.quickflat.make_figure(vol, height=500)\n",
    "# plt.title(\"prediction performance with visual features\", fontsize=20)\n",
    "\n",
    "### STUDENT ANSWER\n",
    "weights = OLS(Train_X, Train_Y)\n",
    "Pred_Y = np.dot(Test_X, weights)\n",
    "corr = compute_correlation(Test_Y, Pred_Y)\n",
    "vol = cortex.Volume(corr, 'MNI', 'atlas336', mask=mask, vmin=0, vmax=0.6)\n",
    "fig = cortex.quickflat.make_figure(vol)\n",
    "plt.title(\"prediction performance with visual features\", fontsize=20);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
