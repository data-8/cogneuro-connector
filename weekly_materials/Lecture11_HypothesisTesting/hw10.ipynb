{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 10\n",
    "In this homework you will work on significance testing using permutation tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't change this cell; just run it. \n",
    "# The result will give you directions about how to log in to the submission system, called OK.\n",
    "# Once you're logged in, you can run this cell again, but it won't ask you who you are because\n",
    "# it remembers you. However, you will need to log in once per assignment.\n",
    "from client.api.notebook import Notebook\n",
    "ok = Notebook('hw10.ok')\n",
    "_ = ok.auth(inline=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. 1D regression** In a very simple 1D regression setting you will explore the relationship between noise level, amount of data points, and significance of regression slope and intercept. You will create a simple regression line with slope 0.5 and intercept 1.0. Then you will add different levels of noise to the y-variable, use linear regression to estimate the slope and intercept, and check their significance in a permutation test.\n",
    "\n",
    "Run the cell below with all the python modules you will need for the homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import nibabel\n",
    "import cortex\n",
    "from nistats.hemodynamic_models import glover_hrf as create_hrf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Create Fake Data** [1pt]\n",
    "\n",
    "Create a name called `slope`, set it to `0.5`, and a name called `intercept` and set it to `1.0`. \n",
    "\n",
    "Create a 1D array of 1000 random Gaussian values using `np.random.randn` and call it `xn_0` (for *x-noise-0*). \n",
    "\n",
    "Then create `yn_0` from `xn_0` by multiplying `xn_0` with `slope` and adding `intercept`. \n",
    "\n",
    "Plot `xn_0` and `yn_0` together in a scatterplot in a figure called `fig_xn_0_yn_0`. \n",
    "\n",
    "Fit a `LinearRegression` model to `xn_0` and `yn_0` and print its attributes `coef_` and `intercept_`. \n",
    "\n",
    "Compare the estimated betas to the true values `slope` and `intercept` using `np.allclose`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Adding Noise** [1pt] Now you will create noisy versions of `y` and plot them. \n",
    "\n",
    "Use `np.random.randn` to create a 1D array that is the same shape as `yn_0` and call it `noise`. \n",
    "\n",
    "Create three new `y`-vectors called `yn_1, yn_10, yn_100` that have different noise levels by adding `1, 10, 100` times `noise` to `yn_0` respectively. Also store the values of `.coef_` and `.intercept_` from each model estimated into `s0, sn_1, sn_10, sn_100`, and `i0, in_1, in_10, in_100` respectively so they can later be compared to a null distribution of slopes and intercepts for determining significance.\n",
    "\n",
    "Fit a linear model of `xn_0` onto each `y` vector and use `.predict` to compute the regression line for each model, and store those regression lines in `yhat_n0, yhat_n, yhat_n10, yhat_n100`.\n",
    "\n",
    "Create a figure named `fig_noisy` of size `(20, 5)`.\n",
    "\n",
    "Use `plt.subplot` to make 4 scatterplots horizontally next to each other (all in 1 row with 4 columns), in which you plot the x data (without noise) to the 4 y data vectors that have varying amounts of noise. To be precise, plot `(xn_0, yn_0), (xn_0, yn_1), (xn_0, yn_10), (xn_0, yn_100)` respectively. \n",
    "\n",
    "Add a plot for the regrssion line (yhat) to each plot. \n",
    "\n",
    "Give each subplot a title containing noise level (`0, 1, 10, 100`) and respective slope and intercept (up to 2 digits after the decimal point). \n",
    "\n",
    "Note that in the plots the lines will appear to have quite different slopes because the y-axis scales will be different. (If you want to compare slopes, you can use `plt.axis([-4, 4, -4, 4])` on all the plots to make them be on the same scale, but look at the point clouds without scaling as well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Permutation Test** [1pt] In order to do permutation testing, you need to compute a null distribution to compare the above estimates with. In this part you will create this distribution for the first 10 entries of the dataset `(xn_0, yn_0)`.\n",
    "\n",
    "\n",
    "Because there are no complicated temporal dependencies in the fake data generated here, we can permute the entries of the y-data many times instead of blocks of x like we do for fMRI data. We use permutations to break the relationship between the data and the model in order to see whether the unpermuted setting looks different in any way. Whether we permute the x-side or the y-side doesn't matter if there are no temporal dependencies. Permuting the y-side many times makes it possible for us to exploit the fact that the scikit-learn `LinearRegression` models can do many regressions at once. You will make a Y-matrix with 1000 columns, where each column is a permuted version of yn_0. As for computation, this is like performing simple linear regression on 1000 voxels.\n",
    "\n",
    "Use slicing to create `xn_0_10` and `yn_0_10` which should contain the first ten entries of `xn_0` and `yn_0` respectively.\n",
    "\n",
    "Now create 1000 permuted versions of `yn_0_10` by doing the following:\n",
    "Preallocate the array of permuted y vectors by creating a 2-D array called `permuted_Y_10` of shape `(10, 1000)`. \n",
    "\n",
    "\n",
    "Create a `for` loop that iterates over the variable `i` going from 0 to 1000 (the number of columns of `permuted_Y_10`). Inside the loop, set the `i`-th column of `Y_10` to `yn_0_10` using slicing. Then use `np.random.shuffle` to shuffle that same column.\n",
    "\n",
    "Now perform a linear regression of `xn_0_10` on all 1000 columns of `permuted_Y_10` in one call to `.fit`. Extract the slopes into a name called `permuted_slopes_0_10`.\n",
    "\n",
    "Do a linear regression on `xn_0_10` and `yn_0_10` and store the slope in `slope_0_10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Visualize Null Distribution and Test Statistic** [0.5pts] Now you can compare the null distributions with the test statistics (the slope and intercept) by visualizing the two. \n",
    "\n",
    "Display `permuted_slopes_0_10` in a histogram with 100 bins. Then draw a vertical line using `np.vlines` at the x-value of `slope_0_10` (make it start at y=0 and end at 25 or so, where the histogram peaks).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) Compute p-value** [0.5pts] Now you can compute the p-values by finding the proportion of values in the null distribution that are greater than the test statistic. Make sure to use the corrected formula learned in class though! Compute the p-values for the slope and store them in `p_slope_0_10`, and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f) Explore effects of noise on p-value** [1pt]\n",
    "\n",
    "The idea of this exercise is to look at the effect that varying values of noise have on p-values. The previous example was significant with only 10 data points because there was no noise, so let's now take a look at the examples created above that have more noise, still using only 10 samples of each: \n",
    "\n",
    "Set a counter to 1\n",
    "\n",
    "In a for-loop, loop over `noise_level in (0.0, 1.0, 10.0, 100.0)`: In it \n",
    "- Create `cur_y_noisy` by adding `noise_level` times `noise` to `yn_0`. Then create `cur_y_noisy_10` by selecting the first 10 elements of `cur_y_noisy` using slicing.\n",
    "\n",
    "- Now do the for loop in which you fill the columns of `permuted_Y_10` with shuffled versions of `cur_y_noisy_10`. Then perform the regression of `xn_0_10` against `permuted_Y_10`.\n",
    "\n",
    "- Store the permuted slopes in `cur_permuted_slopes`.\n",
    "\n",
    "- Do the unpermuted regression of `xn_0_10` and `cur_y_noisy_10` and store the slope in `cur_slope`.\n",
    "\n",
    "- Compute `cur_pvalue` using `cur_slope` and `cur_permuted_slopes` as above.\n",
    "\n",
    "- Create a subplot (4 in one row), select the one indicated by the counter, and plot `cur_permuted_slopes` as a histogram with 100 bins, and `cur_slope` as a `vline`. Add a title indicating the `p-value` and the `noise_level` to **3 digits after the decimal point**\n",
    "\n",
    "- increment the counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**(g) Explore effects of noise and data size on p-value** [1pt] Now you'll see the effects that changing the number of data points has on p-values, and how noise interacts with those effects.\n",
    "\n",
    "Repeat what you did in **(f)** using `100` and `1000` samples, and plot the results for `10, 100, 1000` samples and noise levels of `0, 1, 10, 100` into three lines of four plots.\n",
    "\n",
    "You can reuse the code from **(f)**: Create an outer for-loop over `n_samples in (10, 100, 1000)`, in which you allocated `permuted_Y` to be of shape `(n_samples, 1000)`, and then use the two nested for-loops from above to compute the permutation distributions and plots. You will have to shorten `y_noisy` to `:n_samples` instead of `:10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Contrast maps**\n",
    "This exercise aims at creating p-value maps for the motor contrasts from last homework.\n",
    "\n",
    "Run all the below cells. They are copied from the lecture. You will need to use the functions and data from these cells to complete this exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motor_data_file = \"/data/cogneuro/fMRI/motor/s01_motorloc.nii.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nifti(filename, zscore=True, mask=None):\n",
    "    img = nibabel.load(filename)\n",
    "    data = img.get_data().T\n",
    "    if mask is not None:\n",
    "        data = data[:, mask]\n",
    "    if zscore:\n",
    "        data -= data.mean(0)\n",
    "        data /= data.std(0) + 1e-8\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = cortex.db.get_mask('s01', 'catloc', 'cortical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motor_data = load_nifti(motor_data_file, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motor_categories = np.load(\"/home/jovyan/motorloc_experimental_conditions.npy\")\n",
    "unique_motor_categories = np.unique(motor_categories)\n",
    "unique_motor_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stimulus_vectors = []\n",
    "for category in unique_motor_categories:\n",
    "    stimulus_vectors.append(motor_categories == category)\n",
    "stimulus_design = np.stack(stimulus_vectors, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve_designmat(designmat):\n",
    "    \n",
    "    num_stim_types = designmat.shape[1]\n",
    "    n = designmat.shape[0]\n",
    "\n",
    "    # create the hrf\n",
    "    hrf = create_hrf(tr=2, oversampling=1, time_length=32)\n",
    "    \n",
    "    # create the current shuffled design matrix using the shuffled indices\n",
    "    cur_designmat = np.zeros(designmat.shape)\n",
    "    for cur_column in np.arange(num_stim_types):\n",
    "        cur_stim_vec_shuffled = designmat[:,cur_column]\n",
    "        cur_designmat[:,cur_column] = np.convolve(cur_stim_vec_shuffled, hrf)[:n]        \n",
    "\n",
    "    return cur_designmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_test(x, y, shuffle_func, test_stat_func, test_stat_param=None, num_resamples=1000,\n",
    "                    return_null_dist=True):\n",
    "    n = x.shape[0]\n",
    "    num_dependent = 1 if y.ndim == 1 else y.shape[1]\n",
    "    if return_null_dist:\n",
    "        null_dist = np.zeros((num_resamples,num_dependent))\n",
    "    else:\n",
    "        null_dist = None\n",
    "    # calculate the real test statistic for the x and y data, passing in the test_stat\n",
    "    # param if one was passed in\n",
    "    if test_stat_param is None:\n",
    "        real_test_stat = test_stat_func(x,y)\n",
    "    else:\n",
    "        real_test_stat = test_stat_func(x,y,test_stat_param)\n",
    "\n",
    "    num_greater_null = np.zeros(y.shape[1], dtype=int)\n",
    "    for cur_iter in range(num_resamples):\n",
    "\n",
    "        # resample x without replacement by shuffling the indices in x using a function\n",
    "        x_shuffle = shuffle_func(x)\n",
    "\n",
    "        # calculate the test statistic using the function passed in\n",
    "        if test_stat_param is None:\n",
    "            test_stat = test_stat_func(x_shuffle,y)\n",
    "        else:\n",
    "            test_stat = test_stat_func(x_shuffle,y,test_stat_param)\n",
    "        \n",
    "        num_greater_null[test_stat > real_test_stat] += 1\n",
    "        # store the test statistic in the null distribution vector\n",
    "        if return_null_dist:\n",
    "            null_dist[cur_iter,:] = test_stat\n",
    "        \n",
    "    \n",
    "    # calculate the p-value\n",
    "    p_value = (num_greater_null + 1) / (num_resamples + 1)\n",
    "    \n",
    "    # return the p-value and the null distribution\n",
    "    return (p_value, null_dist, real_test_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_blocks(x, block_size=10):\n",
    "    \n",
    "    n = x.shape[0]\n",
    "    n_blocks = n // block_size\n",
    "    n_block_samples = n_blocks * block_size\n",
    "\n",
    "    # shuffle the indices by blocks\n",
    "    indices = np.arange(n_block_samples)\n",
    "    blocked_indices = indices.reshape([n_blocks, block_size])\n",
    "    np.random.shuffle(blocked_indices)\n",
    "    indices_block_shuffled = blocked_indices.reshape(-1)\n",
    "    adjusted_indices = np.concatenate((indices_block_shuffled, np.arange(n_block_samples, len(x))))\n",
    "    x_block_shuffled = x[adjusted_indices]\n",
    "\n",
    "    return x_block_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_contrast_convolve(designmat, bold_data, contrast_idxs):\n",
    "    # convolve the stimulus design matrix with the hrf and return it\n",
    "    designmat_conv = convolve_designmat(designmat)\n",
    "    \n",
    "    # fit the model on the convolved design matrix\n",
    "    model = LinearRegression()\n",
    "    model.fit(designmat_conv, bold_data)\n",
    "    contrast = model.coef_[:, contrast_idxs[0]] - model.coef_[:, contrast_idxs[1]]\n",
    "    return contrast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** [0.5pts] In last week's homework you learned what contrasts are used to localizer the motor cortex ROIs. Below is a reminder of those contrasts. Here you'll create tuples to quantify the indices into the design matrix that are needed to create those contrasts.\n",
    "\n",
    "First print out the order that the motor tasks are stored in the design matrix by finding the relevant name from the code that creates the design matrix above.\n",
    "\n",
    "Then create one 2 value tuple for each contrast, where the values are the column indices of the motor task design matrix. The second index represents the motor task that will be subtracted from the first. Call each tuple `contrast_idxs_XXXX`, where `XXXX` is the name of each of the contrasts listed below:\n",
    "\n",
    "- eyes vs rest\n",
    "- foot vs rest\n",
    "- hand vs rest\n",
    "- speak vs rest\n",
    "- speak vs mouth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrast_idxs_eyes_rest = ...\n",
    "contrast_idxs_foot_rest = ...\n",
    "contrast_idxs_hand_rest = ...\n",
    "contrast_idxs_speak_rest = ...\n",
    "contrast_idxs_speak_mouth = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** [1pt] Now you're going to do a permutation test on the `eyes_rest` contrast you just defined. To do this use the `permutation_test` from the lecture to compute `contrast_eyes_rest_pvalue, null_dists_contrast_eyes_rest, contrast_eyes_rest`. Choose 100 permutations, which should take about 5 minutes to calculate. \n",
    "\n",
    "For voxels `[34854, 37594, 36630, 25004, 12135, 0]`, plot the null distribution histogram (20 bins) and the actual contrast value as a `vline`. Make a figure with 6 subplots for this, arranged in two lines of three plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** [0.5pts] Plot the negative log p-values for `eyes_rest` on a flatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** [0.5 pts]\n",
    "Use all the p-values you just calculated for the `eyes_rest` localizer and make a mask that indicates which voxels are significant, and which aren't. Use a threshold of `p < 0.01`  because that value will choose only the voxels that were entirely above their null distribution.\n",
    "\n",
    "Plot that mask as a flatmap. You'll want to limit the range of values assigned to the colormap since the mask can be either `True` or `False`, meaning `1` and `0`. To do that, specify `vmin=0` and `vmax=1` as arguments to the `cortex.Volume` that you create. Also use the `Reds` colormap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)** [1.5pts] Compute permutation tests for the four other contrasts listed above and display their thresholded (p < 0.01) p-value-maps. Use `return_null_dist=False` in `permutation_test` to avoid memory problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to submit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ok.submit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
